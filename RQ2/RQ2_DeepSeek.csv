NAME,LINK,DESCRIPTION,DeepSeek_RQ2,Priority,Final_Result,Rationale
Website updates to satisfy Apache privacy policies,https://issues.apache.org/jira/browse/KAFKA-13868,"The ASF has updated its privacy policy and all websites must be compliant. 

The full guidelines can be found in https://privacy.apache.org/faq/committers.html 

The Kafka website has a few issues, including: 

It's missing a link to the privacy policy: https://privacy.apache.org/policies/privacy-policy-public.html 
It's using Google Analytics 
It's using Google Fonts 
It's using scripts hosted on Cloudflare CDN 
Embedded videos don't have an image placeholder 
As per the email sent to the PMC, all updates have to be done by July 22.","Decomposed Bugs:  
B: The Kafka website is missing a link to the privacy policy: https://privacy.apache.org/policies/privacy-policy-public.html.  
B: The Kafka website is using Google Analytics.  
B: The Kafka website is using Google Fonts.  
B: The Kafka website is using scripts hosted on Cloudflare CDN.  
B: The Kafka website has embedded videos that don't have an image placeholder.",Critical,TRUE,
ApplicationMasterProtocolPBClientImpl.allocate fails with EOFException when RPC privacy is enabled,https://issues.apache.org/jira/browse/HADOOP-14062,"When privacy is enabled for RPC (hadoop.rpc.protection = privacy), ApplicationMasterProtocolPBClientImpl.allocate sometimes (but not always) fails with an EOFException. I've reproduced this with Spark 2.0.2 built against latest branch-2.8 and with a simple distcp job on latest branch-2.8. 

Steps to reproduce using distcp: 

1. Set hadoop.rpc.protection equal to privacy 
2. Write data to HDFS. I did this with Spark as follows: 

sc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789"")).mkString(""|"")).toDF().repartition(100).write.parquet(""hdfs:///tmp/testData"") 
3. Attempt to distcp that data to another location in HDFS. For example: 

hadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy 
I observed this error in the ApplicationMaster's syslog: 

2016-12-19 19:13:50,097 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1482189777425_0004, File: hdfs://<namenode_host>:8020/tmp/hadoop-yarn/staging/<hdfs_user>/.staging/job_1482189777425_0004/job_1482189777425_0004_1.jhist 
2016-12-19 19:13:51,004 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0 
2016-12-19 19:13:51,031 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1482189777425_0004: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:22528, vCores:23> knownNMs=3 
2016-12-19 19:13:52,043 INFO [RMCommunicator Allocator] org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking ApplicationMasterProtocolPBClientImpl.allocate over null. Retrying after sleeping for 30000ms. 
java.io.EOFException: End of File Exception between local host is: ""<application_master_host>/<ip_addr>""; destination host is: ""<rm_host>"":8030; : java.io.EOFException; For more details see: http://wiki.apache.org/hadoop/EOFException 
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) 
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 
at java.lang.reflect.Constructor.newInstance(Constructor.java:422) 
at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801) 
at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765) 
at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486) 
at org.apache.hadoop.ipc.Client.call(Client.java:1428) 
at org.apache.hadoop.ipc.Client.call(Client.java:1338) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) 
at com.sun.proxy.$Proxy80.allocate(Unknown Source) 
at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:497) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335) 
at com.sun.proxy.$Proxy81.allocate(Unknown Source) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281) 
at java.lang.Thread.run(Thread.java:745) 
Caused by: java.io.EOFException 
at java.io.DataInputStream.readInt(DataInputStream.java:392) 
at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785) 
at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156) 
at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053) 
Marking as ""critical"" since this blocks YARN users from encrypting RPC in their Hadoop clusters.","B: When privacy is enabled for RPC (hadoop.rpc.protection = privacy), ApplicationMasterProtocolPBClientImpl.allocate sometimes fails with an EOFException. This issue has been reproduced with Spark 2.0.2 built against the latest branch-2.8 and with a simple distcp job on the latest branch-2.8.

B: The following steps reproduce the issue using distcp:
1. Set hadoop.rpc.protection equal to privacy.
2. Write data to HDFS using Spark as follows: 
   sc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789"")).mkString(""|"")).toDF().repartition(100).write.parquet(""hdfs:///tmp/testData"").
3. Attempt to distcp that data to another location in HDFS using: 
   hadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy.

B: The error observed in the ApplicationMaster's syslog includes:
2016-12-19 19:13:50,097 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1482189777425_0004, File: hdfs://<namenode_host>:8020/tmp/hadoop-yarn/staging/<hdfs_user>/.staging/job_1482189777425_0004/job_1482189777425_0004_1.jhist.
2016-12-19 19:13:51,004 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0.
2016-12-19 19:13:51,031 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1482189777425_0004: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:22528, vCores:23> knownNMs=3.
2016-12-19 19:13:52,043 INFO [RMCommunicator Allocator] org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking ApplicationMasterProtocolPBClientImpl.allocate over null. Retrying after sleeping for 30000ms.
java.io.EOFException: End of File Exception between local host is: ""<application_master_host>/<ip_addr>""; destination host is: ""<rm_host>"":8030; : java.io.EOFException; For more details see: http://wiki.apache.org/hadoop/EOFException.

B: The issue is marked as ""critical"" since it blocks YARN users from encrypting RPC in their Hadoop clusters.",Critical,FALSE,Incorrect Interpretation of Solutions
Add privacy policy in web site,https://issues.apache.org/jira/browse/AMBARI-766,As the web site use google analytics privacy policy page must be added.,"B: The website is using Google Analytics, and a privacy policy page must be added.",Major,TRUE,
Thrift Server 1 uses different QOP settings than RPC and Thrift Server 2 and can easily be misconfigured so there is no encryption when the operator expects it.,https://issues.apache.org/jira/browse/HBASE-17513,"As of HBASE-14400 the setting hbase.thrift.security.qop was unified to behave the same as the general HBase RPC protection. However, this only happened for the Thrift2 server. The Thrift server found in the thrift package (aka Thrift Server 1) still hard codes the old configs of 'auth', 'auth-int', and 'auth-conf'.

Additionally, these Quality of Protection (qop) settings are used only by the SASL transport. If a user configures the HBase Thrift Server to make use of the HTTP transport (to enable doAs proxying e.g. for Hue) then a QOP setting of 'privacy' or 'auth-conf' won't get them encryption as expected.

We should

1) update hbase-thrift/src/main/.../thrift/ThriftServerRunner to rely on SaslUtil to use the same 'authentication', 'integrity', 'privacy' configs in a backward compatible way
2) also have ThriftServerRunner warn when both hbase.thrift.security.qop and hbase.regionserver.thrift.http are set, since the latter will cause the former to be ignored. (users should be directed to hbase.thrift.ssl.enabled and related configs to ensure their transport is encrypted when using the HTTP transport.)","Decomposed Bugs:  
B: As of HBASE-14400, the setting hbase.thrift.security.qop was unified to behave the same as the general HBase RPC protection, but this only applies to the Thrift2 server. The Thrift server found in the thrift package (Thrift Server 1) still hard codes the old configs of 'auth', 'auth-int', and 'auth-conf'.  
B: The Quality of Protection (qop) settings are used only by the SASL transport. If a user configures the HBase Thrift Server to use the HTTP transport (e.g., for doAs proxying for Hue), a QOP setting of 'privacy' or 'auth-conf' won't provide encryption as expected.  
B: The hbase-thrift/src/main/.../thrift/ThriftServerRunner should be updated to rely on SaslUtil to use the same 'authentication', 'integrity', and 'privacy' configs in a backward-compatible way.  
B: ThriftServerRunner should warn when both hbase.thrift.security.qop and hbase.regionserver.thrift.http are set, as the latter will cause the former to be ignored. Users should be directed to hbase.thrift.ssl.enabled and related configs to ensure transport encryption when using the HTTP transport.",Critical,FALSE,Incorrect Interpretation of Solutions
Missing localization strings in the installer,https://issues.apache.org/jira/browse/FLEX-34392,"the following strings aren't localized 
Select AIR and Flash Player 
Select Flex Version 
Select AIR Version 
Select Flash Player Version 
anonymous usage statics will be collected in accordance with our privacy policy.","B: The string ""Select AIR and Flash Player"" isn't localized.  
B: The string ""Select Flex Version"" isn't localized.  
B: The string ""Select AIR Version"" isn't localized.  
B: The string ""Select Flash Player Version"" isn't localized.  
B: The string ""anonymous usage statics will be collected in accordance with our privacy policy"" isn't localized.",Minor,TRUE,
Use SaslUtil to set Sasl.QOP in 'Thrift',https://issues.apache.org/jira/browse/HBASE-19118,"In Configure the Thrift Gateway, it says ""set the property hbase.thrift.security.qop to one of the following three values: privacy, integrity, authentication"", which would lead to failure of starting up a thrift server. 
In fact, the value of hbase.thrift.security.qop should be auth, auth-int, auth-conf, according to the documentation of Sasl.QOP","Decomposed Bugs:  
B: In Configure the Thrift Gateway, it says ""set the property hbase.thrift.security.qop to one of the following three values: privacy, integrity, authentication"", which would lead to failure of starting up a thrift server.  
B: The value of hbase.thrift.security.qop should be auth, auth-int, auth-conf, according to the documentation of Sasl.QOP.",Major,FALSE,Incorrect Interpretation of Solutions
Netbeans not allowed to access personnal files and folder under macOS,https://issues.apache.org/jira/browse/NETBEANS-5004,"With the new privacy and security features enforced macOS Catalina, Netbeans and any of its subprocesses are not allowed to access any of the personnal files and folders (e.g.: Downloads, Documents...).

Normally, a system prompt should ask the user when the application attempts to access such files, but here nothing happens. For instance, when I try to access the Document directory through the ""Open file..."" menu, this directory appears empty.

The bug seems to be related to the fact that the shell script used to launch Netbeans (/bin/sh [/Applications/NetBeans/Apache NetBeans) is sandboxed.

Giving full disk access to Netbeans in system preferences did not solve the issue.

The system console error related to this bug is attached to this ticket.","Decomposed Bugs:  
B: With the new privacy and security features enforced by macOS Catalina, Netbeans and any of its subprocesses are not allowed to access personal files and folders (e.g., Downloads, Documents).  
B: Normally, a system prompt should ask the user when the application attempts to access such files, but here nothing happens. For instance, when trying to access the Document directory through the ""Open file..."" menu, this directory appears empty.  
B: The bug seems to be related to the fact that the shell script used to launch Netbeans (/bin/sh [/Applications/NetBeans/Apache NetBeans) is sandboxed.  
B: Giving full disk access to Netbeans in system preferences did not solve the issue.  
B: The system console error related to this bug is attached to this ticket.",Major,FALSE,Incorrect Interpretation of Solutions
Convert all Lucene web properties to use the ASF CMS,https://issues.apache.org/jira/browse/LUCENE-2748,"The new CMS has a lot of nice features (and some kinks to still work out) and Forrest just doesn't cut it anymore, so we should move to the ASF CMS: http://apache.org/dev/cms.html","B: The new CMS has a lot of nice features (and some kinks to still work out).  
B: Forrest just doesn't cut it anymore, so we should move to the ASF CMS: http://apache.org/dev/cms.html.",Major,TRUE,
PutHDFS will pass files to success when they were not successfully written with hadoop client misconfiguration,https://issues.apache.org/jira/browse/NIFI-1062,"PutHDFS will create an empty file, but the data it is attempting to write will fail with this stack trace, and the flow file gets routed to success. 

2015-10-24 11:16:19,278 WARN [Thread-4674] org.apache.hadoop.hdfs.DFSClient DataStreamer Exception 
java.lang.IllegalArgumentException: null 
at javax.security.auth.callback.NameCallback.<init>(NameCallback.java:90) ~[na:1.8.0_45] 
at com.sun.security.sasl.digest.DigestMD5Client.processChallenge(DigestMD5Client.java:324) ~[na:1.8.0_45] 
at com.sun.security.sasl.digest.DigestMD5Client.evaluateChallenge(DigestMD5Client.java:220) ~[na:1.8.0_45] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.evaluateChallengeOrResponse(SaslParticipant.java:113) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:451) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getSaslStreams(SaslDataTransferClient.java:390) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:262) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:210) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:182) ~[na:na] 
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1413) ~[na:na] 
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1361) ~[na:na] 
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588) ~[na:na] 
2015-10-24 11:16:19,303 INFO [Timer-Driven Process Thread-2] o.apache.nifi.processors.hadoop.PutHDFS PutHDFS[id=3c30a474-86b4-45fc-b771-95d7a1b5054d] copied StandardFlowFileRecord[uuid=5a3deada-9739-474f-a83d-0447ad5aefd9,claim=StandardContentClaim [resourceClaim=StandardResourceClaim[id=1445694733273-1, container=default, section=1], offset=87, length=29],offset=0,name=x11.txt,size=29] to HDFS at /use/hdfs/x11.txt in 56 milliseconds at a rate of 511 bytes/sec","Decomposed Bugs:  
B: PutHDFS creates an empty file when attempting to write data, and the flow file is incorrectly routed to success despite the failure.  
B: The data write operation fails with the following stack trace:  
2015-10-24 11:16:19,278 WARN [Thread-4674] org.apache.hadoop.hdfs.DFSClient DataStreamer Exception  
java.lang.IllegalArgumentException: null  
at javax.security.auth.callback.NameCallback.<init>(NameCallback.java:90) ~[na:1.8.0_45]  
at com.sun.security.sasl.digest.DigestMD5Client.processChallenge(DigestMD5Client.java:324) ~[na:1.8.0_45]  
at com.sun.security.sasl.digest.DigestMD5Client.evaluateChallenge(DigestMD5Client.java:220) ~[na:1.8.0_45]  
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.evaluateChallengeOrResponse(SaslParticipant.java:113) ~[na:na]  
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:451) ~[na:na]  
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getSaslStreams(SaslDataTransferClient.java:390) ~[na:na]  
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:262) ~[na:na]  
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:210) ~[na:na]  
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:182) ~[na:na]  
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1413) ~[na:na]  
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1361) ~[na:na]  
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588) ~[na:na]  
B: The flow file is incorrectly marked as successful despite the failure, as shown in the log:  
2015-10-24 11:16:19,303 INFO [Timer-Driven Process Thread-2] o.apache.nifi.processors.hadoop.PutHDFS PutHDFS[id=3c30a474-86b4-45fc-b771-95d7a1b5054d] copied StandardFlowFileRecord[uuid=5a3deada-9739-474f-a83d-0447ad5aefd9,claim=StandardContentClaim [resourceClaim=StandardResourceClaim[id=1445694733273-1, container=default, section=1], offset=87, length=29],offset=0,name=x11.txt,size=29] to HDFS at /use/hdfs/x11.txt in 56 milliseconds at a rate of 511 bytes/sec.",Major,FALSE,Over-decomposition
"When a cube built successfully with around 170 measures, some queries cannot be executed",https://issues.apache.org/jira/browse/KYLIN-1518,"A cube with the same data model, the same cube definition except the number of measures. If the number is around 150, the same query can be executed successfully. The error log is as follows: 

at org.apache.calcite.avatica.Helper.createException(Helper.java:41) 
at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:112) 
at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:130) 
at org.apache.kylin.rest.service.QueryService.execute(QueryService.java:361) 
at org.apache.kylin.rest.service.QueryService.queryWithSqlMassage(QueryService.java:276) 
at org.apache.kylin.rest.service.QueryService.query(QueryService.java:118) 
at org.apache.kylin.rest.service.QueryService$$FastClassByCGLIB$$4957273f.invoke(<generated>) 
at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618) 
at org.apache.kylin.rest.service.QueryService$$EnhancerByCGLIB$$ab2dbbe7.query(<generated>) 
at org.apache.kylin.rest.controller.QueryController.doQueryWithCache(QueryController.java:191) 
at org.apache.kylin.rest.controller.QueryController.query(QueryController.java:95) 
at sun.reflect.GeneratedMethodAccessor169.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:606) 
at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:213) 
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:126) 
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:96) 
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:617) 
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:578) 
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80) 
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:923) 
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:852) 
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:882) 
at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:789) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:646) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:727) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330) 
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118) 
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:105) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192) 
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160) 
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346) 
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:195) 
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:266) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220) 
at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122) 
at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504) 
at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170) 
at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) 
at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950) 
at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) 
at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421) 
at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074) 
at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611) 
at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) 
at java.lang.Thread.run(Thread.java:745) 
Caused by: java.lang.IndexOutOfBoundsException: Index: 187, Size: 186 
at java.util.ArrayList.rangeCheck(ArrayList.java:635) 
at java.util.ArrayList.get(ArrayList.java:411) 
at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:914) 
at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:885) 
at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) 
at org.apache.calcite.rex.RexProgramBuilder.registerInput(RexProgramBuilder.java:274) 
at org.apache.calcite.rex.RexProgramBuilder.addProject(RexProgramBuilder.java:185) 
at org.apache.calcite.rex.RexProgram.create(RexProgram.java:215) 
at org.apache.calcite.rex.RexProgram.create(RexProgram.java:184) 
at org.apache.kylin.query.relnode.OLAPProjectRel.implementEnumerable(OLAPProjectRel.java:219) 
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:159) 
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:155) 
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:155) 
at org.apache.kylin.query.relnode.OLAPToEnumerableConverter.implement(OLAPToEnumerableConverter.java:97) 
at org.apache.calcite.adapter.enumerable.EnumerableRelImplementor.implementRoot(EnumerableRelImplementor.java:99) 
at org.apache.calcite.adapter.enumerable.EnumerableInterpretable.toBindable(EnumerableInterpretable.java:92) 
at org.apache.calcite.prepare.CalcitePrepareImpl$CalcitePreparingStmt.implement(CalcitePrepareImpl.java:1050) 
at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:293) 
at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:188) 
at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:671) 
at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:572) 
at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:541) 
at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:173) 
at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:561) 
at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:477) 
at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:109) 
... 75 more","Decomposed Bugs:  
B: A cube with the same data model and cube definition, but with around 150 measures, causes a query execution failure. The error log indicates an `IndexOutOfBoundsException` at `java.util.ArrayList.rangeCheck(ArrayList.java:635)`.  
B: The error log shows a failure at `org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:914)` due to an index mismatch (`Index: 187, Size: 186`).  
B: The query execution fails in `org.apache.kylin.query.relnode.OLAPProjectRel.implementEnumerable(OLAPProjectRel.java:219)` when processing the cube with a high number of measures.  
B: The failure propagates through the query execution stack, starting from `org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:109)` and ending with `java.lang.Thread.run(Thread.java:745)`.",Major,FALSE,Over-analysis
Statestore should garbage collect hung connections,https://issues.apache.org/jira/browse/IMPALA-1726,"If a node is truly hung, the statestore may apparently wait forever to receive the heartbeat response. We need to check the TCP timeouts on the connections from the statestore to the subscriber. 

Since the operating system can also interfere, we should periodically visit all heartbeat threads and see how long they've been in the heartbeat RPC for. I think we can forcibly close the socket in a GC thread if it's taken too long. The next time round should hit the TCP cnxn timeout (or be refused), and the subscriber should be marked as dead.","Decomposed Bugs:  
B: If a node is truly hung, the statestore may apparently wait forever to receive the heartbeat response. We need to check the TCP timeouts on the connections from the statestore to the subscriber.  
B: Since the operating system can interfere, we should periodically visit all heartbeat threads and see how long they've been in the heartbeat RPC.  
B: We can forcibly close the socket in a GC thread if it's taken too long. The next time round should hit the TCP connection timeout (or be refused), and the subscriber should be marked as dead.",Critical,FALSE,Over-decomposition
Yarn NodeManager OOM Listener Fails Compilation on Ubuntu 18.04,https://issues.apache.org/jira/browse/YARN-8498,"While building this project, I ran into a few compilation errors here. The first one was in this file: 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/impl/oom_listener_main.c 

At the very end, during the compilation of the OOM test, it fails again: 
hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:256:7: error: ¡®__WAIT_STATUS¡¯ was not declared in this scope 
__WAIT_STATUS mem_hog_status = {}; 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:257:30: error: ¡®mem_hog_status¡¯ was not declared in this scope 
__pid_t exited0 = wait(mem_hog_status); 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:275:21: error: expected ¡®;¡¯ before ¡®oom_listener_status¡¯ 
__WAIT_STATUS oom_listener_status = {}; 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:276:30: error: ¡®oom_listener_status¡¯ was not declared in this scope 
__pid_t exited1 = wait(oom_listener_status);","Decomposed Bugs:  
B: Compilation error in `hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:256:7`: `__WAIT_STATUS` was not declared in this scope.  
B: Compilation error in `hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:257:30`: `mem_hog_status` was not declared in this scope.  
B: Compilation error in `hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:275:21`: Expected `;` before `oom_listener_status`.  
B: Compilation error in `hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:276:30`: `oom_listener_status` was not declared in this scope.",Blocker,FALSE,Over-analysis
Performance of DirectColorModel RGB bitmap images,https://issues.apache.org/jira/browse/XGC-71,When RGB bitmaps are used there can be performance improvements made to mitigate some of the impact.,"B: When RGB bitmaps are used, there can be performance improvements made to mitigate some of the impact.",-,TRUE,
activemq-client tests are failing with JDK17+,https://issues.apache.org/jira/browse/AMQ-9341,"activemq-client module doesn't build due to test failure: 

Caused by: java.net.BindException: Can't assign requested address 
at java.base/sun.nio.ch.Net.connect0(Native Method) 
at java.base/sun.nio.ch.Net.connect(Net.java:579) 
at java.base/sun.nio.ch.Net.connect(Net.java:568) 
at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593) 
at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) 
at java.base/java.net.Socket.connect(Socket.java:633) 
at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:304) 
at org.apache.activemq.transport.tcp.TcpTransport.connect(TcpTransport.java:525)","Decomposed Bugs:  
B: The activemq-client module doesn't build due to a test failure caused by `java.net.BindException: Can't assign requested address`.  
B: The test failure occurs due to an issue in the connection process, as shown in the stack trace starting from `java.base/sun.nio.ch.Net.connect0(Native Method)` to `org.apache.activemq.transport.tcp.TcpTransport.connect(TcpTransport.java:525)`.",Major,FALSE,Over-analysis
Python DataFrame CSV load on large file is writing to console in Ipython,https://issues.apache.org/jira/browse/SPARK-14103,"I am using the spark from the master branch and when I run the following command on a large tab separated file then I get the contents of the file being written to the stderr

df = sqlContext.read.load(""temp.txt"", format=""csv"", header=""false"", inferSchema=""true"", delimiter=""\t"")
Here is a sample of output:

^M[Stage 1:> (0 + 2) / 2]16/03/23 14:01:02 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 2)
com.univocity.parsers.common.TextParsingException: Error processing input: Length of parsed input (1000001) exceeds the maximum number of characters defined in your parser settings (1000000). Identified line separator characters in the parsed content. This may be the cause of the error. The line separator in your parser settings is set to '\n'. Parsed content:
Privacy-shake"",: a haptic interface for managing privacy settings in mobile location sharing applications privacy shake a haptic interface for managing privacy settings in mobile location sharing applications 2010 2010/09/07 international conference on human computer interaction interact 43331058 19371[\n] 3D4F6CA1 Between the Profiles: Another such Bias. Technology Acceptance Studies on Social Network Services between the profiles another such bias technology acceptance studies on social network services 2015 2015/08/02 10.1007/978-3-319-21383-5_12 international conference on human-computer interaction interact 43331058 19502[\n]

.......

.........

web snippets 2008 2008/05/04 10.1007/978-3-642-01344-7_13 international conference on web information systems and technologies webist 44F29802 19489
06FA3FFA Interactive 3D User Interfaces for Neuroanatomy Exploration interactive 3d user interfaces for neuroanatomy exploration 2009 internationa]
at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:241)
at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:356)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:137)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:120)
at scala.collection.Iterator$class.foreach(Iterator.scala:742)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foreach(CSVParser.scala:120)
at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foldLeft(CSVParser.scala:120)
at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:212)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.aggregate(CSVParser.scala:120)
at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:69)
at org.apache.spark.scheduler.Task.run(Task.scala:82)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException
16/03/23 14:01:03 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
^M[Stage 1:> (0 + 1) / 2]


For a small sample (<10,000 lines) of the data, I am not getting any error. But as soon as I go above more than 100,000 samples, I start getting the error.

I don't think the spark platform should output the actual data to stderr ever as it decreases the readability.","Decomposed Bugs:  
B: When running a command to read a large tab-separated file using Spark, the contents of the file are written to stderr, which decreases readability.  
B: The command `df = sqlContext.read.load(""temp.txt"", format=""csv"", header=""false"", inferSchema=""true"", delimiter=""\t"")` fails with a `TextParsingException` for files larger than 100,000 lines, as the parsed input length exceeds the maximum character limit (1,000,000) defined in the parser settings.  
B: The error message indicates that the line separator in the parser settings is set to `\n`, but the parsed content contains line separator characters, which may be causing the issue.  
B: For small sample files (<10,000 lines), the command executes without errors, but for larger files (>100,000 lines), the error occurs.  
B: The error stack trace includes an `ArrayIndexOutOfBoundsException`, which may be related to the parsing issue.",Major,FALSE,Over-analysis
Missing menu items,https://issues.apache.org/jira/browse/ASFSITE-27,"The redesign has lost several links from the navigation: 

Privacy Page 
Incubator 
Memorials 
Conferences","Decomposed Bugs:  
B: The redesign has lost the link to the Privacy Page from the navigation.  
B: The redesign has lost the link to the Incubator from the navigation.  
B: The redesign has lost the link to the Memorials from the navigation.  
B: The redesign has lost the link to the Conferences from the navigation.",Major,TRUE,
Site-to-Site Transit URI is inconsistent,https://issues.apache.org/jira/browse/NIFI-2028,"Site-to-Site client and server create provenance event at both end, and those have Transit URI in it to record how flow files are transferred. However, the URI formats are inconsistent among RAW vs HTTP. 

Test result as follows: 

These port numbers are configurable in nifi.properties 
3080: Web API port (nifi.web.http.port) 
3081: Site-to-Site RAW Socket port (nifi.remote.input.socket.port) 
Before Fix 

PUSH - RAW 

Client - SEND: nifi://localhost:3081/flow-file-uuid 
Server - RECEIVE: nifi://localhost:3081/flow-file-uuid 
PULL - RAW 

Client - RECEIVE: nifi://localhost:3081flow-file-uuid 
Server - SEND: nifi://localhost:3081/flow-file-uuid 
PUSH - HTTP 

Client - SEND: http://127.0.0.1:3080/nifi-api/flow-file-uuid 
Server - RECEIVE: nifi://127.0.0.1:57390 
PULL - HTTP 

Client - RECEIVE: http://127.0.0.1:3080/flow-file-uuid 
Server - SEND: nifi://127.0.0.1:57673 
Issues 

PULL - RAW, Client - RECEIVE: lacks '/' in between port and flow-file uuid 
RAW uses server's host and port on both end (by transit url prefix), HTTP should follow this rule 
HTTP transit uri looks like REST endpoint but it is not a real endpoint. It should be an actual endpoint URI 
RAW uses hostname, while HTTP uses IP address 
After Fix 

PUSH - RAW 

Client - SEND: nifi://localhost:3081/flow-file-uuid 
Server - RECEIVE: nifi://localhost:3081/flow-file-uuid 
PULL - RAW 

Client - RECEIVE: nifi://localhost:3081/flow-file-uuid 
Server - SEND: nifi://localhost:3081/flow-file-uuid 
PUSH - HTTP 

Client - SEND: http://localhost:3080/nifi-api/data-transfer/input-ports/$port-id/transactions/$tx-id/flow-files 
Server - RECEIVE: http://localhost:3080/nifi-api/data-transfer/input-ports/$port-id/transactions/$tx-id/flow-files 
PULL - HTTP 

Client - RECEIVE: http://localhost:3080/nifi-api/data-transfer/output-ports/$port-id/transactions/$tx-id/flow-files 
Server - SEND: http://localhost:3080/nifi-api/data-transfer/output-ports/$port-id/transactions/$tx-id/flow-files","Decomposed Bugs:  
B: PULL - RAW, Client - RECEIVE: lacks '/' in between port and flow-file uuid.  
B: RAW uses server's host and port on both ends (by transit URL prefix), but HTTP does not follow this rule.  
B: HTTP transit URI looks like a REST endpoint but is not a real endpoint. It should be an actual endpoint URI.  
B: RAW uses hostname, while HTTP uses IP address.",Major,FALSE,Over-decomposition
compat with pandas 0.20.0,https://issues.apache.org/jira/browse/ARROW-879,pandas changes the location of an import for ``DatetimeTZDtype``: http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#reorganization-of-the-library-privacy-changes,B: pandas changes the location of an import for ``DatetimeTZDtype``: http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#reorganization-of-the-library-privacy-changes.,Blocker,TRUE,
Unexpected subelement Name,https://issues.apache.org/jira/browse/AXIS2-1747,"When the generated stub processes response, it expects the nodes come in a specific order. If the order is different, exception is thrown. 

The code is in parse() methods of different result processing factory. 

For example, the code: 
/// process the ""first"" element ""IsPrivacyChanged"" 
if (reader.isStartElement() 
... 
""IsPrivacyChanged"").equals(reader.getName())) { 
read the value 
}else 
{ 
throw exception 
} 

///now it processes 2nd element. 
if (reader.isStartElement() 
... 
""Name"").equals(reader.getName())) { 
read the value 
}else 
{ 
throw exception 
} 

If ""Name"" came in before ""IsPrivacyChanged"", exception is thrown. 

A loop should be used, and any elements can appear in any order.","Decomposed Bugs:  
B: When the generated stub processes a response, it currently expects nodes to come in a specific order, and throws an exception if the order differs. If ""Name"" came in before ""IsPrivacyChanged"", exception is thrown.  
B: The parse() methods in different result processing factories should use a loop to allow elements to appear in any order.",Major,FALSE,Over-decomposition
query fails when calcite parse sql,https://issues.apache.org/jira/browse/KYLIN-1539,"There are two cases of query failure when calcite parses sql. However, both are solved by the same solution, just restarting kylin server.","Decomposed Bugs:  
B: There is a case of query failure when Calcite parses SQL. Restarting the Kylin server solves it.  
B: There is another case of query failure when Calcite parses SQL. Restarting the Kylin server solves it.",Major,TRUE,
System views design leads to bad user expirience.,https://issues.apache.org/jira/browse/IGNITE-12921,"Current implementation of system views has broken system behavior which is related with querying system views. 

Before 2.8 system views were available via SQL queries (if indexing is enabled). It did not depend on any configuration. 

After implementation of IGNITE-12145 system views available only if SqlViewExporterSpi is passed to IgniteConfiguration.setSystemViewExporterSpi(). Now, if an user configures some SystemViewExporterSpi then provided user configuration will rewrite default configuration and SqlViewExporterSpi won't be initialized. As result it is impossible to query system views and any query to the views fails with exception. This behavior is not obvious for the user. See tests below. 

The second problem is kind of design problem. System view is internal part of the system and should be available regardless of any exporter configuration (at least via SQL) such as it was implemented before 2.8 release. 

My suggestion is the following: we should remove SqlViewExporterSpi and configure all views on indexing module initialization. SqlViewExporterSPI also doesn't make sense because: 

it operates by some internal API (SchemaManager, GridKernalContext, IgniteH2Indexing). 
it doesn't allow to end user to add any new system view. 
Only thing that could be useful is a filtering. But it could be done with SQL. 

Reproducer of broken behavior: 

package org.apache.ignite.internal.processors.cache.metric; 

import org.apache.ignite.cache.query.SqlFieldsQuery; 
import org.apache.ignite.cluster.ClusterState; 
import org.apache.ignite.configuration.DataRegionConfiguration; 
import org.apache.ignite.configuration.DataStorageConfiguration; 
import org.apache.ignite.configuration.IgniteConfiguration; 
import org.apache.ignite.internal.IgniteEx; 
import org.apache.ignite.spi.systemview.jmx.JmxSystemViewExporterSpi; 
import org.apache.ignite.testframework.junits.common.GridCommonAbstractTest; 
import org.junit.Test; 

import java.util.HashSet; 
import java.util.List; 
import java.util.Set; 

import static java.util.Arrays.asList; 
import static org.apache.ignite.internal.processors.cache.index.AbstractSchemaSelfTest.queryProcessor; 

public class SystemViewTest extends GridCommonAbstractTest { 

private static boolean useDefaultSpi; 

/** {@inheritDoc} */ 
@Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception { 
IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName); 

cfg.setConsistentId(igniteInstanceName); 

cfg.setDataStorageConfiguration(new DataStorageConfiguration() 
.setDataRegionConfigurations( 
new DataRegionConfiguration().setName(""in-memory"").setMaxSize(100L * 1024 * 1024)) 
.setDefaultDataRegionConfiguration( 
new DataRegionConfiguration() 
.setPersistenceEnabled(true))); 

if (!useDefaultSpi) { 
// Configure user provided system view exporter SPI. 
cfg.setSystemViewExporterSpi(new JmxSystemViewExporterSpi()); 
} 

return cfg; 
} 

/** 
* Will executed succefully. 
*/ 
@Test 
public void testSystemViewWithDefaultSpi() throws Exception { 
useDefaultSpi = true; 

doTestSystemView(); 
} 

/** 
* Will fail with <code>Table ""VIEWS"" not found</code>. 
*/ 
@Test 
public void testSystemViewWithCustomSpi() throws Exception { 
useDefaultSpi = false; 

doTestSystemView(); 
} 

private void doTestSystemView() throws Exception { 
try (IgniteEx ignite = startGrid()) { 
ignite.cluster().state(ClusterState.ACTIVE); 

Set<String> cacheNames = new HashSet<>(asList(""cache-1"", ""cache-2"")); 

for (String name : cacheNames) 
ignite.getOrCreateCache(name); 

SqlFieldsQuery qry = new SqlFieldsQuery(""SELECT * FROM SYS.VIEWS""); 

List<List<?>> res = queryProcessor(ignite).querySqlFields(qry, true).getAll(); 

res.forEach(item -> log.info(""VIEW FOUND: "" + item)); 
} 
} 

}","Decomposed Bugs:  
B: After implementation of IGNITE-12145, system views are only available if SqlViewExporterSpi is passed to IgniteConfiguration.setSystemViewExporterSpi(). If a user configures some SystemViewExporterSpi, the provided user configuration rewrites the default configuration, and SqlViewExporterSpi won't be initialized. As a result, it is impossible to query system views, and any query to the views fails with an exception.  
B: System views are an internal part of the system and should be available regardless of any exporter configuration (at least via SQL), as it was implemented before the 2.8 release.  
B: SqlViewExporterSpi operates using internal APIs (SchemaManager, GridKernalContext, IgniteH2Indexing) and does not allow end users to add any new system views.  
B: The only useful feature of SqlViewExporterSpi is filtering, which could be done with SQL.  
B: The test `testSystemViewWithCustomSpi` fails with the error `Table ""VIEWS"" not found` when a custom SystemViewExporterSpi is configured.  
B: The test `testSystemViewWithDefaultSpi` executes successfully when the default SqlViewExporterSpi is used.",Critical,FALSE,Over-decomposition
TestFailpoints::test_failpoints crash in ARM build,https://issues.apache.org/jira/browse/IMPALA-11542,"Saw the crash in https://jenkins.impala.io/job/ubuntu-16.04-from-scratch-ARM/13 

In the ERROR log: 

Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30000,server=y,suspend=n 
impalad: /home/ubuntu/native-toolchain/source/llvm/llvm-5.0.1-asserts.src-p3/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldELF.cpp:400: void llvm::RuntimeDyldELF::resolveAArch64Relocation(const llvm::SectionEntry&, uint64_t, uint64_t, uint32_t, int64_t): Assertion `isInt<33>(Result) && ""overflow check failed for relocation""' failed. 
Minidump in thread [20013]exec-finstance (finst:1e4a0f56622f2a15:51c2970900000005) running query 1e4a0f56622f2a15:51c2970900000000, fragment instance 1e4a0f56622f2a15:51c2970900000005 
Wrote minidump to /home/ubuntu/Impala/logs/ee_tests/minidumps/impalad/de4830d8-009d-47f4-f14bb68a-f0d8cd4c.dmp 
In the INFO log: 

I0830 06:54:49.173234 11329 impala-beeswax-server.cc:516] query: Query { 
01: query (string) = ""SELECT STRAIGHT_JOIN *\n FROM alltypes t1\n JOIN /*+broadcast*/ alltypesagg t2 ON t1.id = t2.id\n WHERE t2.int_col < 1000"", 
03: configuration (list) = list<string>[10] { 
[0] = ""CLIENT_IDENTIFIE[...](273)"", 
[1] = ""TEST_REPLAN=1"", 
[2] = ""DISABLE_CODEGEN=False"", 
[3] = ""BATCH_SIZE=0"", 
[4] = ""NUM_NODES=0"", 
[5] = ""DISABLE_CODEGEN_ROWS_THRESHOLD=0"", 
[6] = ""MT_DOP=4"", 
[7] = ""ABORT_ON_ERROR=1"", 
[8] = ""DEBUG_ACTION=4:GETNEXT:MEM_LIMIT_EXCEEDED|COORD_BEFORE_EXEC_RPC:JITTER@100@0.3"", 
[9] = ""EXEC_SINGLE_NODE_ROWS_THRESHOLD=0"", 
}, 
04: hadoop_user (string) = ""ubuntu"", 
} 
... 
74: client_identifier (string) = ""failure/test_failpoints.py::TestFailpoints::()::test_failpoints[protocol:beeswax|table_format:seq/snap/block|exec_option:{'test_replan':1;'batch_size':0;'num_nodes':0;'disable_codegen_rows_threshold':0;'disable_codegen':False;'abort_on_error':1;'exec_sing"", 
... 
I0830 06:54:49.173739 11329 Frontend.java:1877] 1e4a0f56622f2a15:51c2970900000000] Analyzing query: SELECT STRAIGHT_JOIN * 
FROM alltypes t1 
JOIN /*+broadcast*/ alltypesagg t2 ON t1.id = t2.id 
WHERE t2.int_col < 1000 db: functional_seq_snap 
The client_identifier shows it's TestFailpoints::test_failpoints.","Decomposed Bugs:  
B: A crash occurred in the Jenkins job https://jenkins.impala.io/job/ubuntu-16.04-from-scratch-ARM/13 with the error log showing an assertion failure in `RuntimeDyldELF::resolveAArch64Relocation` due to an overflow check failure for relocation.  
B: A minidump was generated in thread [20013]exec-finstance (finst:1e4a0f56622f2a15:51c2970900000005) running query 1e4a0f56622f2a15:51c2970900000000, fragment instance 1e4a0f56622f2a15:51c2970900000005, and written to `/home/ubuntu/Impala/logs/ee_tests/minidumps/impalad/de4830d8-009d-47f4-f14bb68a-f0d8cd4c.dmp`.  
B: The INFO log shows a query execution with the SQL statement:  
```  
SELECT STRAIGHT_JOIN *  
FROM alltypes t1  
JOIN /*+broadcast*/ alltypesagg t2 ON t1.id = t2.id  
WHERE t2.int_col < 1000  
```  
and configuration settings including `CLIENT_IDENTIFIER`, `TEST_REPLAN=1`, `DISABLE_CODEGEN=False`, `BATCH_SIZE=0`, `NUM_NODES=0`, `DISABLE_CODEGEN_ROWS_THRESHOLD=0`, `MT_DOP=4`, `ABORT_ON_ERROR=1`, `DEBUG_ACTION=4:GETNEXT:MEM_LIMIT_EXCEEDED|COORD_BEFORE_EXEC_RPC:JITTER@100@0.3`, and `EXEC_SINGLE_NODE_ROWS_THRESHOLD=0`.  
B: The client_identifier in the INFO log indicates the crash occurred during the execution of `TestFailpoints::test_failpoints` in the test suite.",Critical,FALSE,Over-decomposition
Value::Record containing enums fail to validate when using namespaces in Schema,https://issues.apache.org/jira/browse/AVRO-3674,"Consider the following schema: 

{ 
""type"": ""record"", 
""name"": ""NamespacedMessage"", 
""namespace"": ""com.domain"", 
""fields"": [ 
{ 
""type"": ""record"", 
""name"": ""field_a"", 
""fields"": [ 
{ 
""name"": ""enum_a"", 
""type"": { 
""type"": ""enum"", 
""name"": ""EnumType"", 
""symbols"": [ 
""SYMBOL_1"", 
""SYMBOL_2"" 
], 
""default"": ""SYMBOL_1"" 
} 
}, 
{ 
""name"": ""enum_b"", 
""type"": ""EnumType"" 
} 
] 
} 
] 
} 
I might represent this in Rust using the following structs: 

#[derive(Serialize)] 
enum EnumType { 
#[serde(rename = ""SYMBOL_1"")] 
Symbol1, 
#[serde(rename = ""SYMBOL_2"")] 
Symbol2, 
} 

#[derive(Serialize)] 
struct FieldA { 
enum_a: EnumType, 
enum_b: EnumType, 
} 

#[derive(Serialize)] 
struct NamespacedMessage { 
field_a: FieldA, 
} 

let msg = NamespacedMessage { 
field_a: FieldA { 
enum_a: EnumType::Symbol2, 
enum_b: EnumType::Symbol1, 
}, 
}; 
and then serialize this into a `Value` using the following logic: 

let mut ser = Serializer::default(); 
let test_value: Value = msg.serialize(&mut ser).unwrap(); 
After serializing into `test_value` I would expect that `test_value.validate(&schema)` yields True. However this is not the case. 

I can work around it by removing the `namespace` definition from my schema which allows the validation to proceed. 

I believe the cause of schema validation failure is [this lookup failing](https://github.com/apache/avro/blob/release-1.11.1-rc1/lang/rust/avro/src/types.rs#L370) when schemas are utilized as the `Value` will not have a namespace associated with it. 

I believe this could be fixed by altering `validate_internal` to accept an optional namespace that is derived from the provided schema to `validate`. However, I'm not sure if this is an appropriate fix.","Decomposed Bugs:  
B: The schema validation fails when serializing a Rust struct into a `Value` and validating it against a schema that includes a namespace. The validation succeeds when the `namespace` definition is removed from the schema.  
B: The schema validation failure is caused by a lookup failing in the Avro Rust implementation when the `Value` does not have a namespace associated with it, as seen in [this code](https://github.com/apache/avro/blob/release-1.11.1-rc1/lang/rust/avro/src/types.rs#L370).  
B: A potential fix could involve modifying `validate_internal` to accept an optional namespace derived from the provided schema during validation. However, it is unclear if this is the appropriate solution.",Major,FALSE,Incorrect Interpretation of Solutions
user context not passed down in fabric_view_all_docs,https://issues.apache.org/jira/browse/COUCHDB-3232,"We omitted to pass user_ctx down in fabric_view_all_docs. Since auth has happened beforehand this hasn't been an obvious issue, but it matters for the _users db as that reacts differently based on the user. couchdb intentionally hides design documents in that database from non-admins and intentionally hides the user docs of other users. 

passing the user ctx down fixes both issues.","Decomposed Bugs:  
B: We omitted to pass user_ctx down in fabric_view_all_docs. Since auth has happened beforehand this hasn't been an obvious issue, but it matters for the _users db as that reacts differently based on the user. Passing the user ctx down could fix it.  
B: Couchdb intentionally hides design documents in that database from non-admins and intentionally hides the user docs of other users. Passing the user ctx down could fix it.",Major,TRUE,
HBase server doesn't preserve SASL sequence number on the network,https://issues.apache.org/jira/browse/HBASE-22492,"When auth-conf is enabled on RPC, the server encrypt response in setReponse() using saslServer. The generated cryptogram included a sequence number manage by saslServer. But then, when the response is sent over the network, the sequence number order is not preserved. 

The client receives reply in the wrong order, leading to a log message from DigestMD5Base: 

sasl:1481 - DIGEST41:Unmatched MACs 

Then the message is discarded, leading the client to a timeout. 

I propose a fix here: https://github.com/sbarnoud/hbase-release/commit/ce9894ffe0e4039deecd1ed51fa135f64b311d41 

It seems that any HBase 1.x is affected. 

This part of code has been fully rewritten in HBase 2.x, and i haven't do the analysis on HBase 2.x which may be affected. 



Here, an extract of client log that i added to help me to understand: 

 

2019-05-28 12:53:48,644 DEBUG [Default-IPC-NioEventLoopGroup-1-32] NettyRpcDuplexHandler:80 - callId: 5846 /192.163.201.65:58870 -> dtltstap004.fr.world.socgen/192.163.201.72:16020 

2019-05-28 12:53:48,651 INFO [Default-IPC-NioEventLoopGroup-1-18] NioEventLoop:101 - SG: Channel ready to read 1315913615 unsafe 1493023957 /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 

2019-05-28 12:53:48,651 INFO [Default-IPC-NioEventLoopGroup-1-18] SaslUnwrapHandler:78 - SG: after unwrap:46 -> 29 for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 seqNum 150 

2019-05-28 12:53:48,652 DEBUG [Default-IPC-NioEventLoopGroup-1-18] NettyRpcDuplexHandler:192 - callId: 5801 received totalSize:25 Message:20 scannerSize:(null)/192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 

2019-05-28 12:53:48,652 INFO [Default-IPC-NioEventLoopGroup-1-18] sasl:1481 - DIGEST41:Unmatched MACs 

2019-05-28 12:53:48,652 WARN [Default-IPC-NioEventLoopGroup-1-18] SaslUnwrapHandler:70 - Sasl error (probably invalid MAC) detected for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 saslClient @4ac31121 ctx @14fb001d msg @140313192718406 len 118 data:1c^G?^P?3??h?k??????""??x?$^_??^D;^]7^Es??Em?c?w^R^BL?????????x??omG?z?I???45}???dE?^\^S>D?^????/4f?^^?? ?^E????d?????????D?kM^@^A^@^@^@? readerIndex 118 writerIndex 118 seqNum 152 
We can see that the client unwraps the Sasl message with sequence number 152 before sequence number 151 and fails with the unmatched MAC. 



I opened a case to Oracle because we should had an error (and not the message ignored). That's because the JDK doesn't controls integrity in the right way. 

https://github.com/openjdk/jdk/blob/master/src/java.security.sasl/share/classes/com/sun/security/sasl/digest/DigestMD5Base.java 

The actual JDK controls the HMac before the sequence number and hides the real error (bad sequence number) because SASL is stateful. The JDK should check FIRST the sequence number and THEN the HMac. 

When (and if) the JDK will be patched, and accordingly to https://www.ietf.org/rfc/rfc2831.txt , we will get an exception in that case instead of having the message ignored.","Decomposed Bugs:  
B: When auth-conf is enabled on RPC, the server encrypts the response in setResponse() using saslServer, but the sequence number managed by saslServer is not preserved when the response is sent over the network. This causes the client to receive replies in the wrong order.  
B: The client logs an error message ""DIGEST41:Unmatched MACs"" when receiving replies in the wrong order, leading to the message being discarded and the client timing out.  
B: The JDK does not control integrity correctly, as it checks the HMac before the sequence number, hiding the real error (bad sequence number). The JDK should check the sequence number first and then the HMac.  
B: The issue affects HBase 1.x, and the code has been fully rewritten in HBase 2.x, but it is unclear if HBase 2.x is also affected.  
B: The JDK should throw an exception when the sequence number is incorrect, as per RFC 2831, instead of ignoring the message.",Major,FALSE,Over-decomposition
"Website ""site"" section links all inaccessible",https://issues.apache.org/jira/browse/KARAF-743,"The links pointed to on the ""site"" page: http://karaf.apache.org/site.html are all broken; I'm not sure why as the site/trunk repository does indeed have pages for all the links on the site page. This also affects the Apache ""Privacy Policy"" link at the very bottom of all the Karaf website pages.","Decomposed Bugs:  
B: The links pointed to on the ""site"" page: http://karaf.apache.org/site.html are all broken.  
B: The Apache ""Privacy Policy"" link at the very bottom of all the Karaf website pages is broken.",Minor,TRUE,
Datanode#checkSecureConfig should allow SASL and privileged HTTP,https://issues.apache.org/jira/browse/HDFS-13081,"Datanode#checkSecureConfig currently check the following to determine if secure datanode is enabled. 

The server has bound to privileged ports for RPC and HTTP via SecureDataNodeStarter. 
The configuration enables SASL on DataTransferProtocol and HTTPS (no plain HTTP) for the HTTP server. 
Authentication of Datanode RPC server can be done either via SASL handshake or JSVC/privilege RPC port. 
This guarantees authentication of the datanode RPC server before a client transmits a secret, such as a block access token. 

Authentication of the HTTP server can also be done either via HTTPS/SSL or JSVC/privilege HTTP port. This guarantees authentication of datandoe HTTP server before a client transmits a secret, such as a delegation token. 

This ticket is open to allow privileged HTTP as an alternative to HTTPS to work with SASL based RPC protection. 

cc: cnauroth , daryn, jnpandey for additional feedback.","Decomposed Bugs:  
B: Datanode#checkSecureConfig currently checks if the server has bound to privileged ports for RPC and HTTP via SecureDataNodeStarter.  
B: Datanode#checkSecureConfig currently checks if the configuration enables SASL on DataTransferProtocol and HTTPS (no plain HTTP) for the HTTP server.  
B: Datanode RPC server authentication can be done either via SASL handshake or JSVC/privileged RPC port, guaranteeing authentication before a client transmits a secret, such as a block access token.  
B: Datanode HTTP server authentication can be done either via HTTPS/SSL or JSVC/privileged HTTP port, guaranteeing authentication before a client transmits a secret, such as a delegation token.  
B: This ticket is open to allow privileged HTTP as an alternative to HTTPS to work with SASL-based RPC protection.",Major,TRUE,
RTF parser misses text content,https://issues.apache.org/jira/browse/TIKA-1713,"We have a lot of Outlook msg files that have RTF body content. Tika is not finding any text within these messages. It appears to be a mixture of RTF and HTML. 

I've extracted an example RTF body (see attachment) for use with the following test case: 

ByteArrayOutputStream bytes = new ByteArrayOutputStream() 
rtfParser.parse( 
this.class.getResourceAsStream(""/problems/no-text.rtf""), 
new EmbeddedContentHandler(new BodyContentHandler(bytes)), 
new Metadata(), new ParseContext() 
); 
assertTrue(""Document is missing required text"", bytes.toByteArray().length > 0)","Decomposed Bugs:  
B: Tika is not finding any text within Outlook msg files that have RTF body content.  
B: The RTF body content in the Outlook msg files appears to be a mixture of RTF and HTML.  
B: The test case using `rtfParser.parse` fails to extract text from the provided RTF body, as `bytes.toByteArray().length` is 0.",Major,FALSE,Over-decomposition
NiFi Registry fails to push flow to GIT using SSH,https://issues.apache.org/jira/browse/NIFI-11927,"I have been successfully running NiFi Registry version 1.20.0 (and others before it) for about a year now. Now I needed to switch server that Registry runs on, since the old one was outdated, and decided to set up NiFi Registry version 1.22.0. 

The setup worked great and NiFi seems to have been running properly. However I recently noticed that it wasn't pushing anything to GIT. The changes are made, new versions are committed, they show up both in Registry GUI as well as the NiFi instance, but when I try to fetch some version it will not allow me giving me the following error: 



I started investigating the issue and noticed that the GIT repository that NiFi Registry was using was several versions ahead of origin, no version got pushed to the GIT repo. 

I investigated the logs and noticed following messages show up when I make new version in NiFi and commit it to registry: 

2023-08-10 07:52:03,317 WARN [NiFi Registry Web Server-18] javax.persistence.spi javax.persistence.spi::No valid providers found. 
2023-08-10 07:52:05,120 INFO [GitFlowMetaData Push thread] o.a.s.c.u.s.e.EdDSASecurityProviderRegistrar getOrCreateProvider(EdDSA) created instance of net.i2p.crypto.eddsa.EdDSASecurityProvider 
2023-08-10 07:52:05,326 INFO [GitFlowMetaData Push thread] o.a.s.c.i.DefaultIoServiceFactoryFactory No detected/configured IoServiceFactoryFactory; using Nio2ServiceFactoryFactory 
2023-08-10 07:52:05,351 ERROR [GitFlowMetaData Push thread] o.a.n.r.p.flow.git.GitFlowMetaData Failed to push commits to origin due to org.eclipse.jgit.api.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly 
org.eclipse.jgit.api.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly 
at org.eclipse.jgit.api.PushCommand.call(PushCommand.java:147) 
at org.apache.nifi.registry.provider.flow.git.GitFlowMetaData.lambda$startPushThread$1(GitFlowMetaData.java:299) 
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) 
at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) 
at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) 
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) 
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) 
at java.base/java.lang.Thread.run(Thread.java:833) 
Caused by: org.eclipse.jgit.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly 
at org.eclipse.jgit.transport.TransportGitSsh$SshPushConnection.<init>(TransportGitSsh.java:382) 
at org.eclipse.jgit.transport.TransportGitSsh.openPush(TransportGitSsh.java:159) 
at org.eclipse.jgit.transport.PushProcess.execute(PushProcess.java:127) 
at org.eclipse.jgit.transport.Transport.push(Transport.java:1384) 
at org.eclipse.jgit.api.PushCommand.call(PushCommand.java:137) 
... 7 common frames omitted 
Caused by: java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()' 
at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:189) 
at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:142) 
at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:99) 
at org.eclipse.jgit.transport.sshd.SshdSessionFactory.getSession(SshdSessionFactory.java:235) 
at org.eclipse.jgit.transport.sshd.SshdSessionFactory.getSession(SshdSessionFactory.java:1) 
at org.eclipse.jgit.transport.SshTransport.getSession(SshTransport.java:107) 
at org.eclipse.jgit.transport.TransportGitSsh$SshPushConnection.<init>(TransportGitSsh.java:358) 
... 11 common frames omitted 
I think the line Caused by: java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()' is what's causing the issue with pushing to GIT. This does not show up on the old Registry instance when I push to that. 



I checked the setup and compared it to the previous version running on old server. They have exactly the same configuration, save for new one running on Java 17 not 11 (although I tried running it on 11 too). 



The SSH keys are valid and recognized by our Gitlab instance, I can push manually as the nifi user, there are literally no differences between the configuration on one server compared to another. I even checked the configuration inside .git folder in the flow_storage directory, it was the same as on the other server. The ~/.ssh folder for nifi user has similar data between two servers, both have keys and known hosts set up, with right permissions, etc. 

I decided to try running version 1.20.0 of registry on new server, and it seems to have worked immediately, without any issue, it manages to push to GIT on its own, with no changes to config. I tested version 1.23 as well and it had the same issue, I haven't tested 1.21.0 though. 



For more information, here is the structure I have: 

©À©¤©¤ nifi-registry-1.20.0 
©¦ ©À©¤©¤ bin 
©¦ ©À©¤©¤ conf 
©¦ ©À©¤©¤ docs 
©¦ ©À©¤©¤ ext 
©¦ ©À©¤©¤ lib 
©¦ ©À©¤©¤ logs 
©¦ ©À©¤©¤ run 
©¦ ©¸©¤©¤ work 
©À©¤©¤ nifi-registry-1.23.0 
©¦ ©À©¤©¤ bin 
©¦ ©À©¤©¤ conf 
©¦ ©À©¤©¤ docs 
©¦ ©À©¤©¤ ext 
©¦ ©À©¤©¤ lib 
©¦ ©À©¤©¤ logs 
©¦ ©À©¤©¤ run 
©¦ ©¸©¤©¤ work 
©¸©¤©¤ nifi-registry-files 
©À©¤©¤ authorization-files 
©¦ ©À©¤©¤ authorizations.xml 
©¦ ©À©¤©¤ authorizers.xml 
©¦ ©À©¤©¤ login-identity-providers.xml 
©¦ ©¸©¤©¤ users.xml 
©À©¤©¤ certificate-files 
©¦ ©À©¤©¤ keystore.jks 
©¦ ©¸©¤©¤ truststore.jks 
©À©¤©¤ configuration-files 
©¦ ©À©¤©¤ providers.xml 
©¦ ©¸©¤©¤ registry-aliases.xml 
©À©¤©¤ database-drivers 
©¦ ©¸©¤©¤ mariadb-java-client-2.7.4.jar 
©À©¤©¤ extension-bundles 
©¸©¤©¤ flow-storage 
©¸©¤©¤ Buckets... 


Contents of the providers.xml: 

<providers> 
<flowPersistenceProvider> 
<class&amp;amp;gt;org.apache.nifi.registry.provider.flow.git.GitFlowPersistenceProvider</class&amp;amp;gt; 
<property name=""Flow Storage Directory"">/disk1/nifi-registry/nifi-registry-files/flow-storage</property> 
<property name=""Remote To Push"">origin</property> 
<property name=""Remote Access User""></property> 
<property name=""Remote Access Password""></property> 
<property name=""Remote Clone Repository""></property> 
</flowPersistenceProvider> 
<extensionBundlePersistenceProvider> 
<class&amp;amp;gt;org.apache.nifi.registry.provider.extension.FileSystemBundlePersistenceProvider</class&amp;amp;gt; 
<property name=""Extension Bundle Storage Directory"">/disk1/nifi-registry/nifi-registry-files/extension-bundles</property> 
</extensionBundlePersistenceProvider> 
</providers> 


Contents of the .git/config file in the flow_storage directory: 

[core] 
repositoryformatversion = 0 
filemode = true 
bare = false 
logallrefupdates = true 
[http] 
sslVerify = false 
proxy = http://<PROXY_URL>:3128 
[https] 
sslVerify = false 
proxy = http://<PROXY_URL>:3128 
[user] 
name = nifi_user 
email = OMITTED_FOR_PRIVACY 
[remote ""origin""] 
url = git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git 
fetch = +refs/heads/*:refs/remotes/origin/* 
[branch ""master""] 
remote = origin 
merge = refs/heads/master","Decomposed Bugs:  
B: NiFi Registry version 1.22.0 is not pushing changes to the GIT repository, despite changes being committed and visible in the Registry GUI and NiFi instance.  
B: The GIT repository used by NiFi Registry is several versions ahead of the origin, and no versions are being pushed to the GIT repo.  
B: The logs show the following error when attempting to push changes to GIT: `org.eclipse.jgit.api.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly`.  
B: The error `java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()'` is causing the issue with pushing to GIT.  
B: The issue does not occur in NiFi Registry version 1.20.0, which works correctly on the new server with the same configuration.  
B: The issue persists in NiFi Registry version 1.23.0, but version 1.21.0 has not been tested.  
B: The SSH keys and configuration are valid and recognized by the GitLab instance, and manual pushes work as expected.  
B: The `.git/config` file in the flow_storage directory contains proxy settings and remote origin configuration, which are identical to the working setup on the old server.  
B: The `providers.xml` file contains the configuration for the GitFlowPersistenceProvider, including the flow storage directory and remote push settings.",Major,FALSE,Over-decomposition
BasicHttpCacheStorage leaking variant keys in root response's variantMap,https://issues.apache.org/jira/browse/HTTPCLIENT-2284,"BasicHttpCacheStorage has a memory leak in the root response's variantMap. When a variant cached entry is evicted due to CacheMap being too large, the root cache entry does not remove the variant key in its variantMap. This is a memory leak that can grow the variantMap indefinitely, or until the root entry get's evicted itself. 

Simplified testcase: 

A request is being sent from a client that contains a header ""x-my-variant"" with a hash of the current timestamp. 
The server responds 200, with a cacheable response. The response Vary's on ""x-my-variant"" 
These requests repeat, causing: 
The ""root"" CacheEntry to be kept in CacheMap 
The oldest variant CacheEntry to be evicted due to the CacheMap size limit 
However the ""root"" CacheEntry never removes the evicted variant keys from the variantMap","Decomposed Bugs:  
B: BasicHttpCacheStorage has a memory leak in the root response's variantMap. When a variant cached entry is evicted due to CacheMap being too large, the root cache entry does not remove the variant key in its variantMap.  
B: The memory leak in BasicHttpCacheStorage can grow the variantMap indefinitely, or until the root entry gets evicted itself.  
B: A request is being sent from a client that contains a header ""x-my-variant"" with a hash of the current timestamp. The server responds 200, with a cacheable response. The response Vary's on ""x-my-variant"".  
B: Repeated requests cause the ""root"" CacheEntry to be kept in CacheMap, while the oldest variant CacheEntry is evicted due to the CacheMap size limit.  
B: The ""root"" CacheEntry never removes the evicted variant keys from the variantMap.",Minor,FALSE,Over-decomposition
Update the ubuntu version in the build instruction,https://issues.apache.org/jira/browse/HADOOP-17635,"In BUILDING.txt 

Installing required packages for clean install of Ubuntu 14.04 LTS Desktop: 
Ubuntu 14 is already EoL, should be updated to 18 or 20.","B: Ubuntu 14 is already EoL, should be updated to 18 or 20 in BUILDING.txt.",Major,TRUE,
[Threat Modeling] Drillbit may be spoofed by an attacker and this may lead to data being written to the attacker's target instead of Drillbit,https://issues.apache.org/jira/browse/DRILL-5582,"Consider the scenario: 
Alice has a drillbit (my.drillbit.co) with plain and kerberos authentication enabled containing important data. Bob, the attacker, attempts to spoof the connection and redirect it to his own drillbit (fake.drillbit.co) with no authentication setup. 

When Alice is under attack and attempts to connect to her secure drillbit, she is actually authenticating against Bob's drillbit. At this point, the connection should have failed due to unmatched configuration. However, the current implementation will return SUCCESS as long as the (spoofing) drillbit has no authentication requirement set. 

Currently, the drillbit <- to -> drill client connection accepts the lowest authentication configuration set on the server. This leaves unsuspecting user vulnerable to spoofing.","Decomposed Bugs:  
B: When Alice attempts to connect to her secure drillbit (my.drillbit.co) with plain and kerberos authentication enabled, the connection is redirected to Bob's spoofed drillbit (fake.drillbit.co) with no authentication setup. The connection should fail due to unmatched configuration but currently returns SUCCESS.  
B: The drillbit <- to -> drill client connection currently accepts the lowest authentication configuration set on the server, leaving users vulnerable to spoofing attacks.",Minor,TRUE,
org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl uses internal class com.sun.org.apache.xerces.internal.impl.dv.util.Base64,https://issues.apache.org/jira/browse/NETBEANS-5427,"Netbeans 12.3 fails to deploy on Tomcat when running under JDK 16 

SEVERE [org.openide.util.RequestProcessor]: Error in RequestProcessor org.openide.nodes.AsynchChildrenSEVERE [org.openide.util.RequestProcessor]: Error in RequestProcessor org.openide.nodes.AsynchChildrenjava.lang.IllegalAccessError: class org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl (in unnamed module @0x145afd71) cannot access class com.sun.org.apache.xerces.internal.impl.dv.util.Base64 (in module java.xml) because module java.xml does not export com.sun.org.apache.xerces.internal.impl.dv.util to unnamed module @0x145afd71 at org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl.run(TomcatManagerImpl.java:533) at org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl.list(TomcatManagerImpl.java:372) at org.netbeans.modules.tomcat5.deploy.TomcatManager.modules(TomcatManager.java:718) at org.netbeans.modules.tomcat5.deploy.TomcatManager.getRunningModules(TomcatManager.java:539) at org.netbeans.modules.tomcat5.ui.nodes.TomcatWebModuleChildrenFactory.createKeys(TomcatWebModuleChildrenFactory.java:107) at org.openide.nodes.AsynchChildren.run(AsynchChildren.java:190) at org.openide.util.RequestProcessor$Task.run(RequestProcessor.java:1418) at org.netbeans.modules.openide.util.GlobalLookup.execute(GlobalLookup.java:45) at org.openide.util.lookup.Lookups.executeWith(Lookups.java:278)[catch] at org.openide.util.RequestProcessor$Processor.run(RequestProcessor.java:2033)","Decomposed Bugs:  
B: Netbeans 12.3 fails to deploy on Tomcat when running under JDK 16 due to java.lang.IllegalAccessError: class org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl (in unnamed module @0x145afd71) cannot access class com.sun.org.apache.xerces.internal.impl.dv.util.Base64 (in module java.xml) because module java.xml does not export com.sun.org.apache.xerces.internal.impl.dv.util to unnamed module @0x145afd71.  
B: The error occurs at org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl.run(TomcatManagerImpl.java:533) when attempting to list modules.  
B: The error propagates through org.netbeans.modules.tomcat5.deploy.TomcatManager.getRunningModules(TomcatManager.java:539) and affects the creation of keys in TomcatWebModuleChildrenFactory.createKeys(TomcatWebModuleChildrenFactory.java:107).  
B: The error is caught in org.openide.util.RequestProcessor$Processor.run(RequestProcessor.java:2033) during the execution of AsynchChildren.run(AsynchChildren.java:190).",Major,FALSE,Over-analysis
FileExistsException: Destination .. already exists when DiskFileItem.write was given an existing file,https://issues.apache.org/jira/browse/FILEUPLOAD-293,"Since 1.4, where FILEUPLOAD-248 was shipped, passing an existing file to DiskFileItem.write will cause an FileExistsException with the message ""Destination FILE already exist"", this prevents us from upgrading to 1.4 from 1.3.3. 



2019-02-20 01:12:56,504 http-nio-2990-exec-3 ERROR [|5ccb9b99-a96f-42ba-ad01-ac278516e1a4|] [IssueAttachmentsResource.privacy-safe] Error saving attachment 
org.apache.commons.io.FileExistsException: Destination '/buildeng/bamboo-agent-home/xml-data/build-dir/CLOUDRELEASE-AGILEWD15421-FT18/jira-test-runner-jira/target/cargo/configurations/tomcat9x/temp/attachment-3404789743778163937.tmp' already exists 
{{ at org.apache.commons.io.FileUtils.moveFile(FileUtils.java:3001)}} 
{{ at org.apache.commons.fileupload.disk.DiskFileItem.write(DiskFileItem.java:405)}} 
{{ at com.atlassian.plugins.rest.common.multipart.fileupload.CommonsFileUploadFilePart.write(CommonsFileUploadFilePart.java:49)}} 
{{ at com.atlassian.jira.rest.v2.issue.IssueAttachmentsResource.getFileFromFilePart(IssueAttachmentsResource.java:175)}} 
... 



Apache Felix also ran into the same bug: 
https://issues.apache.org/jira/browse/FELIX-6037","Decomposed Bugs:  
B: Since 1.4, passing an existing file to DiskFileItem.write causes a FileExistsException with the message ""Destination FILE already exists"", preventing upgrading from 1.3.3 to 1.4.  
B: Apache Felix encountered the same bug, as reported in https://issues.apache.org/jira/browse/FELIX-6037.",Major,FALSE,Lacking key information
Fix serialization of structs containing Fixed fields,https://issues.apache.org/jira/browse/AVRO-3631,"Consider the following minimal Avro Schema: 

{ 
""type"": ""record"", 
""name"": ""TestStructFixedField"", 
""fields"": [ 
{ 
""name"": ""field"", 
""type"": { 
""name"": ""field"", 
""type"": ""fixed"", 
""size"": 6 
} 
} 
] 
} 
In Rust, I might represent this schema with the following struct: 

#[derive(Debug, Serialize, Deserialize)] 
struct TestStructFixedField { 
field: [u8; 6] 
} 
I would then expect to be able to use `apache_avro::to_avro_datum()` to convert an instance of `TestStructFixedField` into an `Vec<u8>` using an instance of `Schema` initialized from the schema listed above. 

However, this fails because the `Value` produced by `apache_avro::to_value()` represents `field` as an `Value::Array<Value::Int>` rather than a `Value::Fixed<6, Vec<u8>` which does not pass schema validation. 

I believe that there are two options to fix this: 
1. Allow Value::Array<Vec<Value::Int>> to pass validation if the array has the expected length, and none of the contents of the array are out-of-range for u8. If we go down this route, the implementation of `to_avro_datum()` will have to take care of converting Value::Int to u8 when converting into bytes. 
2. Update `apache_avro::to_value()` such that fixed length arrays are converted into `Value::Fixed<N, Vec<u8>>` rather than `Value::Array`.","Decomposed Bugs:  
B: The `Value` produced by `apache_avro::to_value()` represents `field` as a `Value::Array<Value::Int>` instead of a `Value::Fixed<6, Vec<u8>>`, causing schema validation to fail.  
B: Option 1: Allow `Value::Array<Vec<Value::Int>>` to pass validation if the array has the expected length and none of the contents are out-of-range for `u8`. The implementation of `to_avro_datum()` would need to convert `Value::Int` to `u8` when converting into bytes.  
B: Option 2: Update `apache_avro::to_value()` to convert fixed-length arrays into `Value::Fixed<N, Vec<u8>>` instead of `Value::Array`.",Major,FALSE,Incorrect Interpretation of Solutions
Enabling ranger plugin config should modify dependent configs,https://issues.apache.org/jira/browse/AMBARI-9626,"Changes required for enabling Ranger plugin 
HDFS 

Property Value File 
dfs.permissions.enabled true hdfs-site.xml 
HIVE 

Property Value File 
hive.security.authorization.enabled true hive-site.xml 
hive.security.authorization.manager com.xasecure.authorization.hive.authorizer.XaSecureHiveAuthorizerFactory hiveserver2-site.xml 
hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator hiveserver2-site.xml 
hive.conf.restricted.list Must contain all elements of hive.security.authorization.enabled, hive.security.authorization.manager,hive.security.authenticator.manager hive-site.xml 
HBASE 

Property Value File 
hbase.security.authorization true hbase-site.xml 
hbase.coprocessor.master.classes Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor and add if not present hbase-site.xml 
hbase.coprocessor.region.classes Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor hbase-site.xml 
hbase.rpc.protection privacy hbase-site.xml 
KNOX 
Replace instances of AclsAuthz with XASecurePDPKnox in topology.xml 

STORM 

Property Value File 
nimbus.authorizer com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer storm.yaml 
Note that nimbus.authorizer should be added only when the cluster is already Kerberized; having this property in a non-Kerberized cluster causes Storm to fail. 

Changes required for disabling Ranger plugin 
HDFS 

Property Value File 
HIVE 

Property Value File 
hive.security.authorization.manager org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory hiveserver2-site.xml 
hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator hiveserver2-site.xml 
HBASE 

Property Value File 
hbase.coprocessor.master.classes Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor hbase-site.xml 
hbase.coprocessor.region.classes Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor hbase-site.xml 
hbase.rpc.protection authentication hbase-site.xml 
KNOX 
Replace instance of XASecurePDPKnox with AclsAuthz in topology.xml 

STORM 

Property Value File 
nimbus.authorizer backtype.storm.security.auth.authorizer.SimpleACLAuthorizer com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer storm.yaml 
Note that nimbus.authorizer should be added only when the cluster is already Kerberized; having this property in a non-Kerberized cluster causes Storm to fail.","Decomposed Bugs:  
B: For HDFS, the property `dfs.permissions.enabled` must be set to `true` in `hdfs-site.xml`.  
B: For HIVE, the property `hive.security.authorization.enabled` must be set to `true` in `hive-site.xml`.  
B: For HIVE, the property `hive.security.authorization.manager` must be set to `com.xasecure.authorization.hive.authorizer.XaSecureHiveAuthorizerFactory` in `hiveserver2-site.xml`.  
B: For HIVE, the property `hive.security.authenticator.manager` must be set to `org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator` in `hiveserver2-site.xml`.  
B: For HIVE, the `hive.conf.restricted.list` in `hive-site.xml` must contain all elements of `hive.security.authorization.enabled`, `hive.security.authorization.manager`, and `hive.security.authenticator.manager`.  
B: For HBASE, the property `hbase.security.authorization` must be set to `true` in `hbase-site.xml`.  
B: For HBASE, the property `hbase.coprocessor.master.classes` in `hbase-site.xml` must replace `org.apache.hadoop.hbase.security.access.AccessController` with `com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor` and add it if not present.  
B: For HBASE, the property `hbase.coprocessor.region.classes` in `hbase-site.xml` must replace `org.apache.hadoop.hbase.security.access.AccessController` with `com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor`.  
B: For HBASE, the property `hbase.rpc.protection` must be set to `privacy` in `hbase-site.xml`.  
B: For KNOX, instances of `AclsAuthz` must be replaced with `XASecurePDPKnox` in `topology.xml`.  
B: For STORM, the property `nimbus.authorizer` must be set to `com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer` in `storm.yaml`. Note that this property should only be added when the cluster is already Kerberized.  
B: For HIVE, when disabling the Ranger plugin, the property `hive.security.authorization.manager` must be set to `org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory` in `hiveserver2-site.xml`.  
B: For HIVE, when disabling the Ranger plugin, the property `hive.security.authenticator.manager` must be set to `org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator` in `hiveserver2-site.xml`.  
B: For HBASE, when disabling the Ranger plugin, the property `hbase.coprocessor.master.classes` in `hbase-site.xml` must remove `com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor`.  
B: For HBASE, when disabling the Ranger plugin, the property `hbase.coprocessor.region.classes` in `hbase-site.xml` must remove `com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor`.  
B: For HBASE, when disabling the Ranger plugin, the property `hbase.rpc.protection` must be set to `authentication` in `hbase-site.xml`.  
B: For KNOX, when disabling the Ranger plugin, instances of `XASecurePDPKnox` must be replaced with `AclsAuthz` in `topology.xml`.  
B: For STORM, when disabling the Ranger plugin, the property `nimbus.authorizer` must be set to `backtype.storm.security.auth.authorizer.SimpleACLAuthorizer` in `storm.yaml`. Note that this property should only be added when the cluster is already Kerberized.",Major,FALSE,Over-decomposition
KnoxSSO broken on recent Chrome browsers (version > 80),https://issues.apache.org/jira/browse/KNOX-2387,"Google chrome changed the default behavior of SameSite parameter in Set-Cookie header from None to Lax. This causes partial breakage of Knox SSO. 

Details about Chrome browser feature - https://www.chromestatus.com/feature/5088147346030592 

How it affects - https://support.okta.com/help/s/article/FAQ-How-Chrome-80-Update-for-SameSite-by-default-Potentially-Impacts-Your-Okta-Environment","Decomposed Bugs:  
B: Google Chrome changed the default behavior of the SameSite parameter in the Set-Cookie header from None to Lax.  
B: The change in the SameSite parameter behavior causes partial breakage of Knox SSO.",Major,FALSE,Over-decomposition
"Infinite loop on server disconnect",https://issues.apache.org/jira/browse/HTTPCORE-528,"I am seeing HTTP NIO client get into an infinite loop after the server disconnections the connection. See the log output attached; note some content removed for privacy. 



Note that the HttpAsyncClient has returned the response and the application has completely processed it. The client maintains the connection, as expected, since the response included `Keep-Alive: timeout=5`. Five seconds after everything is complete, the server closes the TCP connection. The client reacts accordingly: the selector wakes up, does a read of -1 bytes, closes the session and sets a 1 second timeout to close the connection in. 



The infinite loop occurs because the selector.select() call constantly returns in AbstractIOReactor.execute() 

readyCount == 1, so events are processed 

processEvent() notes the key is readable and calls: 

session.resetLastRead() 

readable(key); 

Because resetLastRead() is constantly updated, the 1 second timeout is never reached and AbstractIOReactor.timeoutCheck() can never call sessionTimedOut() or close the connection. 



Note the entire time this is happening, netstat shows the connection is in CLOSE_WAIT state. The infinite loop continues until the OS keepalive timeout is reached and the connection is cleaned by the OS. 

I am not sure if this epoll / selector behavior is expected or not. However, I have replicated this issue in multiple environments. It seems like the async client should handle this by detecting the condition and closing the connection. 



Other notes from this infinite loop state: 

SSLIOSession.updateEventMask() never closes the session either since the state remains `CLOSING` 

AbstractIODispatch.inputReady() does not read any data from the connection since ssliosession.isAppInputReady() evaluates to false. 

SelectionKeyImpl.interestOps remains 1 (`OP_READ`)","Decomposed Bugs:  
B: The HTTP NIO client enters an infinite loop after the server disconnects the connection. The selector.select() call in AbstractIOReactor.execute() constantly returns with readyCount == 1, causing events to be processed repeatedly.  
B: The session.resetLastRead() method is constantly updated, preventing the 1-second timeout from being reached, and AbstractIOReactor.timeoutCheck() cannot call sessionTimedOut() or close the connection.  
B: The SSLIOSession.updateEventMask() method does not close the session because the state remains `CLOSING`.  
B: AbstractIODispatch.inputReady() does not read any data from the connection because ssliosession.isAppInputReady() evaluates to false.  
B: The SelectionKeyImpl.interestOps remains set to 1 (`OP_READ`), contributing to the infinite loop behavior.",Major,FALSE,Over-decomposition
Improve the way job history files are managed,https://issues.apache.org/jira/browse/MAPREDUCE-323,"Today all the jobhistory files are dumped in one job-history folder. This can cause problems when there is a need to search the history folder (job-recovery etc). It would be nice if we group all the jobs under a user folder. So all the jobs for user amar will go in history-folder/amar/. Jobs can be categorized using various features like jobid, date, jobname etc but using username will make the search much more efficient and also will not result into namespace explosion.","Decomposed Bugs:  
B: All jobhistory files are currently dumped in one job-history folder, which can cause problems when searching the history folder (e.g., job-recovery).  
B: It would be beneficial to group all jobs under a user folder (e.g., history-folder/amar/) to improve search efficiency and avoid namespace explosion.  
B: Jobs can be categorized using features like jobid, date, or jobname, but using the username for categorization would make the search process more efficient.",Critical,FALSE,Over-decomposition
Updated yajl-ruby ~> 1.3.1,https://issues.apache.org/jira/browse/SAMZA-1582,"Copying the reminder from Apache Security team:

Sign in
gstein,

We found a potential security vulnerability in a repository which you have been granted security alert access.

!https://ci4.googleusercontent.com/proxy/LhcZ7iaFoInSfQy1r_J1HOWPMTtxYnupwChIFTSA_wTfxDY3HgcGigfxusXNNAZ63YSBgrW9Ng_0lnJNyuw-HZe8OlzcTvPaKX4OXHI=s0-d-e1-ft#https://avatars3.githubusercontent.com/u/47359?s=56&v=4 width=28,height=28! apache/samza
Known high severity security vulnerability detected in {{yajl-ruby < 1.3.1}}defined in Gemfile.lock.
Gemfile.lock update suggested: yajl-ruby ~> 1.3.1.
Always verify the validity and compatibility of suggestions with your codebase.

Review vulnerable dependency
Only users who have been assigned access to security alerts will receive these notifications.
Unsubscribe ¡¤ Email preferences ¡¤ Terms ¡¤ Privacy ¡¤ Sign into GitHub
GitHub, Inc. 
88 Colin P Kelly Jr St. 
San Francisco, CA 94107

{quote}","Decomposed Bugs:  
B: A known high severity security vulnerability detected in {{yajl-ruby < 1.3.1}} defined in Gemfile.lock.  
B: Gemfile.lock update suggested: yajl-ruby ~> 1.3.1.  
B: Always verify the validity and compatibility of suggestions with your codebase.",Major,FALSE,Over-decomposition
Fix HBASE-22492 on branch-2 (SASL GapToken),https://issues.apache.org/jira/browse/HBASE-25586,"The issue is also exist on branch-2. 

17:27:41.556 [pool-1-thread-8] WARN org.apache.hadoop.hbase.client.ScannerCallable - Ignore, probably already closed. Current scan: {""startRow"":""19999999"",""stopRow"":"""",""batch"":20,""cacheBlocks"":true,""totalColumns"":0,""maxResultSize"":""2097152"",""families"":{},""caching"":2147483647,""maxVersions"":1,""timeRange"":[""0"",""9223372036854775807""]} on table: cluster_test 
javax.security.sasl.SaslException: Call to XXX/172.27.162.2:22101 failed on local exception: javax.security.sasl.SaslException: Gap token 
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) 
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 
at java.lang.reflect.Constructor.newInstance(Constructor.java:423) 
at org.apache.hadoop.hbase.ipc.IPCUtil.wrapException(IPCUtil.java:224) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:383) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410) 
at org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:117) 
at org.apache.hadoop.hbase.ipc.Call.setException(Call.java:132) 
at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.cleanupCalls(NettyRpcDuplexHandler.java:203) 
at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.exceptionCaught(NettyRpcDuplexHandler.java:220) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273) 
at org.apache.hbase.thirdparty.io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:143) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:381) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) 
at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) 
at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) 
at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) 
at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) 
at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.lang.Thread.run(Thread.java:748) 
Caused by: javax.security.sasl.SaslException: Gap token 
at com.sun.security.sasl.gsskerb.GssKrb5Base.checkMessageProp(GssKrb5Base.java:142) 
at com.sun.security.sasl.gsskerb.GssKrb5Base.unwrap(GssKrb5Base.java:81) 
at org.apache.hadoop.hbase.security.SaslUnwrapHandler.channelRead0(SaslUnwrapHandler.java:52) 
at org.apache.hadoop.hbase.security.SaslUnwrapHandler.channelRead0(SaslUnwrapHandler.java:33) 
at org.apache.hbase.thirdparty.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
... 20 common frames omitted 
The is can be reproduced with the attached code. hbase.rpc.protection must be set to integrity or privacy.","Decomposed Bugs:  
B: The issue exists on branch-2, where a warning is logged: ""Ignore, probably already closed. Current scan: {""startRow"":""19999999"",""stopRow"":"""",""batch"":20,""cacheBlocks"":true,""totalColumns"":0,""maxResultSize"":""2097152"",""families"":{},""caching"":2147483647,""maxVersions"":1,""timeRange"":[""0"",""9223372036854775807""]} on table: cluster_test.""  
B: A `javax.security.sasl.SaslException: Gap token` occurs when `hbase.rpc.protection` is set to integrity or privacy, causing a call to fail with a local exception.  
B: The exception is caused by a failure in the SASL unwrap process, specifically in `com.sun.security.sasl.gsskerb.GssKrb5Base.checkMessageProp` and `com.sun.security.sasl.gsskerb.GssKrb5Base.unwrap`.  
B: The issue can be reproduced with the attached code when `hbase.rpc.protection` is set to integrity or privacy.",Major,FALSE,Over-decomposition
CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest is broken on Fedora 25.,https://issues.apache.org/jira/browse/MESOS-7049,"Test output: 

[==========] Running 1 test from 1 test case. 
[----------] Global test environment set-up. 
[----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest 
[ RUN ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
../../src/tests/containerizer/cgroups_tests.cpp:1020: Failure 
(statistics).failure(): Failed to parse perf sample: Failed to parse perf sample line '6186960975,,cycles,mesos_test,2000511515,100.00,3.093,GHz': Unexpected number of fields 
../../src/tests/containerizer/cgroups_tests.cpp:193: Failure 
(cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy 
[ FAILED ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest (2123 ms) 
[----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest (2123 ms total) 

[----------] Global test environment tear-down 
../../src/tests/environment.cpp:836: Failure 
Failed 
Tests completed with child processes remaining: 
-+- 20455 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
\--- 20500 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
[==========] 1 test from 1 test case ran. (2141 ms total) 
[ PASSED ] 0 tests. 
[ FAILED ] 1 test, listed below: 
[ FAILED ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
Software versions: 

[jpeach@jpeach src]$ uname -a 
Linux jpeach.apple.com 4.9.6-200.fc25.x86_64 #1 SMP Thu Jan 26 10:17:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 
[jpeach@jpeach src]$ perf -v 
perf version 4.9.6.200.fc25.x86_64.g51a0 
[jpeach@jpeach src]$ cat /etc/os-release 
NAME=Fedora 
VERSION=""25 (Workstation Edition)"" 
ID=fedora 
VERSION_ID=25 
PRETTY_NAME=""Fedora 25 (Workstation Edition)"" 
ANSI_COLOR=""0;34"" 
CPE_NAME=""cpe:/o:fedoraproject:fedora:25"" 
HOME_URL=""https://fedoraproject.org/"" 
BUG_REPORT_URL=""https://bugzilla.redhat.com/"" 
REDHAT_BUGZILLA_PRODUCT=""Fedora"" 
REDHAT_BUGZILLA_PRODUCT_VERSION=25 
REDHAT_SUPPORT_PRODUCT=""Fedora"" 
REDHAT_SUPPORT_PRODUCT_VERSION=25 
PRIVACY_POLICY_URL=https://fedoraproject.org/wiki/Legal:PrivacyPolicy 
VARIANT=""Workstation Edition"" 
VARIANT_ID=workstation 
The test then fails to clean up, leaving stale processes and cgroups.","Decomposed Bugs:  
B: The test fails to parse a perf sample line '6186960975,,cycles,mesos_test,2000511515,100.00,3.093,GHz' due to an unexpected number of fields in the line.  
B: The test fails to remove the cgroup '/sys/fs/cgroup/perf_event/mesos_test' because the device or resource is busy.  
B: The test fails to clean up properly, leaving stale processes and cgroups after execution.",Major,FALSE,Over-analysis
NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/ColumnVector,https://issues.apache.org/jira/browse/RANGER-4178,"Observed below error when enabled audit type as ORC format. 

NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/ColumnVector 

https://issues.apache.org/jira/browse/RANGER-1837 

https://issues.apache.org/jira/browse/RANGER-3235 

cc: rmani","Decomposed Bugs:  
B: Observed NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/ColumnVector when audit type is enabled as ORC format.  
B: The issue is referenced in https://issues.apache.org/jira/browse/RANGER-1837.  
B: The issue is referenced in https://issues.apache.org/jira/browse/RANGER-3235.",Critical,FALSE,Incorrect Interpretation of Solutions
Race between FeatureService and ConfigAdmin for resolving mvn: URLs?,https://issues.apache.org/jira/browse/KARAF-910,"I have an intermittent problem where my custom features.xml cannot be resolved. I use a tweaked etc/org.ops4j.pax.url.mvn.cfg file so the features.xml file is never present in $HOME/.m2/repo but is instead is resolved to local repo relative to the app: 

org.ops4j.pax.url.mvn.defaultRepositories=file:${karaf.base}/system@snapshots,\ 
file:${karaf.home}/${karaf.default.repository}@snapshots,\ 
file:${karaf.base}/../../../.env/.m2/repo@snapshots 

Sometimes when I start Karaf, I get this error (actual URL edited for privacy) 

karaf@tsf> 2011-09-30 09:23:09,760 WARN [FeaturesServiceImpl.java:924] Unable to add features repository mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features at startup - o.a.k.f.i.FeaturesServiceImpl 
java.lang.RuntimeException: URL [mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features] could not be resolved. 
at org.ops4j.pax.url.mvn.internal.Connection.getInputStream(Connection.java:195) [na:na] 
at org.ops4j.pax.url.mvn.internal.AetherBridgeConnection.getInputStream(AetherBridgeConnection.java:68) [na:na] 
at org.apache.karaf.features.internal.FeatureValidationUtil.validate(FeatureValidationUtil.java:49) [na:na] 
at org.apache.karaf.features.internal.FeaturesServiceImpl.validateRepository(FeaturesServiceImpl.java:199) [na:na] 
at org.apache.karaf.features.internal.FeaturesServiceImpl.internalAddRepository(FeaturesServiceImpl.java:210) [na:na] 
at org.apache.karaf.features.internal.FeaturesServiceImpl.start(FeaturesServiceImpl.java:922) [na:na] 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [na:1.6.0_26] 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) [na:1.6.0_26] 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) [na:1.6.0_26] 
at java.lang.reflect.Method.invoke(Method.java:597) [na:1.6.0_26] 
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:226) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:824) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:636) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:724) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:640) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:331) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:227) [org.apache.aries.blueprint:0.3.1] 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [na:1.6.0_26] 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_26] 
at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_26] 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) [na:1.6.0_26] 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206) [na:1.6.0_26] 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_26] 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_26] 
at java.lang.Thread.run(Thread.java:662) [na:1.6.0_26] 

If I put a breakpoint in org.ops4j.pax.url.mvn.internal.Connection.getInputStream(), I can see that when it fails m_configuration.getDefaultRepositories() contains one repo ($HOME/.m2/repo) and when it succeeds m_configuration.getDefaultRepositories() contains the three repos I've specified in etc/org.ops4j.pax.url.mvn.cfg. 

I interpret that to mean that sometimes the features resolution happens before Felix reads the files in etc/ and sometimes the features load afterward. Mostly I'm using the same startlevels as Karaf _ my startup.properties file is identical to the following except for a few additions I made. 

http://svn.apache.org/viewvc/karaf/trunk/assemblies/apache-karaf/src/main/filtered-resources/etc/startup.properties?revision=1176017&view=markup","Decomposed Bugs:  
B: The custom features.xml file cannot be resolved intermittently due to the features resolution sometimes occurring before Felix reads the files in etc/, leading to the default repository ($HOME/.m2/repo) being used instead of the configured repositories.  
B: The configured repositories in etc/org.ops4j.pax.url.mvn.cfg are not consistently applied during Karaf startup, causing the features repository to fail resolution when the default repository is used.  
B: The error occurs when the features resolution process attempts to resolve the URL [mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features] before the configured repositories are loaded, resulting in a RuntimeException.  
B: The issue is observed when m_configuration.getDefaultRepositories() contains only the default repository ($HOME/.m2/repo) instead of the three specified repositories during the features resolution process.",Blocker,FALSE,Over-decomposition
SimpleRpcServer is broken,https://issues.apache.org/jira/browse/HBASE-27097,"Concerns about SimpleRpcServer are not new, and not new to 2.5. @chenxu noticed a problem on HBASE-23917 back in 2020. After some simple evaluations it seems quite broken. 

When I run an async version of ITLCC against a 2.5.0 cluster configured with hbase.rpc.server.impl=SimpleRpcServer, the client almost immediately stalls because there are too many in flight requests. The logic to pause with too many in flight requests is my own. That's not important. Looking at the server logs it is apparent that SimpleRpcServer is quite broken. Handlers suffer frequent protobuf parse errors and do not properly return responses to the client. This is what stalls my test client. Rather quickly all available request slots are full of requests that will have to time out on the client side. 

Exceptions have three patterns but they all have in common SimpleServerRpcConnection#process. It seems likely the root cause is mismatched expectations or bugs in connection buffer handling in SimpleRpcServer/SimpleServerRpcConnection versus downstream classes that process and parse the buffers. It also seems likely that changes were made to downstream classes like ServerRpcConnection expecting NettyRpcServer's particulars without updating SimpleServerRpcConnection and/or SimpleRpcServer. That said, this is just a superficial analysis. 

1) ""Protocol message end-group tag did not match expected tag"" 

2022-06-07T16:44:04,625 WARN [Reader=5,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1 
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag. 
at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.invalidEndTag(InvalidProtocolBufferException.java:129) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.checkLastTagWas(CodedInputStream.java:4034) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4275) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9981) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9910) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14190) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15304) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14651) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:420) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:317) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.mergeFrom(ProtobufUtil.java:2638) ~[hbase-client-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?] 
2) ""Protocol message tag had invalid wire type."" 

2022-06-07T16:44:04,705 WARN [Reader=6,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1 
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException$InvalidWireTypeException: Protocol message tag had invalid wire type. 
at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.invalidWireType(InvalidProtocolBufferException.java:134) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:527) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.GeneratedMessageV3.parseUnknownField(GeneratedMessageV3.java:320) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10700) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10620) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11481) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11475) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9981) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9910) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14190) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15304) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14651) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:420) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:317) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.mergeFrom(ProtobufUtil.java:2638) ~[hbase-client-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?] 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?] 
at java.lang.Thread.run(Thread.java:829) ~[?:?] 
3) ""While parsing a protocol message, the input ended unexpectedly in the middle of a field."" 

2022-06-07T16:44:04,885 WARN [Reader=9,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1 
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field. This could mean either that the input has been truncated or that an embedded message misreported its own length. 
at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:107) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readRawLittleEndian64(CodedInputStream.java:4478) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readFixed64(CodedInputStream.java:4167) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:511) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.GeneratedMessageV3.parseUnknownField(GeneratedMessageV3.java:320) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10700) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10620) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11481) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11475) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at 
org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at 
org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?] 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?] 
at java.lang.Thread.run(Thread.java:829) ~[?:?]

3. I verified elasticsearch is well configured and I noticed in /var/log/elasticsearch/contextElasticSearch.log:

[2018-08-23T12:58:51,898][WARN ][o.e.x.s.t.n.SecurityNetty4ServerTransport] [hjui3pv] exception caught on transport layer [NettyTcpChannel

{localAddress=/127.0.0.1:9300, remoteAddress=/127.0.0.1:43570}
], closing connection
java.lang.IllegalStateException: Received message from unsupported version: [5.2.2] minimal compatible version is: [5.6.0]
at org.elasticsearch.transport.TcpTransport.ensureVersionCompatibility(TcpTransport.java:1462) ~[elasticsearch-6.3.2.jar:6.3.2]
at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1409) ~[elasticsearch-6.3.2.jar:6.3.2]
at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:64) ~[transport-netty4-6.3.2.jar:6.3.2]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:297) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:413) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:241) [netty-handler-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:545) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.16.Final.jar:4.1.16.Final]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]

Please advise what should be done in order to resolve this.

Thank you.","Decomposed Bugs:  
B: When running an async version of ITLCC against a 2.5.0 cluster configured with hbase.rpc.server.impl=SimpleRpcServer, the client stalls almost immediately due to too many in-flight requests. Handlers suffer frequent protobuf parse errors and do not properly return responses to the client, causing the test client to stall.  
B: SimpleRpcServer/SimpleServerRpcConnection has mismatched expectations or bugs in connection buffer handling compared to downstream classes like ServerRpcConnection, which may have been updated for NettyRpcServer without corresponding updates to SimpleRpcServer.  
B: SimpleRpcServer throws ""Protocol message end-group tag did not match expected tag"" errors, causing RpcServer to be unable to read call parameters from the client.  
B: SimpleRpcServer throws ""Protocol message tag had invalid wire type"" errors, causing RpcServer to be unable to read call parameters from the client.  
B: SimpleRpcServer throws ""While parsing a protocol message, the input ended unexpectedly in the middle of a field"" errors, causing RpcServer to be unable to read call parameters from the client.  
B: Elasticsearch logs show an error: ""Received message from unsupported version: [5.2.2] minimal compatible version is: [5.6.0]."" This indicates a version compatibility issue between Elasticsearch and the client.",Blocker,FALSE,Over-analysis
add -Wall to compile log4cxx will get many warning,https://issues.apache.org/jira/browse/LOGCXX-14,"When I build my project using log4cxx with -Wall ,it will get many warning. I think that why don't you use -Wall in log4cxx to check source code. it's very horrible to see these warning !","Decomposed Bugs:  
B: When building a project using log4cxx with -Wall, many warnings are generated.  
B: The log4cxx source code is not checked using -Wall, which could help identify and resolve these warnings.  
B: The presence of these warnings is problematic and makes the build process unpleasant.",Minor,FALSE,Over-decomposition
WSDL2Java fails with imported schemas in WSDL with file not found.,https://issues.apache.org/jira/browse/AXIS2-796,"When using WSDL2Java as ant task schemas that are imported as relative and contained in the same directory as the WSDL cause the task to fail. It finds them early on and then looks in a different directory later on according to the output. This same WSDL and xsd's will work fine in the Eclipse AxisCodegen plugin. 

I believe this is possibly due to the basedir not being set correctly. The Eclipse tool specifically sets the basedir early on in the configuration whereas WSDL2Java does not. 

Stack trace produced. 

wsdl2java: 
[delete] Deleting directory D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\target\generated-sources\java 
[java] Retrieving schema at 'CustomerHeaderData.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/'. 
[java] Retrieving schema at 'CustomerMessages.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/'. 
[java] Retrieving schema at 'CustomerData.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/CustomerMessages.xsd'. 
[java] org.apache.axis2.wsdl.codegen.CodeGenerationException: Error parsing WSDL 
[java] at org.apache.axis2.wsdl.codegen.CodeGenerationEngine.<init>(CodeGenerationEngine.java:94) 
[java] at org.apache.axis2.wsdl.WSDL2Code.main(WSDL2Code.java:32) 
[java] at org.apache.axis2.wsdl.WSDL2Java.main(WSDL2Java.java:21) 
[java] Caused by: org.apache.axis2.AxisFault: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified); nested exception is: 
[java] java.lang.RuntimeException: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.populateService(WSDL2AxisServiceBuilder.java:243) 
[java] at org.apache.axis2.wsdl.codegen.CodeGenerationEngine.<init>(CodeGenerationEngine.java:87) 
[java] ... 2 more 
[java] Caused by: java.lang.RuntimeException: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1916) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1929) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.handleImport(SchemaBuilder.java:1714) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.handleXmlSchemaElement(SchemaBuilder.java:126) 
[java] at org.apache.ws.commons.schema.XmlSchemaCollection.read(XmlSchemaCollection.java:250) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.getXMLSchema(WSDL2AxisServiceBuilder.java:959) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.copyExtensibleElements(WSDL2AxisServiceBuilder.java:1067) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.populateService(WSDL2AxisServiceBuilder.java:221) 
[java] ... 3 more 
[java] Caused by: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified) 
[java] at org.apache.ws.commons.schema.XmlSchemaCollection.read(XmlSchemaCollection.java:221) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1911) 
[java] ... 10 more 
[java] Java Result: 1 
[java] Exception in thread ""main""","Decomposed Bugs:  
B: When using WSDL2Java as an ant task, schemas that are imported as relative and contained in the same directory as the WSDL cause the task to fail. It finds them early on and then looks in a different directory later on according to the output.  
B: The same WSDL and XSDs work fine in the Eclipse AxisCodegen plugin, suggesting the issue is specific to the WSDL2Java ant task.  
B: The issue is possibly due to the basedir not being set correctly in WSDL2Java. The Eclipse tool specifically sets the basedir early on in the configuration, whereas WSDL2Java does not.  
B: The stack trace indicates an error parsing the WSDL, specifically a failure to locate the file `CustomerMessages.xsd` at the expected path `D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd`.  
B: The error is caused by `org.apache.ws.commons.schema.XmlSchemaException`, which indicates the system cannot find the specified file `CustomerMessages.xsd`.",Major,FALSE,Over-analysis
Include documentation for previous version on the website,https://issues.apache.org/jira/browse/HELIX-270,"The documentation on the website is for 0.6.2 but as far as I can see, this version is still under heavy development. 

I can't find a link to the documentation for the 0.6.1 release. I can build the website locally, though I don't get the latest doc fixes for 0.6.1, and that's much less convenient anyway. 

Could you provide separate links to the documentation of the currently released version, previous versions (in the future) and dev version? 

Same for the javadoc: the online one is 0.6.2-snapshot I think (you should probably display a version number on the javadoc headers). 

Thanks!","Decomposed Bugs:  
B: The documentation on the website is for 0.6.2, but this version is still under heavy development.  
B: There is no link to the documentation for the 0.6.1 release.  
B: Building the website locally does not provide the latest doc fixes for 0.6.1, which is inconvenient.  
B: Separate links to the documentation of the currently released version, previous versions (in the future), and dev version are needed.  
B: The online javadoc is for 0.6.2-snapshot, and a version number should be displayed on the javadoc headers.",Major,FALSE,Over-decomposition
axis is vulnerable to XXE,https://issues.apache.org/jira/browse/AXIS-471,"See note from ""Gregory Steuck"" <greg@nest.cx>. He posted an advisory @ 
http://groups.google.com/groups?selm=apn8hv%2421a9%241%40FreeBSD.csie.NCTU.edu.tw&oe=UTF-8&output=gplain 
as well. 

============================================================================ 
Hi Davanum, 

I sent a similar message to Sam Ruby yesterday but never got a reply. 
You seem to be actively committing changes to Axis, so you may be in a 
better position to address the problem. 

Unfortunately Axis (at least 1.0) is vulnerable to XXE attack as 
described in my post below. 

Thanks 
Greg 
----BEGIN PGP SIGNED MESSAGE---- 
Hash: SHA1 

Gregory Steuck security advisory #1, 2002 

Overview: 
XXE (Xml eXternal Entity) attack is an attack on an application that parses 
XML input from untrusted sources using incorrectly configured XML parser. 
The application may be coerced to open arbitrary files and/or TCP connections. 

Legal Notice: 
This Advisory is Copyright (c) 2002 Gregory Steuck. 
You may distribute it unmodified. 
You may not modify it and distribute it or distribute parts 
of it without the author's written permission. 

Disclaimer: 
The information in this advisory is believed to be true though 
it may be false. 
The opinions expressed in this advisory and program are my own and 
not of any company. The usual standard disclaimer applies, 
especially the fact that Gregory Steuck is not liable for any damages 
caused by direct or indirect use of the information or functionality 
provided by this advisory or program. Gregory Steuck bears no 
responsibility for content or misuse of this advisory or program or 
any derivatives thereof. 
Anything in this document may change without notice. 

Details: 
External entity references allow embedding data outside the main file into 
an XML document. In the DTD, one declares the external reference with the 
following syntax: 
<!ENTITY name SYSTEM ""URI""> 

XML processor behavior as specified is 
http://www.w3.org/TR/REC-xml#include-if-valid: 

""When an XML processor recognizes a reference to a parsed entity, in 
order to validate the document, the processor must include its 
replacement text. If the entity is external, and the processor is not 
attempting to validate the XML document, the processor may, but need 
not, include the entity's replacement text..."" 

Now assume that the XML processor parses data originating from a source under 
attacker control. Most of the time the processor will not be validating, 
but it MAY include the replacement text thus initiating an unexpected 
file open operation, or HTTP transfer, or whatever system ids the XML 
processor knows how to access. 

Suspect systems: 
The buzz on the street is ""web services"". They accept XML encoded 
data over the network, sometimes from untrusted clients. So, the 
prime targets are SOAP and XMLRPC implementations. Yet, there are 
many more XML based protocols and vulnerability does not necessary 
lie with the servers. Pick any ""XML based network protocol"" and 
try to apply the attack methodology. 

Suggested fix: 
Most XML parsers allow their user to explicitly specify external 
entity handler. In case of untrusted XML input it is best to prohibit 
all external general entities. 

Successful exploitation may yield: 

DoS on the parsing system by making it open, e.g. 
file:///dev/random | file:///dev/urandom | file://c:/con/con 
TCP scans using HTTP external entities (including behind firewalls 
since application servers often have world view different 
from that of the attacker) 
Unauthorized access to data stored as XML files on the parsing 
system file system (of course the attacker still needs a way to 
get these data back) 
DoS on other systems (if parsing system is allowed to establish 
TCP connections to other systems) 
NTLM authentication material theft by initiating UNC file access to 
systems under attacker control (far fetched?) 
Doomsday scenario: A widely deployed and highly connected application 
vulnerable to this attack may be used for DDoS. 
Products review: 
Several SOAP and XMLRPC implementation were found vulnerable. I will 
be contacting their respective authors directly. It will be up to 
those authors to publish the patches and/or advisories. 

The following implementations were found NOT vulnerable and the reasons 
contributing to their resistance were researched. 

Java: 
Apache XML-RPC server is NOT vulnerable in the default configuration 
due to its use of MinML parser which doesn't support external entities. 
Yet should be vulnerable if used with a full blown parser like Xerces 
or Crimson. To make it invulnerable in all configurations it needs to 
explicitly setup an EntityResolver that aborts having found external 
entities. 

Marqu¨¦e XML-RPC also uses MinML and thus is NOT vulnerable. 

XMLRPC-J uses freeDOM that only supports Minimal XML which 
lacks entity references (http://www.docuverse.com/smldev/minxml.jsp) 

WebLogic 6.1sp3 SOAP implementation was NOT found vulnerable. It 
appears to be using a parser that ignores entities altogether. Ignorance 
is bliss... 

Python: 
Python 2.2 SimpleXMLRPCServer does NOT seem to be vulnerable. It can use 
multiple different parsers: 

xmllib.XMLParser is the default one shipped with Python. It 
doesn't implement processing of doctype definition and thus doesn't 
understand external entities defined in there 
ExpatParser is used when expat python-expat is installed, 
it understands the references but seems to replace them with 
empty strings unconditionally. This negates the attack. 
SGMLOP parser, judging by comments in its source doesn't recognize 
external entities 
FastParser was not available for inspection 
Acknowledgments: 
Even though the issue was discovered and researched independently I 
cannot claim to be the first one to realize the risks associated with 
XML external entities. E.g. RFC 2518 discusses the issue in section 
17.7 Implications of XML External Entities. 

----BEGIN PGP SIGNATURE---- 
Version: GnuPG v1.0.6 (OpenBSD) 
Comment: Processed by Mailcrypt 3.5.6 and Gnu Privacy Guard <http://www.gnupg.org/> 

iEYEARECAAYFAj2/FZkACgkQCxVCvY31obB6vQCbBlV+v0jDRQQ7GcNxYRtajtAf 
FxUAnRCDfjLy2692iGF3Ewmxzo/VXYmz 
=t4QF 
----END PGP SIGNATURE---- 
============================================================================","Decomposed Bugs:  
B: Axis (at least 1.0) is vulnerable to XXE (Xml eXternal Entity) attack, which allows an attacker to coerce the application to open arbitrary files and/or TCP connections.  
B: The XML processor in Axis may include replacement text from external entities, initiating unexpected file open operations, HTTP transfers, or other system accesses when parsing untrusted XML input.  
B: The suggested fix is to explicitly specify an external entity handler in the XML parser to prohibit all external general entities when processing untrusted XML input.  
B: Successful exploitation of the XXE vulnerability in Axis could lead to DoS on the parsing system, unauthorized access to XML files, TCP scans, or NTLM authentication material theft.  
B: The vulnerability in Axis could potentially be used in a Doomsday scenario for DDoS attacks if widely deployed and highly connected applications are affected.  
B: The advisory highlights that Apache XML-RPC server is NOT vulnerable in its default configuration due to its use of MinML parser, but it could be vulnerable if used with a full-blown parser like Xerces or Crimson.  
B: The advisory notes that WebLogic 6.1sp3 SOAP implementation is NOT vulnerable, as it appears to use a parser that ignores entities altogether.  
B: The advisory acknowledges that Python 2.2 SimpleXMLRPCServer is NOT vulnerable, as its default parser (xmllib.XMLParser) does not process doctype definitions or external entities.",-,FALSE,Over-decomposition
PDF parsing to XHTML results in tika attempting to write invalid HTML characters.,https://issues.apache.org/jira/browse/TIKA-2955,"Hi, I am trying to parse: 314.pdf 

what is happening when I try to convert it to XHTML is my XML parser fails because: 

14:35:12.876 [main] ERROR com.funnelback.common.filter.TikaFilterProvider - Unable to filter stream with document type '.pdf' 
org.xml.sax.SAXException: net.sf.saxon.trans.XPathException: Illegal HTML character: decimal 147 
at net.sf.saxon.event.ReceivingContentHandler.endElement(ReceivingContentHandler.java:538) ~[Saxon-HE-9.9.0-2.jar:?] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.SecureContentHandler.endElement(SecureContentHandler.java:256) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.SafeContentHandler.endElement(SafeContentHandler.java:274) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.XHTMLContentHandler.endDocument(XHTMLContentHandler.java:229) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.pdf.AbstractPDF2XHTML.endDocument(AbstractPDF2XHTML.java:556) ~[tika-parsers-1.19.1.jar:1.19.1] 
at org.apache.pdfbox.text.PDFTextStripper.writeText(PDFTextStripper.java:267) ~[pdfbox-2.0.12.jar:2.0.12] 
at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:117) ~[tika-parsers-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:172) ~[tika-parsers-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:143) ~[tika-core-1.19.1.jar:1.19.1] 
at 
[removed section of trace] 
Caused by: net.sf.saxon.trans.XPathException: Illegal HTML character: decimal 147 
at net.sf.saxon.serialize.HTMLEmitter.writeEscape(HTMLEmitter.java:379) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.serialize.XMLEmitter.characters(XMLEmitter.java:662) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.serialize.HTMLEmitter.characters(HTMLEmitter.java:441) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.serialize.HTMLIndenter.characters(HTMLIndenter.java:216) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.SequenceNormalizer.characters(SequenceNormalizer.java:183) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ReceivingContentHandler.flush(ReceivingContentHandler.java:646) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ReceivingContentHandler.endElement(ReceivingContentHandler.java:526) ~[Saxon-HE-9.9.0-2.jar:?] 
... 43 more 
It looks like tika is asking the XML library to handle chracter 147 ie 0x93 which is not allowed in HTML. 

This saxon XML library is not happy with that, I think the default java one doesn't complain when given the invalid character though, however tika is probably wrong to write out that character when writing XHTML.","Decomposed Bugs:  
B: The XML parser fails when converting 314.pdf to XHTML due to an illegal HTML character: decimal 147 (0x93).  
B: The Saxon XML library throws an exception when encountering the illegal HTML character (decimal 147) during XHTML conversion.  
B: Tika is writing out an invalid character (decimal 147) when generating XHTML, which is not allowed in HTML.  
B: The default Java XML library does not complain about the invalid character, but Tika's behavior is incorrect when writing XHTML.",Major,FALSE,Over-analysis
ConcurrentModificationException thrown from inside camel splitter,https://issues.apache.org/jira/browse/CAMEL-6771,"We use camel 2.11.1 running on the oracle 1.7 jvm for linux. 

I have a route that looks like this. It reads in files and puts them on a seda queue with 8 concurrent consumers. 

The SpatialInterpolationPojo reads each file is read and split into two messages X and Y. 
The MyAggregator uses X and Y together and outputs a combined message A.B 
The MySplitterPojo splits A.B into two messages A and B 
from(""file://somefile"") 
.to(""seda:filteraccept?concurrentConsumers=8""); 

from(""seda:filteraccept?concurrentConsumers=8"") 
.split() 
.method(new SpatialInterpolationPojo(), ""split"") 
.to(""direct:wind-aggregator""); 

from(""direct:wind-aggregator"") 
.aggregate(packageCorrelationId(), new MyAggregator()) 
.completionPredicate(header(FIELD_AGGREGATION_COMPLETE).isNotNull()) 
.split() 
.method(new MySplitterPojo()) 
.to(""seda:output""); 
The MySplitterPojo simply returns List<Message> containing two messages that come from data in the input message body. We copy the body headers to the result messages. 

It is thread safe, it has no state, ie there are no object fields that are modified. 

The method is like this it is edited for clarity/privacy: 

public class MySplitterPojo { 

public List<Message> splitMessage( 
@Headers Map<String, Object> headers, 
@Body CombinedObject body) { 

DefaultMessage a = new DefaultMessage(); 
a.setBody(body.getA()); 
a.setHeaders(new HashMap<String, Object>(headers)); 

DefaultMessage b = new DefaultMessage(); 
b.setBody(body.getB()); 
b.setHeaders(new HashMap<String, Object>(headers)); 

ArrayList<Message> result = new ArrayList<Message>(2); 
result.add(a); 
result.add(b); 

return result; 
} 
} 
When we run this route we very occasionally get the exception below. You can see that it is entirely within camel, it appears to be trying to copy the map stored under the exchange property Exchange.AGGREGATION_STRATEGY which is a camel internal property key. 

By inspection of the message I can see that Exchange has just come out of the WindVectorAggregator. 

This seems like it must be a camel bug to me. Any ideas? 

15 Sep 2013 23:06:47,140[Camel (camel-1) thread #21 - seda://filteraccept] WARN AggregateProcessor Error processing aggregated exchange. Exchange[Message: { Trondheim, NO=WindVector [u=-5.92894983291626, v=7.060009002685547], ... }]. Caused by: [java.util.ConcurrentModificationException - null] 
java.util.ConcurrentModificationException 
at java.util.HashMap$HashIterator.nextEntry(Unknown Source) 
at java.util.HashMap$EntryIterator.next(Unknown Source) 
at java.util.HashMap$EntryIterator.next(Unknown Source) 
at java.util.HashMap.putAllForCreate(Unknown Source) 
at java.util.HashMap.<init>(Unknown Source) 
at org.apache.camel.processor.MulticastProcessor.setAggregationStrategyOnExchange(MulticastProcessor.java:1011) 
at org.apache.camel.processor.Splitter.process(Splitter.java:95) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:86) 
at org.apache.camel.processor.aggregate.AggregateProcessor$1.run(AggregateProcessor.java:495) 
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) 
at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source) 
at java.util.concurrent.FutureTask.run(Unknown Source) 
at org.apache.camel.util.concurrent.SynchronousExecutorService.execute(SynchronousExecutorService.java:62) 
at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) 
at org.apache.camel.processor.aggregate.AggregateProcessor.onSubmitCompletion(AggregateProcessor.java:487) 
at org.apache.camel.processor.aggregate.AggregateProcessor.onCompletion(AggregateProcessor.java:471) 
at org.apache.camel.processor.aggregate.AggregateProcessor.doAggregation(AggregateProcessor.java:325) 
at org.apache.camel.processor.aggregate.AggregateProcessor.process(AggregateProcessor.java:229) 
at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122) 
at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:60) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:571) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:504) 
at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:213) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RecipientList.sendToRecipientList(RecipientList.java:151) 
at org.apache.camel.component.bean.MethodInfo$1.doProceed(MethodInfo.java:285) 
at org.apache.camel.component.bean.MethodInfo$1.proceed(MethodInfo.java:251) 
at org.apache.camel.component.bean.BeanProcessor.process(BeanProcessor.java:161) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:117) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:80) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122) 
at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:60) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:122) 
at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:298) 
at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:571) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:504) 
at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:213) 
at org.apache.camel.processor.Splitter.process(Splitter.java:98) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:117) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:80) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:294) 
at org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:203) 
at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:150) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) 
at java.lang.Thread.run(Unknown Source)","Decomposed Bugs:  
B: The route occasionally throws a `java.util.ConcurrentModificationException` when processing aggregated exchanges, specifically when copying the map stored under the exchange property `Exchange.AGGREGATION_STRATEGY`.  
B: The `MySplitterPojo` class creates new `DefaultMessage` instances and copies headers from the input message, but the issue arises when the headers are being copied during the split process.  
B: The exception occurs within the `MulticastProcessor.setAggregationStrategyOnExchange` method, indicating a potential thread-safety issue in the aggregation strategy handling.  
B: The `Splitter.process` method triggers the exception when processing the split messages, suggesting a problem with how the splitter handles concurrent modifications to the exchange properties.  
B: The `AggregateProcessor` encounters the exception during the aggregation process, particularly when handling the completion of aggregated exchanges.",Major,FALSE,Over-analysis
proxy-user not working for Spark on k8s in cluster deploy mode,https://issues.apache.org/jira/browse/SPARK-39399,"As part of https://issues.apache.org/jira/browse/SPARK-25355 Proxy user support was added for Spark on K8s. But the PR only added proxy user argument on the spark-submit command. The actual functionality of authentication using the proxy user is not working in case of cluster deploy mode. 

We get AccessControlException when trying to access the kerberized HDFS through a proxy user. 

Spark-Submit: 
$SPARK_HOME/bin/spark-submit \ 
--master <K8S_APISERVER> \ 
--deploy-mode cluster \ 
--name with_proxy_user_di \ 
--proxy-user <username> \ 
--class org.apache.spark.examples.SparkPi \ 
--conf spark.kubernetes.container.image=<SPARK3.2_with_hadoop3.1_image> \ 
--conf spark.kubernetes.driver.limit.cores=1 \ 
--conf spark.executor.instances=1 \ 
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \ 
--conf spark.kubernetes.namespace=<namespace_name> \ 
--conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \ 
--conf spark.eventLog.enabled=true \ 
--conf spark.eventLog.dir=hdfs://<hdfs_cluster>/scaas/shs_logs \ 

--conf spark.kubernetes.file.upload.path=hdfs://<hdfs_cluster>/tmp \ 

--conf spark.kubernetes.container.image.pullPolicy=Always \ 
$SPARK_HOME/examples/jars/spark-examples_2.12-3.2.0-1.jar 
Driver Logs: 

++ id -u 
+ myuid=185 
++ id -g 
+ mygid=0 
+ set +e 
++ getent passwd 185 
+ uidentry= 
+ set -e 
+ '[' -z '' ']' 
+ '[' -w /etc/passwd ']' 
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false' 
+ SPARK_CLASSPATH=':/opt/spark/jars/*' 
+ env 
+ grep SPARK_JAVA_OPT_ 
+ sort -t_ -k4 -n 
+ sed 's/[^=]*=\(.*\)/\1/g' 
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS 
+ '[' -n '' ']' 
+ '[' -z ']' 
+ '[' -z ']' 
+ '[' -n '' ']' 
+ '[' -z x ']' 
+ SPARK_CLASSPATH='/opt/hadoop/conf::/opt/spark/jars/*' 
+ '[' -z x ']' 
+ SPARK_CLASSPATH='/opt/spark/conf:/opt/hadoop/conf::/opt/spark/jars/*' 
+ case ""$1"" in 
+ shift 1 
+ CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"") 
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=<addr> --deploy-mode client --proxy-user proxy_user --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.SparkPi spark-internal 
WARNING: An illegal reflective access operation has occurred 
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0-1.jar) to constructor java.nio.DirectByteBuffer(long,int) 
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform 
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations 
WARNING: All illegal access operations will be denied in a future release 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of successful kerberos logins and latency (milliseconds)""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of failed kerberos logins and latency (milliseconds)""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""GetGroups""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since startup""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since last successful login""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics 
22/04/26 08:54:38 DEBUG SecurityUtil: Setting hadoop.security.token.service.use_ip to true 
22/04/26 08:54:38 DEBUG Shell: Failed to detect a valid hadoop home directory 
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. 
at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:469) 
at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:440) 
at org.apache.hadoop.util.Shell.<clinit>(Shell.java:517) 
at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78) 
at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665) 
at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:102) 
at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:86) 
at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315) 
at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:303) 
at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1827) 
at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:709) 
at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:659) 
at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:570) 
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161) 
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) 
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90) 
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043) 
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052) 
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 
22/04/26 08:54:38 DEBUG Shell: setsid exited with exit code 0 
22/04/26 08:54:38 DEBUG Groups: Creating new Groups object 
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: backing jks path initialized to file:/etc/security/bind.jceks 
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: initialized local file as '/etc/security/bind.jceks'. 
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: the local file does not exist. 
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Usersearch baseDN: dc=<dc> 
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Groupsearch baseDN: dc=<dc> 
22/04/26 08:54:38 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.LdapGroupsMapping; cacheTimeout=300000; warningDeltaMs=5000 
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login 
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login commit 
22/04/26 08:54:38 DEBUG UserGroupInformation: using local user:UnixPrincipal: 185 
22/04/26 08:54:38 DEBUG UserGroupInformation: Using user: ""UnixPrincipal: 185"" with name 185 
22/04/26 08:54:38 DEBUG UserGroupInformation: User entry: ""185"" 
22/04/26 08:54:38 DEBUG UserGroupInformation: Reading credentials from location set in HADOOP_TOKEN_FILE_LOCATION: /mnt/secrets/hadoop-credentials/..2022_04_26_08_54_34.1262645511/hadoop-tokens 
22/04/26 08:54:39 DEBUG UserGroupInformation: Loaded 3 tokens 
22/04/26 08:54:39 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE) 
22/04/26 08:54:39 DEBUG UserGroupInformation: PrivilegedAction as:proxy_user (auth:PROXY) via 185 (auth:SIMPLE) from:org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163) 
22/04/26 08:54:39 DEBUG FileSystem: Loading filesystems 
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar 
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar 
22/04/26 08:54:39 DEBUG FileSystem: Looking for FS supporting hdfs 
22/04/26 08:54:39 DEBUG FileSystem: looking for configuration option fs.hdfs.impl 
22/04/26 08:54:39 DEBUG FileSystem: Looking in service filesystems for implementation class 
22/04/26 08:54:39 DEBUG FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket 
22/04/26 08:54:39 DEBUG DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0 
22/04/26 08:54:39 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>/tmp/spark-upload-bf713a0c-166b-43fc-a5e6-24957e75b224/spark-examples_2.12-3.0.1.jar 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket 
22/04/26 08:54:39 DEBUG RetryUtils: multipleLinearRandomRetry = null 
22/04/26 08:54:39 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@4a325eb9 
22/04/26 08:54:39 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@2577d6c8 
22/04/26 08:54:40 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library... 
22/04/26 08:54:40 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path: [/usr/java/packages/lib, /usr/lib64, /lib64, /lib, /usr/lib] 
22/04/26 08:54:40 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib 
22/04/26 08:54:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
22/04/26 08:54:40 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded. 
22/04/26 08:54:40 DEBUG DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.transfer.protection = authentication,privacy, configured class dfs.data.transfer.saslproperties.resolver.class = class org.apache.hadoop.security.SaslPropertiesResolver 
22/04/26 08:54:40 DEBUG Client: The ping interval is 60000 ms. 
22/04/26 08:54:40 DEBUG Client: Connecting to <server>/<ip>:8020 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796) 
22/04/26 08:54:40 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE22/04/26 08:54:40 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector.class) 
22/04/26 08:54:40 DEBUG SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one 
22/04/26 08:54:40 DEBUG SaslRpcClient: client isn't using kerberos 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720) 
22/04/26 08:54:40 WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
22/04/26 08:54:40 DEBUG Client: closing ipc connection to <server>/<ip>:8020: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:757) 
at java.base/java.security.AccessController.doPrivileged(Native Method) 
at java.base/javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) 
at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720) 
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:813) 
at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410) 
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558) 
at org.apache.hadoop.ipc.Client.call(Client.java:1389) 
at org.apache.hadoop.ipc.Client.call(Client.java:1353) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) 
at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source) 
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) 
at java.base/java.lang.reflect.Method.invoke(Unknown Source) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) 
at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source) 
at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654) 
at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579) 
at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1576) 
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) 
at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1591) 
at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65) 
at org.apache.hadoop.fs.Globber.doGlob(Globber.java:270) 
at org.apache.hadoop.fs.Globber.glob(Globber.java:149) 
at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2067) 
at org.apache.spark.util.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:318) 
at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:273) 
at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:271) 
at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293) 
at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) 
at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) 
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38) 
at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293) 
at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290) 
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108) 
at org.apache.spark.util.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:271) 
at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:364) 
at scala.Option.map(Option.scala:230) 
at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:364) 
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898) 
at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:165) 
at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163) 
at java.base/java.security.AccessController.doPrivileged(Native Method) 
at java.base/javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) 
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163) 
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) 
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90) 
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043) 
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052) 
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 
Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173) 
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390) 
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:614) 
at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:410) 
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800) 
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:796) 
at java.base/java.security.AccessController.doPrivileged(Native Method) 
at java.base/javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) 
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796) 
... 53 more 


The reason for no delegation token found is that the proxy user UGI doesn't have any credentials/tokens ( tokenSize:: 0 ) 

22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:<hdfs>, Ident: (token for proxyUser: HDFS_DELEGATION_TOKEN owner=proxyUser, renewer=proxyUser, realUser=superuser/test@test.com, issueDate=1651165129518, maxDate=1651769929518, sequenceNumber=180516, masterKeyId=601) 
22/04/28 16:59:37 DEBUG Token: Cannot find class for token kind HIVE_DELEGATION_TOKEN 
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 08 73 68 72 70 72 61 73 61 04 68 69 76 65 1e 6c 69 76 79 2f 6c 69 76 79 2d 69 6e 74 40 43 4f 52 50 44 45 56 2e 56 49 53 41 2e 43 4f 4d 8a 01 80 71 1c 71 b5 8a 01 80 b9 35 79 b5 8e 15 cd 8e 03 6e 
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: kms-dt, Service: <ip>:9292, Ident: (kms-dt owner=proxyUser, renewer=proxyUser, realUser=superuser, issueDate=1651165129566, maxDate=1651769929566, sequenceNumber=181197, masterKeyId=1152) 
22/04/28 16:59:37 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE) 
22/04/28 16:59:37 DEBUG UserGroupInformation: createProxyUser: from:org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) 
22/04/28 16:59:37 DEBUG UserGroupInformation: proxy user created, ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) subject::Subject: 
Principal: proxyUser 
Principal: 185 (auth:SIMPLE) 
tokenSize:: 0 
22/04/28 16:59:38 DEBUG AbstractNNFailoverProxyProvider: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) tokensize:: 0 
22/04/28 16:59:38 DEBUG HAUtilClient: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) tokenSize::0 
22/04/28 16:59:38 DEBUG AbstractDelegationTokenSelector: kindName:: HDFS_DELEGATION_TOKEN service:: ha-hdfs:<hdfs> tokens size:: 0 
22/04/28 16:59:38 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>:8020/tmp/spark-upload-10582dde-f07c-4bf7-a611-5afbdd12ff6c/spark-examples_2.12-3.0.1.jar 


Please refer to the last 4 comments on https://issues.apache.org/jira/browse/SPARK-25355.","B: The proxy user argument was added to the spark-submit command, but the actual functionality of authentication using the proxy user is not working in cluster deploy mode.  
B: AccessControlException occurs when trying to access kerberized HDFS through a proxy user in cluster deploy mode.  
B: The proxy user UGI does not have any credentials/tokens (tokenSize:: 0), leading to no delegation token being found.  
B: The reason for the authentication failure is that the client cannot authenticate via [TOKEN, KERBEROS].",Major,FALSE,Over-analysis
Update contribution guide w/ suggestions for GitHub merge button,https://issues.apache.org/jira/browse/BEAM-3264,"Today our guide for committers has instructions for rebasing, adjusting commits, etc. 

I think with the use of gitbox, we should encourage using the GitHub button, keeping a merge commit when possibly, but editing the default message since it references useless information like the transient branch that was merged.","B: The guide for committers has instructions for rebasing, adjusting commits, etc., but with the use of gitbox, we should encourage using the GitHub button.  
B: The guide should encourage keeping a merge commit when possible but editing the default message since it references useless information like the transient branch that was merged.",P2,FALSE,Over-decomposition
Remove HDP 3.0 stack from Ambari,https://issues.apache.org/jira/browse/AMBARI-22873,Remove HDP-3.0 stack definition from Ambari. Management Packs will replace stacks in Ambari 3.0.,"B: Remove HDP-3.0 stack definition from Ambari.  
B: Management Packs will replace stacks in Ambari 3.0.",Critical,TRUE,
The OpenJPA web site must post the privacy policy,https://issues.apache.org/jira/browse/OPENJPA-844,"During the last few board meetings, the usage of Google Analytics to 
track the usage of our web sites was discussed. While this is not a 
problem per se, Google requires in its Terms and Conditions that all 
sites using it must post a privacy policy (to be exact, paragraph 7 of 
the Analytics Terms and Conditions at 
http://www.google.com/analytics/tos.html states that ""You must post a 
privacy policy and that policy must provide notice of your use of a 
cookie that collects anonymous traffic data."") 

The legal-discuss group, together with the Jackrabbit PMC and a number 
of individuals has pursued this issue and drafted up a privacy policy 
for Jackrabbit, which is available at 
http://jackrabbit.apache.org/privacy-policy.html 

The board would like to thank you for this and appreciates the effort 
and diligence that went into it. 

If your PMC is collecting information through Google Analytics (ATM 
there are at least 18 PMCs using it; you know who you are), we expect 
you to set up a privacy policy along the lines of the Jackrabbit PMC and 
add a note to your next board report after you have done so. 

Thank you for your cooperation. 

For the Apache board 
Henning","Decomposed Bugs:  
B: Google requires in its Terms and Conditions that all sites using Google Analytics must post a privacy policy, as stated in paragraph 7 of the Analytics Terms and Conditions at http://www.google.com/analytics/tos.html.  
B: The legal-discuss group, together with the Jackrabbit PMC and a number of individuals, has drafted a privacy policy for Jackrabbit, available at http://jackrabbit.apache.org/privacy-policy.html.  
B: If your PMC is collecting information through Google Analytics, you are expected to set up a privacy policy similar to the one created by the Jackrabbit PMC.  
B: You are required to add a note to your next board report after setting up the privacy policy.",Major,FALSE,Over-decomposition
typing issue in defaultDeniedProperties (org.apache.unomi.privacy.cfg),https://issues.apache.org/jira/browse/UNOMI-55,"Tested CXS 1.1.0 #685: 

open /etc/org.apache.unomi.privacy.cfg 
the property ""linedInId"" has a typing issue - the ""k"" is missing 
-> please add (linkedInId)","Decomposed Bugs:  
B: The property ""linedInId"" in /etc/org.apache.unomi.privacy.cfg has a typing issue - the ""k"" is missing. It should be corrected to ""linkedInId"".",Major,TRUE,
Email privacy issues in comment notification,https://issues.apache.org/jira/browse/ROL-650,"Notifications of new comments are sent with the commenter's email address as the FROM address. 

Having the poster's address as the FROM address seems at first glance to be a ""good thing"", however it means that any returns such as mailbox full, out of office messages etc. go back to that ""untrusted"" address. Also, if the address is a known email spammer, the comment will be posted but the weblog owner may not get the notification. The first issue is a bit of a privacy issue, since a site may decide not to publish contributors' email addresses, only to have the sent out through this route to unknow 3rd parties.","Decomposed Bugs:  
B: Notifications of new comments are sent with the commenter's email address as the FROM address, which causes privacy issues since a site may decide not to publish contributors' email addresses, only to have them sent out through this route to unknown third parties.  
B: Using the commenter's email address as the FROM address results in returns such as mailbox full or out-of-office messages being sent back to that ""untrusted"" address.  
B: If the commenter's email address is a known email spammer, the comment will be posted, but the weblog owner may not receive the notification.",Minor,FALSE,Over-decomposition
Trademarks / privacy policy footer displays broken,https://issues.apache.org/jira/browse/SUREFIRE-1844,"The footer which is at the end of Surefire's documentation pages, such as this one, have a broken display (at least in Firefox 81 and Google Chrome 85). The horizontal alignment is incorrect, causing the sentence to start outside the visible area and its end to overlap with the ""Privacy policy"" link, as can be seen in the screenshot below:","Decomposed Bugs:  
B: The footer at the end of Surefire's documentation pages has a broken display in Firefox 81, where the horizontal alignment is incorrect, causing the sentence to start outside the visible area and its end to overlap with the ""Privacy policy"" link.  
B: The footer at the end of Surefire's documentation pages has a broken display in Google Chrome 85, where the horizontal alignment is incorrect, causing the sentence to start outside the visible area and its end to overlap with the ""Privacy policy"" link.",Trivial,TRUE,
iOS 6 - deal with new Privacy functionality in Contacts (ABAddressBook:: ABAddressBookCreateWithOptions),https://issues.apache.org/jira/browse/CB-902,"Currently crashes if the user does not have AddressBook permission on iOS 6. 

The user will get a popup dialog similar to the Geolocation permissions dialog. When creating an address book, we should handle the condition where the app does not have permission, and the address book returned is NULL.","Decomposed Bugs:  
B: The app currently crashes if the user does not have AddressBook permission on iOS 6.  
B: When creating an address book, the app does not handle the condition where the address book returned is NULL due to lack of permission.  
B: The user gets a popup dialog similar to the Geolocation permissions dialog when the app attempts to access the AddressBook without permission.",Critical,FALSE,Over-decomposition
Content is getting public to web search engine no privacy,https://issues.apache.org/jira/browse/OFBIZ-4360,"all content hosted on ofbiz trees is getting public throuth a general through this link 
myhost:8080/ecommerce/control/ViewSimpleContent?dataResourceId=10170",B: All content hosted on OFBiz trees is getting publicly exposed through the link: myhost:8080/ecommerce/control/ViewSimpleContent?dataResourceId=10170.,Major,TRUE,
Fix HBase RPC protection documentation,https://issues.apache.org/jira/browse/HBASE-14400,"HBase configuration 'hbase.rpc.protection' can be set to 'authentication', 'integrity' or 'privacy'. 
""authentication means authentication only and no integrity or privacy; integrity implies 
authentication and integrity are enabled; and privacy implies all of 
authentication, integrity and privacy are enabled."" 

However hbase ref guide incorrectly suggests in some places to set the value to 'auth-conf' instead of 'privacy'. Setting value to 'auth-conf' doesn't provide rpc encryption which is what user wants. 

This jira will fix: 

documentation: change 'auth-conf' references to 'privacy' 
SaslUtil to support both set of values (privacy/integrity/authentication and auth-conf/auth-int/auth) to be backward compatible with what was being suggested till now. 
change 'hbase.thrift.security.qop' to be consistent with other similar configurations by using same set of values (privacy/integrity/authentication).","Decomposed Bugs:  
B: The HBase configuration 'hbase.rpc.protection' incorrectly suggests setting the value to 'auth-conf' instead of 'privacy' in some places in the HBase reference guide. Documentation needs to be updated to change 'auth-conf' references to 'privacy'.  
B: SaslUtil needs to support both sets of values (privacy/integrity/authentication and auth-conf/auth-int/auth) to maintain backward compatibility with previously suggested values.  
B: The configuration 'hbase.thrift.security.qop' should be updated to use the same set of values (privacy/integrity/authentication) to be consistent with other similar configurations.",Critical,TRUE,
"Obfuscate commentors email addresses when they select ""Notify me by email of new comments""",https://issues.apache.org/jira/browse/ROL-1455,"Currently, if a commentor leaves a comment and selects the option to be notified of new comments, they and all other users who selected to be notified receive the email notification with all of their email addresses rendered in the ""To"" line. Need to obfuscate their email adresses for privacy purposes. 

","Decomposed Bugs:  
B: Currently, if a commentor leaves a comment and selects the option to be notified of new comments, they and all other users who selected to be notified receive the email notification with all of their email addresses rendered in the ""To"" line.  
B: Need to obfuscate their email addresses for privacy purposes.",Major,FALSE,Incorrect Interpretation of Solutions
Impala website is missing mandatory elements,https://issues.apache.org/jira/browse/IMPALA-11899,"The Apache project website checker lists the following problems about impala.apache.org: 

missing link to the Apache Privacy Policy 
missing copyright notice 
see https://whimsy.apache.org/site/project/impala (the link may require committer or PMC privileges) 
These entries should be simple to add.","Decomposed Bugs:  
B: The Apache project website checker lists that impala.apache.org is missing a link to the Apache Privacy Policy.  
B: The Apache project website checker lists that impala.apache.org is missing a copyright notice.",Major,FALSE,Lacking key information
CookieSpecBase.domainMatch() leaks cookies to 3rd party domains,https://issues.apache.org/jira/browse/HTTPCLIENT-467,"The change committed for #32833
<http://issues.apache.org/bugzilla/show_bug.cgi?id=32833> is buggy; it doesn't
match browser behavior and in fact leaks cookies to third party domains.

To see, try the following:

CookieSpecBase cspec = new CookieSpecBase();
Cookie cookie = new Cookie("".hotmail.com"",""foo"",""bar"",""/"",Integer.MAX_VALUE,false);
cspec.match(""iwanttostealcookiesfromhotmail.com"",80,""/"",false,cookie);

It will return true. Testing in Firefox1.0.4 and IE6 show no such similar
leakage for similar cases. (Indeed, it'd be a headline-making privacy bug if
they were to do this.)

Those browsers do, in my limited testing, behave as desired by the filer of
#32833: a cookie of domain value '.mydomain.com' will be returned to exact host
'mydomain.com' (. However, the fix that was suggested was overbroad.

I suggest instead for CookieSpecBase.domainMatch():

public boolean domainMatch(final String host, final String domain)

{ // BUGGY: matches a '.service.com' cookie to hosts like 'enemyofservice.com' // return host.endsWith(domain) // || (domain.startsWith(""."") && host.endsWith(domain.substring(1))); // BETTER: RFC2109, plus matches a '.service.com' cookie to exact host 'service.com' return host.equals(domain) || (domain.startsWith(""."") && (host.endsWith(domain) || host.equals(domain.substring(1)))); }","Decomposed Bugs: 
B: The change committed for #32833 is buggy; it doesn't match browser behavior and leaks cookies to third-party domains. For example, `CookieSpecBase.match(""iwanttostealcookiesfromhotmail.com"", 80, ""/"", false, cookie)` returns true, which is incorrect. 
B: The current implementation of `CookieSpecBase.domainMatch()` incorrectly matches a `.service.com` cookie to hosts like `enemyofservice.com`. 
B: The suggested fix for `CookieSpecBase.domainMatch()` should be updated to match RFC2109, ensuring a `.service.com` cookie is only returned to the exact host `service.com`. The proposed fix is: ```java public boolean domainMatch(final String host, final String domain) { return host.equals(domain) || (domain.startsWith(""."") && (host.endsWith(domain) || host.equals(domain.substring(1)))); } ```",Major,FALSE,Over-decomposition
NonSequential parser gives an error,https://issues.apache.org/jira/browse/PDFBOX-2293,"I get the following error when using the sequential parse with Pdfbox 1.8.5. 

expected='endstream' actual='' org.apache.pdfbox.io.PushBackInputStream@eb43bd5: java.io.IOException: at org.apache.pdfbox.pdfparser.BaseParser.parseCOSStream(BaseParser.java:628) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:605) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:194) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1220) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1187) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.util.PDFMergerUtility.mergeDocuments(PDFMergerUtility.java:236) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.util.PDFMergerUtility.mergeDocuments(PDFMergerUtility.java:185) [pdfbox-1.8.5.jar:] 
After looking at some of the fixed issues reported for similar problem(s), I have tried using PDFBox 2.0.0 built from the latest repository code and the nonsequential parser for the pdf processing. However, the file created as randomAccessFile seems to get damaged (cannot be opened in Acrobat Reader after the run) when I use PDFbox 2.0.0 for my processing. 
I am unable to attach a sample file because of privacy concerns for the content. I also get an error and am not able to generate the merged output. 
The code snippet is as follows- 

for (String fName : fileList) { 
pd = null; 
File pdFile = new File(fName); 
fNameStr = fName.substring(0, fName.lastIndexOf('.')) 
+ ""_new.pdf""; 

InputStream is = new FileInputStream(pdFile); 
RandomAccessFile raf = new RandomAccessFile(pdFileNew, ""rws""); 
pd = PDDocument.loadNonSeq(is, raf ); 
pd.getDocumentCatalog(); 
pd.save(fNameStr); 
pd.close(); 
if (is != null) { 
is.close(); 
} 
if(raf != null) { 
raf.close(); 
} 

ut.addSource(fNameStr); 
} 
FileOutputStream fos = new FileOutputStream(outFileName); 
ut.setDestinationStream(fos); 
ut.setIgnoreAcroFormErrors(true); 
ut.mergeDocuments(); 
fos.close(); 
Thank You.","Decomposed Bugs:  
B: When using the sequential parse with Pdfbox 1.8.5, an error occurs: expected='endstream' actual='' org.apache.pdfbox.io.PushBackInputStream@eb43bd5: java.io.IOException: at org.apache.pdfbox.pdfparser.BaseParser.parseCOSStream(BaseParser.java:628).  
B: When using PDFBox 2.0.0 built from the latest repository code and the nonsequential parser for PDF processing, the file created as RandomAccessFile gets damaged and cannot be opened in Acrobat Reader after the run.  
B: An error occurs when attempting to generate the merged output using PDFMergerUtility, and the merged output is not created.  
B: The code snippet provided for PDF processing and merging fails to handle the PDF files correctly, resulting in damaged or unreadable output files.",Major,FALSE,Over-analysis
validate_cluster.sh should auto-detect DOCKER_ARCH,https://issues.apache.org/jira/browse/YUNIKORN-1308,"By default, the script validate_cluster.sh does not detect/set DOCKER_ARCH. This can result in the following problem:

Creating kind validation cluster
Apache YuniKorn version: 1.1.0
Helm chart directory: ./helm-charts/yunikorn
Kind cluster config: ./kind.yaml
Kubernetes image: kindest/node:v1.22.4
Registry name: apache
Plugin mode: false
Image Architecture: 
Creating cluster ""yk8s"" ...
_ Ensuring node image (kindest/node:v1.22.4) __
_ Preparing nodes __ __ __ 
_ Writing configuration __ 
_ Starting control-plane ___ 
_ Installing CNI __ 
_ Installing StorageClass __ 
_ Joining worker nodes __ 
Set kubectl context to ""kind-yk8s""
You can now use your cluster with:

kubectl cluster-info --context kind-yk8s

Not sure what to do next? __ Check out https://kind.sigs.k8s.io/docs/user/quick-start/

Pre-Loading docker images...

Pre-Loading admission--1.1.0 image failed, aborting
Removing kind cluster
Deleting cluster ""yk8s"" ...
After setting DOCKER_ARCH to amd64, the problem disappeared.","Decomposed Bugs:  
B: The script validate_cluster.sh does not detect/set DOCKER_ARCH by default.  
B: Pre-Loading admission--1.1.0 image failed due to the missing DOCKER_ARCH setting, causing the process to abort.  
B: After setting DOCKER_ARCH to amd64, the problem disappeared.",Major,TRUE,
"Liberal ""babel"" parser that accepts all SQL dialects",https://issues.apache.org/jira/browse/CALCITE-2280,"Create a parser that accepts all SQL dialects.

It would accept common dialects such as Oracle, MySQL, PostgreSQL, BigQuery. If you have preferred dialects, please let us know in the comments section. (If you're willing to work on a particular dialect, even better!)

We would do this in a new module, inheriting and extending the parser in the same way that the DDL parser in the ""server"" module does.

This would be a messy and difficult project, because we would have to comply with the rules of each parser (and its set of built-in functions) rather than writing the rules as we would like them to be. That's why I would keep it out of the core parser. But it would also have large benefits.

This would be new territory Calcite: as a tool for manipulating/understanding SQL, not (necessarily) for relational algebra or execution.

Some possible uses:

analyze query lineage (what tables and columns are used in a query);
translate from one SQL dialect to another (using the JDBC adapter to generate SQL in the target dialect);
a ""deep"" compatibility mode (much more comprehensive than the current compatibility mode) where Calcite could pretend to be, say, Oracle;
SQL parser as a service: a REST call gives a SQL query, and returns a JSON or XML document with the parse tree.
If you can think of interesting uses, please discuss in the comments.

There are similarities with Uber's QueryParser tool. Maybe we can collaborate, or make use of their test cases.

We will need a lot of sample queries. If you are able to contribute sample queries for particular dialects, please discuss in the comments section. It would be good if the sample queries are based on a familiar schema (e.g. scott or foodmart) but we can be flexible about this.","Decomposed Bugs:  
B: Create a parser that accepts common SQL dialects such as Oracle, MySQL, PostgreSQL, and BigQuery.  
B: The parser should be implemented in a new module, inheriting and extending the parser in the same way that the DDL parser in the ""server"" module does.  
B: The parser must comply with the rules of each SQL dialect and its set of built-in functions, rather than writing the rules as desired.  
B: The parser should support analyzing query lineage (identifying tables and columns used in a query).  
B: The parser should support translating SQL queries from one dialect to another using the JDBC adapter to generate SQL in the target dialect.  
B: The parser should support a ""deep"" compatibility mode where Calcite can emulate a specific SQL dialect (e.g., Oracle) more comprehensively than the current compatibility mode.  
B: The parser should support SQL parsing as a service, where a REST call provides a SQL query and returns a JSON or XML document with the parse tree.  
B: Collaborate with or utilize test cases from Uber's QueryParser tool.  
B: Collect and incorporate sample queries for various SQL dialects, ideally based on familiar schemas (e.g., scott or foodmart).",Major,FALSE,Over-decomposition
Wrong sort elimination when using permuted join order,https://issues.apache.org/jira/browse/DERBY-6148,"I have a query that looks like this: 

SELECT tests.id,tests.item,title FROM tests,item_usage 
WHERE username=? AND user_role>? 
AND item_usage.item=tests.item 
ORDER BY tests.item,title 

The result ordering is by item code followed by title, but the item codes are listed in the order in which they appear in the ITEMS table where they are the primary key rather than in ascending order as expected. If however I change the ORDER BY clause to sort by item_usage.item rather than tests.item, it works correctly, even though the two values are the same! 

The same thing happens in another unrelated query involving item_usage, and the same workaround cures it. 

The relevant tables are defined like so: 

CREATE TABLE item_usage ( 
username VARCHAR(15) NOT NULL, 
item VARCHAR(15) NOT NULL, 
value SMALLINT DEFAULT 0, 
CONSTRAINT item_usage_pk PRIMARY KEY (username,item), 
CONSTRAINT item_usage_1 FOREIGN KEY (username) 
REFERENCES users(username) 
ON DELETE CASCADE, 
CONSTRAINT item_usage_2 FOREIGN KEY (item) 
REFERENCES items(item) 
ON DELETE CASCADE, 
CONSTRAINT item_usage_3 CHECK (value BETWEEN 0 AND 4) 
); 

CREATE TABLE tests ( 
id INTEGER GENERATED ALWAYS AS IDENTITY, 
item VARCHAR(15) NOT NULL, 
title VARCHAR(255) NOT NULL, 
disp SMALLINT NOT NULL DEFAULT 0, 
starttime TIMESTAMP DEFAULT NULL, 
endtime TIMESTAMP DEFAULT NULL, 
offsetx INTEGER NOT NULL DEFAULT 0, 
offsety INTEGER NOT NULL DEFAULT 0, 
rate INTEGER NOT NULL DEFAULT 0, 
duration INTEGER NOT NULL DEFAULT 0, 
calibrate INTEGER NOT NULL DEFAULT 0, 
deadline TIMESTAMP DEFAULT NULL, 
stepsize INTEGER NOT NULL DEFAULT 0, 
interval INTEGER NOT NULL DEFAULT 0, 
stand CHAR(1) DEFAULT NULL, 
hidden CHAR(1) DEFAULT NULL, 
repeated CHAR(1) DEFAULT NULL, 
private CHAR(1) DEFAULT NULL, 
sequential CHAR(1) DEFAULT NULL, 
final CHAR(1) DEFAULT NULL, 
notes CLOB DEFAULT NULL, 
testxml CLOB NOT NULL, 
author VARCHAR(15) NOT NULL, 
time TIMESTAMP NOT NULL, 
CONSTRAINT tests_pk PRIMARY KEY (id), 
CONSTRAINT tests_1 UNIQUE (item, title), 
CONSTRAINT tests_2 FOREIGN KEY (item) 
REFERENCES items(item) 
ON DELETE CASCADE, 
CONSTRAINT tests_3 CHECK (disp BETWEEN 0 AND 100), 
CONSTRAINT tests_4 CHECK (rate BETWEEN 0 AND 100), 
CONSTRAINT tests_5 CHECK (stepsize BETWEEN 0 AND 100) 
); 

If I run the query manually I get this, as expected: 

ID ITEM TITLE 
37 60001 Test 1 
42 60001 Test 2 
51 60001 Test 3 
17 61303 Test 2a 
16 61303 Test 2b 
7 7205731 Test 2a 
8 7205731 Test 2b 

Now, this is actually part of a web app that should turn this into a list of options in a <select> item using the following code: 

while (query.next()) 

{ println(""<option value='"" + query.getInt(""id"") + ""'>"" + encode(query.getString(""item"") + "": "" + query.getString(""title"")) + ""</option>""); } 
What I actually get is this: 

<option value=""17"">61303: Test 2a</option> 
<option value=""16"">61303: Test 2b</option> 
<option value=""7"">7205731: Test 2a</option> 
<option value=""8"">7205731: Test 2b</option> 
<option value=""37"">60001: Test 1</option> 
<option value=""42"">60001: Test 2</option> 
<option value=""51"">60001: Test 3</option> 

The results are sorted by item then by title, but the item order is the order in which they were originally inserted into the items table (where the item and item description are stored, referenced by item_usage.item) rather than by item code. 

I've tried to reproduce this behaviour in a simple example, but without success. I have logged the query plans for both versions; the log output is as follows, with the INCORRECT query (using ORDER BY tests.item) followed later by the CORRECT query (using ORDER BY item_usage.item): 

(moved queryplans to attachment; see attachment queryplans.txt -dagw)","Decomposed Bugs:  
B: The query result ordering is incorrect when using `ORDER BY tests.item`; the item codes are listed in the order they appear in the ITEMS table rather than in ascending order.  
B: The query result ordering works correctly when using `ORDER BY item_usage.item`, even though `tests.item` and `item_usage.item` are the same.  
B: The same issue occurs in another unrelated query involving `item_usage`, and the same workaround (using `ORDER BY item_usage.item`) resolves it.  
B: The web app generates a `<select>` list with incorrect item order due to the query result ordering issue, despite the query being manually verified to return the correct order.  
B: The query plans for the incorrect (`ORDER BY tests.item`) and correct (`ORDER BY item_usage.item`) queries differ, but the exact cause of the discrepancy is unclear.",Major,FALSE,Over-decomposition
Remove dependency on cloudflare CDN,https://issues.apache.org/jira/browse/TINKERPOP-2775,"As per ASF privacy policy [1], we should not host JS or CSS files on CDNs such as Cloudflare instead, we should opt to host the Fonts/JS/CSS locally on the Apache server. 

We have multiple instances [2] where we are downloading scripts from cloudflare. This task should remove this dependency and host the files locally. 

[1] https://privacy.apache.org/faq/committers.html 
[2] Instances of Fonts/JS/CSS downloaded from CDN (not exhaustive): https://github.com/apache/tinkerpop/blob/master/docs/site/home/index.html#L26 
https://github.com/apache/tinkerpop/blob/master/docs/site/home/index.html#L951","Decomposed Bugs:  
B: As per ASF privacy policy, we should not host JS files on CDNs such as Cloudflare. Instead, we should opt to host the JS files locally on the Apache server.  
B: As per ASF privacy policy, we should not host CSS files on CDNs such as Cloudflare. Instead, we should opt to host the CSS files locally on the Apache server.  
B: As per ASF privacy policy, we should not host Fonts on CDNs such as Cloudflare. Instead, we should opt to host the Fonts locally on the Apache server.",Major,FALSE,Lacking key information
Fix website issues,https://issues.apache.org/jira/browse/UNOMI-843,"There are multiple issues on the website: 

Embedded YouTube videos can't work anymore due to server Content-Server-Policy changes, we must replace them with links or use the technique described here : https://privacy.apache.org/faq/committers.html 
Google Analytics tag is old, should be replaced with more recent one. Actually it might be better to replace it with Matomo, see https://privacy.apache.org/faq/committers.html (even better would be Unomi in the future ) 
Some links use HTTP instead of HTTPS 
Fix all the issues reported by Apache's web checker: https://whimsy.apache.org/site/project/unomi 
Add the monthly meeting information to the contribute page with the following information (and ideally a calendar link) : https://lists.apache.org/thread/70oo862br3d4g7j8dvnyy3o4z1p0ozfq","B: Embedded YouTube videos can't work anymore due to server Content-Server-Policy changes, we must replace them with links or use the technique described here: https://privacy.apache.org/faq/committers.html.  
B: Google Analytics tag is old, should be replaced with a more recent one. Actually, it might be better to replace it with Matomo, see https://privacy.apache.org/faq/committers.html (even better would be Unomi in the future).  
B: Some links use HTTP instead of HTTPS.  
B: Fix all the issues reported by Apache's web checker: https://whimsy.apache.org/site/project/unomi.  
B: Add the monthly meeting information to the contribute page with the following information (and ideally a calendar link): https://lists.apache.org/thread/70oo862br3d4g7j8dvnyy3o4z1p0ozfq.",Major,TRUE,
Broken links on RAT's webpage - mailing-lists / prepare webpage for releasing 0.16.1,https://issues.apache.org/jira/browse/RAT-353,"The current webpage at 
https://creadur.apache.org/rat/ 
links to mail-lists.html, which should be 
https://creadur.apache.org/rat/mailing-lists.html 

Check for other dead links and fix them appropriately. 

Further analysis via 
{{$ linkchecker https://creadur.apache.org/rat/ -F text/UTF-8/rat-linkchecker.txt 
}} showed 108 errors on the current RAT webpage.","Decomposed Bugs:  
B: The current webpage at https://creadur.apache.org/rat/ links to mail-lists.html, which should be https://creadur.apache.org/rat/mailing-lists.html.  
B: Check for other dead links on the RAT webpage and fix them appropriately.  
B: Further analysis via {{$ linkchecker https://creadur.apache.org/rat/ -F text/UTF-8/rat-linkchecker.txt }} showed 108 errors on the current RAT webpage.",Major,TRUE,
bin/solr script doesn't do ps properly on some systems,https://issues.apache.org/jira/browse/SOLR-17112,"On Google's colab, the following fails: 

!wget https://dlcdn.apache.org/solr/solr/9.4.0/solr-9.4.0.tgz && tar -xf solr-9.4.0.tgz && cd solr-9.4.0 && echo `pwd` 

!apt update && apt install bc -y && cd solr-9.4.0 && bin/solr stop -p 8983; bin/solr -c -force -Denable.packages=true 

!cd solr-9.4.0 && bin/solr package add-repo data-import-handler ""https://raw.githubusercontent.com/searchscale/dataimporthandler/master/repo/"" 
If I add the following before the last line, it works: 

!cat solr-9.4.0/bin/solr|sed -e 's:ps -f -p:ps -fww -p:g' > tmp; cp tmp solr-9.4.0/bin/solr; chmod +x solr-9.4.0/bin/solr 
I think that extra ""ww"" is needed to make sure Solr works fine on all systems. FYI dep4b.","Decomposed Bugs:  
B: On Google's colab, the command `!wget https://dlcdn.apache.org/solr/solr/9.4.0/solr-9.4.0.tgz && tar -xf solr-9.4.0.tgz && cd solr-9.4.0 && echo \`pwd\`` fails.  
B: On Google's colab, the command `!apt update && apt install bc -y && cd solr-9.4.0 && bin/solr stop -p 8983; bin/solr -c -force -Denable.packages=true` fails.  
B: On Google's colab, the command `!cd solr-9.4.0 && bin/solr package add-repo data-import-handler ""https://raw.githubusercontent.com/searchscale/dataimporthandler/master/repo/""` fails unless the following is added before it: `!cat solr-9.4.0/bin/solr|sed -e 's:ps -f -p:ps -fww -p:g' > tmp; cp tmp solr-9.4.0/bin/solr; chmod +x solr-9.4.0/bin/solr`.  
B: The extra ""ww"" in the command `ps -fww -p` is needed to ensure Solr works fine on all systems.",Major,FALSE,Over-analysis
[pdmodel.font.PDFont] Error: Could not parse predefined CMAP file for 'PDFXC-Indentity0-0',https://issues.apache.org/jira/browse/PDFBOX-940,"Hi, 

when i am trying to upload a pdf document the following error is thrown in the tomcat.. i am using pdfbox-1.4.0.jar.. 

17:29:33,465 ERROR [pdmodel.font.PDFont] Error: Could not parse predefined CMAP file for 'PDFXC-Indentity0-0' 

please find the solution","Decomposed Bugs:  
B: When uploading a PDF document, an error is thrown in Tomcat: ""Error: Could not parse predefined CMAP file for 'PDFXC-Indentity0-0'"". The user is using pdfbox-1.4.0.jar.",Major,TRUE,
RPC Sasl QOP is broken,https://issues.apache.org/jira/browse/HADOOP-9816,HADOOP-9421 broke the handling of SASL wrapping for RPC QOP integrity and privacy options.,"B: HADOOP-9421 broke the handling of SASL wrapping for RPC QOP integrity option.  
B: HADOOP-9421 broke the handling of SASL wrapping for RPC QOP privacy option.",Blocker,FALSE,Lacking key information
cordova-plugin-geolocation should reverse permissions request in ios8,https://issues.apache.org/jira/browse/CB-8826,"I would like to suggest a change to how the geolocation plugin requests 
permissions in iOS8. In the event that both iOS8 NSLocation usage 
permissions exist, I suggest that we first request the least permissive one 
(NSLocationWhenInUseUsageDescription). 

This should amount to simply reversing the logic in CDVLocation.m: 

if([[NSBundle mainBundle] 
objectForInfoDictionaryKey:@""NSLocationWhenInUseUsageDescription""]) 
{ 
[self.locationManager requestWhenInUseAuthorization]; 
} else if([[NSBundle mainBundle] objectForInfoDictionaryKey:@ 
""NSLocationAlwaysUsageDescription""]) { 
[self.locationManager requestAlwaysAuthorization]; 
} 
I have a use case where an app launches with both descriptions set, but 
depending on client configuration the ""AlwaysInUse"" permission may not be necessary. As the logic is written now, the plugin will always request that one, which could look a bit extreme to the end user.","Decomposed Bugs:  
B: The geolocation plugin should first request the least permissive permission (NSLocationWhenInUseUsageDescription) when both iOS8 NSLocation usage permissions exist.  
B: The logic in CDVLocation.m should be reversed to prioritize requesting NSLocationWhenInUseUsageDescription over NSLocationAlwaysUsageDescription.  
B: The current logic in CDVLocation.m always requests NSLocationAlwaysUsageDescription if both permissions are set, which may appear extreme to the end user.  
B: The use case involves an app launching with both NSLocationWhenInUseUsageDescription and NSLocationAlwaysUsageDescription set, but the ""AlwaysInUse"" permission may not always be necessary depending on client configuration.",Minor,FALSE,Over-decomposition
TextPosition.getHeight() returns erroneous value for some PDFs,https://issues.apache.org/jira/browse/PDFBOX-1001,"For a PDF that worked fine under 1.2.1 the height value returned is negative and the wrong value (i.e. using Math.abs() won't fix it). Other PDFs work fine. 
PDF Debug shows ""Creator:Crystal Reports"" and ""Producer:PDF-XChange (XCPRO30.DLL v3.30.0064) (Windows 2k)"" 
And when examining the 'Stream' items, the text is not what displays. 

Any suggestions on what to look for so that I can do differential analysis against other PDFs to see what they do/not have in common with this one? 
(It's client data so I can't post the PDF. ) 

It's stopping us from moving off 1.2.1 (and later versions fix another issue we have of seeing question marks instead of the actual characters).","Decomposed Bugs:  
B: For a PDF that worked fine under 1.2.1, the height value returned is negative and the wrong value (i.e., using Math.abs() won't fix it). Other PDFs work fine.  
B: PDF Debug shows ""Creator:Crystal Reports"" and ""Producer:PDF-XChange (XCPRO30.DLL v3.30.0064) (Windows 2k)"".  
B: When examining the 'Stream' items, the text is not what displays.  
B: It's stopping the user from moving off 1.2.1 (and later versions fix another issue of seeing question marks instead of the actual characters).",Major,FALSE,Over-decomposition
Getting image with black background when converting from PDF to Image!!,https://issues.apache.org/jira/browse/PDFBOX-1023,"Everytime I try to conver a PDF file with a graphic on it, to Image (PNG) I get a black background beneath the graphic, where the background is white originally, here's my code: 

PDDocument document = PDDocument.load(new File(""C:\\export_settings 
testReport.pdf"")); 
List<PDPage> pages = document.getDocumentCatalog().getAllPages(); 

for (int i = 0; i < pages.size(); i++) 

{ PDPage singlePage = pages.get(i); BufferedImage buffImage = singlePage.convertToImage(); ImageIO.write(buffImage, ""PNG"", new File(""C:\\export_settings\\page"" + i + "".png"")); } 
The image quality is good, except for this, I tried with two different methos but I got the same result, please help me, thanks!","Decomposed Bugs:  
B: When converting a PDF file with a graphic to an image (PNG), a black background appears beneath the graphic where the background is originally white.  
B: The issue occurs when using the following code:  
```  
PDDocument document = PDDocument.load(new File(""C:\\export_settings\\testReport.pdf""));  
List<PDPage> pages = document.getDocumentCatalog().getAllPages();  
for (int i = 0; i < pages.size(); i++) {  
    PDPage singlePage = pages.get(i);  
    BufferedImage buffImage = singlePage.convertToImage();  
    ImageIO.write(buffImage, ""PNG"", new File(""C:\\export_settings\\page"" + i + "".png""));  
}  
```  
B: The image quality is good except for the black background issue.  
B: The issue persists even after trying two different methods.",Major,FALSE,Over-decomposition
Copyright Notice of Website outdated,https://issues.apache.org/jira/browse/DIR-240,"Although in 2009, the Copyright notice on our website is still 

 2003-2008, The Apache Software Foundation - Privacy Policy 

For instance here (footer) 
http://directory.apache.org/ 

The templates need an update (unfortunately, I do not know how to accomplish this)","Decomposed Bugs:  
B: The Copyright notice on the website is outdated and still shows ""2003-2008, The Apache Software Foundation - Privacy Policy"" instead of being updated to include 2009.  
B: The website templates need to be updated to reflect the correct Copyright notice.",Major,FALSE,Over-decomposition
Wire encryption is broken,https://issues.apache.org/jira/browse/HBASE-11149,"Upon some testing with the QOP configuration (hbase.rpc.protection), discovered that RPC doesn't work with ""integrity"" and ""privacy"" values for the configuration key. I was using 0.98.x for testing but I believe the issue is there in trunk as well (haven't checked 0.96 and 0.94).","Decomposed Bugs:  
B: RPC doesn't work with the ""integrity"" value for the QOP configuration key (hbase.rpc.protection).  
B: RPC doesn't work with the ""privacy"" value for the QOP configuration key (hbase.rpc.protection).  
B: The issue was discovered while testing with HBase 0.98.x, but it is believed to exist in the trunk version as well.  
B: The issue has not been verified in HBase 0.96 and 0.94 versions.",Major,FALSE,Over-decomposition
Classpath in XML report is wrong,https://issues.apache.org/jira/browse/SUREFIRE-164,"The XML report contains in the property java.class.path Maven's classpath, but not the class path used to execute the tests.","B: The XML report contains Maven's classpath in the property java.class.path, but not the class path used to execute the tests.",Minor,TRUE,
Added restriction to historic queries on web UI,https://issues.apache.org/jira/browse/HIVE-17701,"The HiveServer2 Web UI (HIVE-12550) shows recently completed queries. 
However, a user can see the queries run by other users as well, and that is a security/privacy concern. 
Only admin users should be allowed to see queries from other users (similar to behavior of display for configs, stack trace etc).","Decomposed Bugs:  
B: The HiveServer2 Web UI (HIVE-12550) shows recently completed queries, and a user can see the queries run by other users, which is a security/privacy concern.  
B: Only admin users should be allowed to see queries from other users (similar to behavior of display for configs, stack trace, etc.).",Major,FALSE,Over-decomposition
Cannot ship code hints without also shipping file-system paths,https://issues.apache.org/jira/browse/FLEX-23026,"For privacy reasons, developers need a way to ship code hints in .swc files without also shipping file-system paths. A new compiler option should be added, perhaps: -exclude-file-paths. 

Currently, file paths can only be disabled by setting debug=false, but that results in no hints and no paths.","Decomposed Bugs:  
B: For privacy reasons, developers need a way to ship code hints in .swc files without also shipping file-system paths.  
B: A new compiler option should be added, perhaps: -exclude-file-paths.  
B: Currently, file paths can only be disabled by setting debug=false, but that results in no hints and no paths.",Major,FALSE,Over-decomposition
Long rendering time,https://issues.apache.org/jira/browse/PDFBOX-3791,"Attached pdf file takes too long (more then 9 secs) to render in PDFDebugger (this is a simplified version of a real life pdf that I can not publish for privacy reasons, it takes 57 seconds to render and it contains 56 images and some text). 

I have tried with the options provided in https://pdfbox.apache.org/2.0/getting-started.html but performance is the same","Decomposed Bugs:  
B: Attached PDF file takes too long (more than 9 seconds) to render in PDFDebugger.  
B: The PDF contains 56 images and some text, which may contribute to the rendering performance issue.  
B: Performance remains the same even after trying the options provided in https://pdfbox.apache.org/2.0/getting-started.html.",Major,FALSE,Over-decomposition
Security: passwords logging and file permisions,https://issues.apache.org/jira/browse/DRILL-6189,"Prerequisites: 
1. Log level is set to ""all"" in the conf/logback.xml: 

<logger name=""org.apache.drill"" additivity=""false""> 
<level value=""all"" /> 
<appender-ref ref=""FILE"" /> 
</logger> 
2. PLAIN authentication mechanism is configured: 

security.user.auth: { 
enabled: true, 
packages += ""org.apache.drill.exec.rpc.user.security"", 
impl: ""pam"", 
pam_profiles: [ ""sudo"", ""login"" ] 
} 
Steps: 
1. Start the drillbits 
2. Connect by sqlline: 

/opt/mapr/drill/drill-1.13.0/bin/sqlline -u ""jdbc:drill:zk=node1:5181;"" -n user1 -p 1111 
Expected result: Logs shouldn't contain clear-text passwords 

Actual results: During the drillbit startup or establishing connections via the jdbc or odbc, the following lines appear in the drillbit.log: 

properties { 
key: ""password"" 
value: ""1111"" 
} 
Same thing happens with storage configuration data, everything, including passwords is being logged to file. 

Another issue: 

Currently Drill config files has the permissions 0644: 

-rw-r--r--. 1 mapr mapr 1081 Nov 16 14:42 core-site-example.xml 
-rwxr-xr-x. 1 mapr mapr 1807 Dec 19 11:55 distrib-env.sh 
-rw-r--r--. 1 mapr mapr 1424 Nov 16 14:42 distrib-env.sh.prejmx 
-rw-r--r--. 1 mapr mapr 1942 Nov 16 14:42 drill-am-log.xml 
-rw-r--r--. 1 mapr mapr 1279 Dec 19 11:55 drill-distrib.conf 
-rw-r--r--. 1 mapr mapr 117 Nov 16 14:50 drill-distrib-mem-qs.conf 
-rw-r--r--. 1 mapr mapr 6016 Nov 16 14:42 drill-env.sh 
-rw-r--r--. 1 mapr mapr 1855 Nov 16 14:50 drill-on-yarn.conf 
-rw-r--r--. 1 mapr mapr 6913 Nov 16 14:42 drill-on-yarn-example.conf 
-rw-r--r--. 1 mapr mapr 1135 Dec 19 11:55 drill-override.conf 
-rw-r--r--. 1 mapr mapr 7820 Nov 16 14:42 drill-override-example.conf 
-rw-r--r--. 1 mapr mapr 3136 Nov 16 14:42 logback.xml 
-rw-r--r--. 1 mapr mapr 668 Nov 16 14:51 warden.drill-bits.conf 
-rw-r--r--. 1 mapr mapr 1581 Nov 16 14:42 yarn-client-log.xml 
As they may contain some sensitive information, like passwords or secret keys, they cannot be viewable to everyone. So I suggest to reduce the permissions at least to 0640.","Decomposed Bugs:  
B: During the drillbit startup or establishing connections via the JDBC or ODBC, the logs contain clear-text passwords, such as:  
properties {  
key: ""password""  
value: ""1111""  
}  
B: Storage configuration data, including passwords, is being logged to the drillbit.log file.  
B: Drill configuration files currently have permissions set to 0644, which allows them to be viewable by everyone. These files may contain sensitive information like passwords or secret keys, so permissions should be reduced to at least 0640.",Major,FALSE,Over-decomposition
Facebook Like iframe too narrow when in topbar,https://issues.apache.org/jira/browse/MSKINS-92,"See Apache Syncope website at http://syncope.apache.org 

On the top right you can see the Facebook Like button rendered by 

<iframe src=""http://www.facebook.com/plugins/like.php?href=http://syncope.apache.org/&send=false&layout=button_count&show-faces=false&action=like&colorscheme=dark"" 
scrolling=""no"" frameborder=""0"" 
style=""border:none; width:80px; height:20px; margin-top: 10px;"" class=""pull-right"" ></iframe> 
when changing style to 

""border:none; width:100px; height:20px; margin-top: 10px;"" 
the right-side box is rendered correctly.","Decomposed Bugs:  
B: The Facebook Like button on the Apache Syncope website is rendered incorrectly with the current style settings.  
B: The right-side box of the Facebook Like button is rendered correctly when the style is changed to ""border:none; width:100px; height:20px; margin-top: 10px;"".",Minor,FALSE,Incorrect Interpretation of Solutions
UUID replacement,https://issues.apache.org/jira/browse/CB-49,"reported at: https://github.com/phonegap/phonegap-iphone/issues/238 
by: https://github.com/sandstrom 

As you might have read iOS 5 will remove the UDID (http://techcrunch.com/2011/08/19/apple-ios-5-phasing-out-udid/). 

This is an excellent alternative and it would be nice if you would implement something along these lines to keep the functionality. The idea of hashing together with the bundle id is great, because it makes it impossible to track across applications, which is what apple wanted to fix (although it can be circumvented that would only anger them, and tracking across apps isn't required for most apps anyway). 

https://github.com/gekitz/UIDevice-with-UniqueIdentifier-for-iOS-5","Decomposed Bugs:  
B: iOS 5 will remove the UDID, which affects functionality. An alternative implementation is needed to maintain functionality.  
B: The idea of hashing together with the bundle ID is great because it makes it impossible to track across applications, which aligns with Apple's intent.  
B: Tracking across apps isn't required for most apps, and circumventing this could anger Apple.  
B: An implementation similar to https://github.com/gekitz/UIDevice-with-UniqueIdentifier-for-iOS-5 is suggested as a solution.",Blocker,FALSE,Over-decomposition
Skip whitespaces when resolving a XRef,https://issues.apache.org/jira/browse/PDFBOX-1737,"Oleg Krechowetzki reported an issue with the non sequential parser via private mail. He provided a working solution and a test pdf which can't be attached due to privacy reasons. 

The following exception occurs when parsing the pdf in question using the non sequential parser: 

Caused by: java.io.IOException: Error: Expected a long type, actual='xref' 
at org.apache.pdfbox.pdfparser.BaseParser.readLong(BaseParser.java:1668) 
at org.apache.pdfbox.pdfparser.BaseParser.readObjectNumber(BaseParser.java:1598) 
at org.apache.pdfbox.pdfparser.NonSequentialPDFParser.parseXrefObjStream(NonSequentialPDFParser.java:458)","Decomposed Bugs:  
B: Oleg Krechowetzki reported an issue with the non-sequential parser where the following exception occurs when parsing a specific PDF:  
Caused by: java.io.IOException: Error: Expected a long type, actual='xref'  
at org.apache.pdfbox.pdfparser.BaseParser.readLong(BaseParser.java:1668)  
at org.apache.pdfbox.pdfparser.BaseParser.readObjectNumber(BaseParser.java:1598)  
at org.apache.pdfbox.pdfparser.NonSequentialPDFParser.parseXrefObjStream(NonSequentialPDFParser.java:458).  

B: Oleg Krechowetzki provided a working solution for the issue with the non-sequential parser, but the test PDF cannot be attached due to privacy reasons.",Major,FALSE,Over-decomposition
iOS 11 Error When Taking Picture Missing NSPhotoLibraryAddUsageDescription,https://issues.apache.org/jira/browse/CB-13332,"I am using this plugin to take a picture, but in iOS11 I receive the following error: 

* 
This app has crashed because it attempted to access privacy-sensitive data without a usage description. The app's Info.plist must contain an NSPhotoLibraryAddUsageDescription key with a string value explaining to the user how the app uses this data.* 

Note: I have descriptions for CAMERA_USAGE_DESCRIPTION and PHOTOLIBRARY_USAGE_DESCRIPTION. I was able to take a picture in iOS 10.","Decomposed Bugs:  
B: The app crashes in iOS11 when attempting to access privacy-sensitive data because the Info.plist is missing the NSPhotoLibraryAddUsageDescription key.  
B: The app's Info.plist contains CAMERA_USAGE_DESCRIPTION and PHOTOLIBRARY_USAGE_DESCRIPTION but lacks NSPhotoLibraryAddUsageDescription, which is required for iOS11.  
B: The app was able to take pictures in iOS10 but crashes in iOS11 due to the missing NSPhotoLibraryAddUsageDescription key.",Trivial,FALSE,Over-decomposition
Tighten HFileLink api to enable non-snapshot uses,https://issues.apache.org/jira/browse/HBASE-12749,"In HBASE-12332 we'd like to use the FileLink's IO redirecting powers but want to be able to specify arbitrary alternate link paths and not be tied to the SnapshotFileLink file pattern (aka, table=region-hfile). 

To do this we need change the constructors and some internals so that it is more generic. Along the way, we remove the FileStatus constructor arguments in favor of Path's and reduce the number of ways to create HFileLinks, and tighten up the scope privacy of many methods.","Decomposed Bugs:  
B: In HBASE-12332, we need to modify the constructors of FileLink to allow specifying arbitrary alternate link paths instead of being tied to the SnapshotFileLink file pattern (e.g., table=region-hfile).  
B: In HBASE-12332, we need to change some internals of FileLink to make it more generic.  
B: In HBASE-12332, we need to remove the FileStatus constructor arguments in favor of Path's.  
B: In HBASE-12332, we need to reduce the number of ways to create HFileLinks.  
B: In HBASE-12332, we need to tighten up the scope privacy of many methods.",Major,FALSE,Over-decomposition
"PDA renderers for tr:page, tr:panelPage and tr:panelPageHeader need to render facets more in line with desktop counterparts",https://issues.apache.org/jira/browse/TRINIDAD-1303,"PDA renderers for tr:page, tr:panelPage and tr:panelPageHeader support different sets of facets and render supported facets at different positions. This makes difficult to develop mobile applications.","Decomposed Bugs:  
B: PDA renderers for tr:page support different sets of facets compared to tr:panelPage and tr:panelPageHeader.  
B: PDA renderers for tr:page render supported facets at different positions compared to tr:panelPage and tr:panelPageHeader.  
B: The inconsistency in facet support and rendering positions makes it difficult to develop mobile applications.",Minor,FALSE,Over-decomposition
CCITTFaxG31DDecodeInputStream - Extended codes have wrong length,https://issues.apache.org/jira/browse/PDFBOX-1233,"When dealing with large fax images there are Extended Make Up Codes. 
They are added to the tree as ... 

buildUpMakeUp(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT); 
buildUpMakeUp(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT); 

Accept, the length is 0 based not starting at 1792. 

The quick hack is to create a new method so the length of the node is correct 

private static void buildUpMakeUpLong(short[] codes, 
NonLeafLookupTreeNode root) 
{ 
for (int len = 0, c = codes.length; len < c; len++) 

{ LookupTreeNode leaf = new MakeUpTreeNode((len + 28) * 64); addLookupTreeNode(codes[len], root, leaf); } 
} 

as thus ... 

buildUpMakeUpLong(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT); 
buildUpMakeUpLong(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT);","Decomposed Bugs:  
B: When dealing with large fax images, Extended Make Up Codes are added to the tree using `buildUpMakeUp(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT)` and `buildUpMakeUp(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT)`, but the length is 0-based instead of starting at 1792.  
B: A quick hack is to create a new method `buildUpMakeUpLong` to ensure the length of the node is correct. The method is implemented as follows:  
```java  
private static void buildUpMakeUpLong(short[] codes, NonLeafLookupTreeNode root) {  
    for (int len = 0, c = codes.length; len < c; len++) {  
        LookupTreeNode leaf = new MakeUpTreeNode((len + 28) * 64);  
        addLookupTreeNode(codes[len], root, leaf);  
    }  
}  
```  
B: The new method `buildUpMakeUpLong` is used to replace the existing calls:  
```java  
buildUpMakeUpLong(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT);  
buildUpMakeUpLong(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT);  
```",Major,FALSE,Over-analysis
Make i18n/LocalizedDisplay.sql and i18n/LocalizedConnectionAttribute.sql behave equally on different platforms,https://issues.apache.org/jira/browse/DERBY-1726,"Myrna van Lunteren commented on DERBY-244: 

The one remark I have is that I still cannot get the LocalizedDisplay.sql and LocalizedConnectionAttribute.sql test from the i18n directory to behave the same under windows and Linux (with sun jdk 1.4.2.). 
For windows, I had to update the masters for these tests, but running them on Linux still failed for me. 
With jdk131, ibm131 and ibm142 the LocalizedDisplay.sql test hung, and LocalizedConnectionAttribute exits with a MalformedInputException. 
It would be nice if we could figure out a way to add these tests to the suites... 

¡ª stack of LocalizedConnectionAttribute on Linux ¡ª 
Exception in thread ""main"" sun.io.MalformedInputException 
at sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java(Compiled Code)) 
at sun.nio.cs.StreamDecoder$ConverterSD.convertInto(StreamDecoder.java:287) 
at sun.nio.cs.StreamDecoder$ConverterSD.implRead(StreamDecoder.java:337) 
at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:223) 
at java.io.InputStreamReader.read(InputStreamReader.java:208) 
at java.io.BufferedReader.fill(BufferedReader.java:153) 
at java.io.BufferedReader.readLine(BufferedReader.java:316) 
at java.io.BufferedReader.readLine(BufferedReader.java:379) 
at org.apache.derbyTesting.functionTests.harness.RunTest.setDirectories(RunTest.java:729) 
at org.apache.derbyTesting.functionTests.harness.RunTest.main(RunTest.java:262) 
----------------------------------------------------------------------------","Decomposed Bugs:  
B: The LocalizedDisplay.sql test from the i18n directory does not behave the same under Windows and Linux (with Sun JDK 1.4.2). On Linux, the test hangs with JDK 1.3.1, IBM 1.3.1, and IBM 1.4.2.  
B: The LocalizedConnectionAttribute.sql test from the i18n directory does not behave the same under Windows and Linux (with Sun JDK 1.4.2). On Linux, the test exits with a MalformedInputException with JDK 1.3.1, IBM 1.3.1, and IBM 1.4.2.  
B: The stack trace for LocalizedConnectionAttribute.sql on Linux shows a MalformedInputException at sun.io.ByteToCharUTF8.convert, which occurs during the execution of the test.",Minor,FALSE,Over-decomposition
java.awt.geom.IllegalPathStateException: missing initial moveto in path definition,https://issues.apache.org/jira/browse/PDFBOX-2728,"I get this exception :

java.awt.geom.IllegalPathStateException: missing initial moveto in path definition
at java.awt.geom.Path2D$Float.needRoom(Path2D.java:280)
at java.awt.geom.Path2D.closePath(Path2D.java:1769)
at org.apache.pdfbox.rendering.PageDrawer.closePath(PageDrawer.java:693)
at org.apache.pdfbox.contentstream.operator.graphics.ClosePath.process(ClosePath.java:35)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processOperator(PDFStreamEngine.java:788)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStreamOperators(PDFStreamEngine.java:454)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:425)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:398)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processPage(PDFStreamEngine.java:164)
at org.apache.pdfbox.rendering.PageDrawer.drawPage(PageDrawer.java:164)
at org.apache.pdfbox.rendering.PDFRenderer.renderPage(PDFRenderer.java:213)
similar to PDFBOX-2189.

I can't include the PDF file for privacy reason but I think a similar solution applied for the other bug could fix this problem too","Decomposed Bugs:  
B: The exception `java.awt.geom.IllegalPathStateException: missing initial moveto in path definition` occurs when processing a PDF file. The stack trace indicates the issue arises in `org.apache.pdfbox.rendering.PageDrawer.closePath` and related methods.  
B: The issue is similar to PDFBOX-2189, and a similar solution applied for that bug could potentially fix this problem.",Major,TRUE,
Regression in 2.0.19,https://issues.apache.org/jira/browse/PDFBOX-4805,"Joel Hirsh reported a regression with PDFTextStripper which was introduced with 2.0.19, see his post on users@ for details. 

He can't share the pdf in questions due to privacy but did some debugging and found out that PDFBOX-4760 is the case for that regression. I accidentally committed some unrelated code which leads to bad text extraction results. As the code targets some corner cases it didn't came up as an issue when running our pre release tests. The issue is limited to the 2.0 trunk.","Decomposed Bugs:  
B: Joel Hirsh reported a regression with PDFTextStripper which was introduced with 2.0.19.  
B: The regression is related to PDFBOX-4760, which causes bad text extraction results.  
B: Unrelated code was accidentally committed, leading to the regression.  
B: The issue is limited to the 2.0 trunk and did not come up during pre-release tests due to targeting corner cases.",Major,FALSE,Over-decomposition
Invalidated signature signing pdf twice,https://issues.apache.org/jira/browse/PDFBOX-4261,"A customer sent us a pdf that has this problem: when it is signed twice by pdfbox 1.8.x the second signature invalidates the first one. 

If we apply the same procedure using pdfbox 2.0.x the problem doesn't occur, but the customer required java 1.5 so we can't switch to the new version in this case. 

For privacy purposes we had anonymized the original PDF file by editing 3 stream inside the pdf, without altering the original structure. So the file ""92752146_noSign_anonymous.pdf"" you can find in attachement has not the original text/image streams, but reproduces the problem as the original one. 

Thank you in advance","Decomposed Bugs:  
B: A customer sent a PDF that, when signed twice using pdfbox 1.8.x, results in the second signature invalidating the first one.  
B: The issue does not occur when using pdfbox 2.0.x, but the customer requires Java 1.5, preventing the switch to the newer version.  
B: For privacy purposes, the original PDF file was anonymized by editing 3 streams inside the PDF without altering the original structure. The file ""92752146_noSign_anonymous.pdf"" reproduces the problem as the original one.",Major,FALSE,Over-decomposition
PHP feature is not activated after Oracle JS Parser Implementation is installed,https://issues.apache.org/jira/browse/NETBEANS-2847,"While re-opening a project that had been imported from previous version and was marked ""Broken"". It re-opened and was no longer marked ""broken"", but then this exception occurred.

","Decomposed Bugs:  
B: While re-opening a project that had been imported from a previous version and was marked ""Broken"", it re-opened and was no longer marked ""broken"", but then an exception occurred.",Major,TRUE,
Subversion demands unnecessary access to parent directories of operations,https://issues.apache.org/jira/browse/SVN-3242,"We have updated to the latest 1.5 version of svn. Now we have a problem with
some operations. E.g.
C:\work>svn -m version cp https://someserver.com/svn/ProjectName/2008/trunk
-r56318 https://someserver.com/svn/ProjectName/2008/tags/8.11.07
svn: Server sent unexpected return value (403 Forbidden) in response to PROPFIND
request for '/svn'

C:\work>svn --version
svn, version 1.5.0 (r31699)
compiled Jun 23 2008, 12:59:48

but the same command line works fine with svn 1.4.

Our svn-server is configured to give no access to root folder
(https://someserver.com/svn/) but gives rw access to project folders
(https://someserver.com/svn/ProjectName/), and seems svn1.5 want to do
something with root even it I work only with project folders.
Moreover this command is not only one which gives that problem, in some
circumstances (not sure which, but I guess if new files were added) it
gives the same error even for ""svn up"" command.
Original issue reported by kan","Decomposed Bugs:  
B: After updating to the latest 1.5 version of SVN, the command `svn -m version cp https://someserver.com/svn/ProjectName/2008/trunk -r56318 https://someserver.com/svn/ProjectName/2008/tags/8.11.07` results in a ""403 Forbidden"" error in response to a PROPFIND request for '/svn'.  
B: The SVN server is configured to deny access to the root folder (`https://someserver.com/svn/`) but grants read/write access to project folders (`https://someserver.com/svn/ProjectName/`). SVN 1.5 attempts to access the root folder even when working with project folders.  
B: The same ""403 Forbidden"" error occurs in some circumstances (e.g., when new files are added) with the `svn up` command.  
B: The command works fine with SVN 1.4 but fails with SVN 1.5.",Critical,FALSE,Over-decomposition
(ios) Present notification view controller by inappbrowser view controller,https://issues.apache.org/jira/browse/CB-13555,"When inappbrowser window is shown, if main uiwebview or wkwebview calls cordova Dialog plugin method to show the dialog view, the dialog should show to user on top of the inappbrowser view controller. 

However, currently the dialog view is shown behind the inappbrowser view, so user cannot see it or click button on the dialog 

An similar issue was reported for barcode scanner plugin at 
https://github.com/phonegap/phonegap-plugin-barcodescanner/issues/570 

The issue can be repeated with the below method 
function confirm(){ 
var win = window.open( ""https://www.google.com"", ""_blank"" ); 
win.addEventListener( ""loadstop"", function() { 
setTimeout(function() { 
function onConfirm(buttonIndex) 

{ console.log('You selected button ' + buttonIndex); } 
navigator.notification.confirm( 
'You are the winner!', // message 
onConfirm, // callback to invoke with index of button pressed 
'Game Over', // title 
['Restart','Exit'] // buttonLabels 
); 
}, 1000 ); 
}); 
}","Decomposed Bugs:  
B: When an inappbrowser window is shown, if the main uiwebview or wkwebview calls the cordova Dialog plugin method to show the dialog view, the dialog should appear on top of the inappbrowser view controller. However, currently, the dialog view is shown behind the inappbrowser view, making it invisible and unclickable to the user.  
B: A similar issue was reported for the barcode scanner plugin at https://github.com/phonegap/phonegap-plugin-barcodescanner/issues/570.  
B: The issue can be reproduced using the following method:  
```  
function confirm(){  
    var win = window.open( ""https://www.google.com"", ""_blank"" );  
    win.addEventListener( ""loadstop"", function() {  
        setTimeout(function() {  
            function onConfirm(buttonIndex) {  
                console.log('You selected button ' + buttonIndex);  
            }  
            navigator.notification.confirm(  
                'You are the winner!', // message  
                onConfirm, // callback to invoke with index of button pressed  
                'Game Over', // title  
                ['Restart','Exit'] // buttonLabels  
            );  
        }, 1000 );  
    });  
}  
```",Major,TRUE,
Possible wrong calculation of header length,https://issues.apache.org/jira/browse/MIME4J-265,"I've implemented a sort of mail server and I have many threads listening for incoming emails.
I'm using mime4j to parse javamail Message.

I had only one case of:
Caused by: org.apache.james.mime4j.io.MaxHeaderLengthLimitException: Maximum header length limit exceeded
at org.apache.james.mime4j.stream.DefaultFieldBuilder.append(DefaultFieldBuilder.java:63)
at org.apache.james.mime4j.stream.MimeEntity.readRawField(MimeEntity.java:212)
at org.apache.james.mime4j.stream.MimeEntity.nextField(MimeEntity.java:258)

Looking at the code of DefaultFieldBuilder, it seems that the check over line length is not done on the single line but on the overall header, I'm refering to this line:

if (this.maxlen > 0 && this.buf.length() + len >= this.maxlen) {
Why should you add ""this.buf.length"" ?
I know that there is no limit on header length, but only in its lines.

I can't attach my eml for privacy reasons but I can confirm that I have no too much long line

Thanks","Decomposed Bugs:  
B: The mail server implementation using mime4j to parse javamail Message encounters a MaxHeaderLengthLimitException due to the check on overall header length instead of individual line length in DefaultFieldBuilder.  
B: The code in DefaultFieldBuilder adds ""this.buf.length"" to check the header length, which may not align with the requirement to limit individual line lengths rather than the overall header length.  
B: The MaxHeaderLengthLimitException occurs despite no excessively long lines in the email headers, indicating a potential mismatch between the implementation and the expected behavior of header length validation.",Major,FALSE,Over-analysis
Issue with SQL Server Database with JUDDI 3.3.6 : The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000),https://issues.apache.org/jira/browse/JUDDI-999,"We were using SQLServer with JUDDI 3.0.4. It is working fine so far.

Now, we are trying to move to JUDDI version 3.3.6. We are encountering following issue on start-up.

Caused by: <openjpa-2.3.0-r422266:1540826 nonfatal general error> org.apache.openjpa.persistence.PersistenceException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002]Caused by: <openjpa-2.3.0-r422266:1540826 nonfatal general error> org.apache.openjpa.persistence.PersistenceException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002] at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:559) at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:455) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.synchronizeMappings(JDBCBrokerFactory.java:160) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.synchronizeMappings(JDBCBrokerFactory.java:164) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.newBrokerImpl(JDBCBrokerFactory.java:122) at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:209) at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:155) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:226) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:153) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:59) at org.apache.juddi.config.PersistenceManager.getEntityManager(PersistenceManager.java:48) at org.apache.juddi.config.AppConfig.getPersistentConfiguration(AppConfig.java:174) at org.apache.juddi.config.AppConfig.loadConfiguration(AppConfig.java:160) at org.apache.juddi.config.AppConfig.<init>(AppConfig.java:82) at org.apache.juddi.config.AppConfig.getInstance(AppConfig.java:272) at org.apache.juddi.config.AppConfig.getConfiguration(AppConfig.java:298) at org.apache.juddi.api.impl.AuthenticatedService.<init>(AuthenticatedService.java:75) at org.apache.juddi.api.impl.UDDIInquiryImpl.<init>(UDDIInquiryImpl.java:88) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Unknown Source) at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:142) ... 36 moreCaused by: org.apache.openjpa.lib.jdbc.ReportingSQLException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002] at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:219) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:203) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.access$700(LoggingConnectionDecorator.java:59) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingStatement.executeUpdate(LoggingConnectionDecorator.java:914) at org.apache.openjpa.lib.jdbc.DelegatingStatement.executeUpdate(DelegatingStatement.java:118) at org.apache.openjpa.jdbc.schema.SchemaTool.executeSQL(SchemaTool.java:1231) at org.apache.openjpa.jdbc.schema.SchemaTool.createTable(SchemaTool.java:976) at org.apache.openjpa.jdbc.schema.SchemaTool.add(SchemaTool.java:552) at org.apache.openjpa.jdbc.schema.SchemaTool.add(SchemaTool.java:364) at org.apache.openjpa.jdbc.schema.SchemaTool.run(SchemaTool.java:341) at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:505) ... 58 more


The error is ""The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000)""

Eventually the table ""j3_tmodel_instance_info"" failed to create. We are using SQLServer version 12.0.5207.0. It poses a limit on varchar fields to 8000.

We have tried modifying the column length in class ""TmodelInstanceInfo"" and redeploying the app, however then it starts giving other issue.

The type ""class org.apache.juddi.model.TmodelInstanceInfo"" has not been enhanced.


Could anyone please help us. We are in RED flag and our application cease to work after update to JUDDI 3.3.6

Any help be greatly appreciated. Kindly let me know if I need to provide more information to assist investigation.

Thanks a lot","Decomposed Bugs:  
B: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000) in SQLServer version 12.0.5207.0, causing the table ""j3_tmodel_instance_info"" to fail to create.  
B: Modifying the column length in the class ""TmodelInstanceInfo"" and redeploying the app results in other issues.  
B: The type ""class org.apache.juddi.model.TmodelInstanceInfo"" has not been enhanced, causing issues during the update to JUDDI 3.3.6.",Major,FALSE,Over-decomposition
NullPointerException in CmapSubtable.getCharCode,https://issues.apache.org/jira/browse/PDFBOX-5465,"Hi, 

I got a NPE in the getCharCode method of CmapSubtable : 

java.lang.NullPointerException: null 
at org.apache.fontbox.ttf.CmapSubtable.getCharCode(CmapSubtable.java:669) ~[fontbox-2.0.25.jar!/:2.0.25] 
at org.apache.fontbox.ttf.CmapSubtable.getCharCodes(CmapSubtable.java:686) ~[fontbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.pdmodel.font.PDType0Font.toUnicode(PDType0Font.java:528) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.showGlyph(PDFStreamEngine.java:811) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.showText(PDFStreamEngine.java:749) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.showTextString(PDFStreamEngine.java:608) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.operator.text.ShowText.process(ShowText.java:56) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processOperator(PDFStreamEngine.java:939) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStreamOperators(PDFStreamEngine.java:514) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:492) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processPage(PDFStreamEngine.java:155) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.LegacyPDFStreamEngine.processPage(LegacyPDFStreamEngine.java:155) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.processPage(PDFTextStripper.java:363) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.processPages(PDFTextStripper.java:291) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.writeText(PDFTextStripper.java:238) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.getText(PDFTextStripper.java:202) ~[pdfbox-2.0.25.jar!/:2.0.25] 


-> It seems, in some cases the glyphIdToCharacterCode array is not instantiated. 

Sorry, but for privacy reason I can't share the PDF which cause this issue.","Decomposed Bugs:  
B: A NullPointerException occurs in the getCharCode method of CmapSubtable due to the glyphIdToCharacterCode array not being instantiated in some cases.  
B: The NPE occurs at org.apache.fontbox.ttf.CmapSubtable.getCharCode(CmapSubtable.java:669) and propagates through the call stack, affecting text processing in PDFBox.",Major,FALSE,Over-analysis
TestReloadableDefinitionsFactory fails when the project is in a path with spaces in its name,https://issues.apache.org/jira/browse/TILES-33,"TestReloadableDefinitionsFactory fails if it is run through Maven under Windows 2000 when the project is in a path with spaces in its name 
The same test run from Eclipse 3.2 does not show the problem. 

Here is the report from Maven (asterisks are there to protect privacy) 
<snip> 
------------------------------------------------------- 
T E S T S 
------------------------------------------------------- 
[surefire] Running org.apache.tiles.TestReloadableDefinitionsFactory 
[surefire] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0,031 sec 
[surefire] 
[surefire] testReloadableDefinitionsFactory(org.apache.tiles.TestReloadableDefinitionsFactory) Time elapsed: 0,016 sec <<< FAILURE! 
junit.framework.AssertionFailedError: Error running test: java.net.URISyntaxException: Illegal character in path at index 18: file:/C:/Documents and Settings/***/tiles/tiles-core/target/test-classes/org/apache/tiles/config/temp-defs.xml 
at junit.framework.Assert.fail(Assert.java:47) 
at org.apache.tiles.TestReloadableDefinitionsFactory.testReloadableDefinitionsFactory(TestReloadableDefinitionsFactory.java:148) 
... 
</snip> 

The reason seem to be that sun.misc.Launcher$AppClassLoader.getResource (used in Eclipse) replaces spaces with '%20', while URLClassLoader.getResource (used by Maven) does not replace them.","Decomposed Bugs:  
B: TestReloadableDefinitionsFactory fails when run through Maven under Windows 2000 if the project path contains spaces, resulting in a URISyntaxException due to unencoded spaces in the path.  
B: The issue does not occur when the same test is run from Eclipse 3.2, as sun.misc.Launcher$AppClassLoader.getResource replaces spaces with '%20', while URLClassLoader.getResource (used by Maven) does not.",Minor,FALSE,Incorrect Interpretation of Solutions
LDAP injection vulnerability in LDAPAuthenticationSchemeImpl,https://issues.apache.org/jira/browse/DERBY-7147,"An LDAP injection vulnerability has been identified in LDAPAuthenticationSchemeImpl.getDNFromUID(). An exploit has not been provided, but there is a possibility that an intruder could bypass authentication checks in Derby-powered applications which rely on external LDAP servers. 

For more information on LDAP injection, see https://www.synopsys.com/glossary/what-is-ldap-injection.html","Decomposed Bugs:  
B: An LDAP injection vulnerability has been identified in LDAPAuthenticationSchemeImpl.getDNFromUID().  
B: There is a possibility that an intruder could bypass authentication checks in Derby-powered applications which rely on external LDAP servers.",Major,FALSE,Over-decomposition
Unable to decrypt PDF with String and Stream filter to identity,https://issues.apache.org/jira/browse/PDFBOX-4517,"I receive a PDF that contains the following Encryption Dictionnary: 

32 0 obj 
<</O (___\f__I_&_¨¦ ^¡ã_>5N,\\q¨¨_#O¡ë2__\b_5__j;_P)/EFF/StdCF/P -1852/R 5/OE (Q0_z_¨¨^_____¨¦_nP}___]_.y_¨²_c¨²_^)/U (_T7_Hib__\t|____U¡é__Nb¨ª¨¤>_@_ ___¡êX_¡®¨²-Uz_L<0_)/EncryptMetadata false/V 5/Length 256/CF<</StdCF<</AuthEvent/EFOpen/Length 32/CFM/AESV3>>>>/StmF/Identity/Filter/Standard/StrF/Identity/Perms (_;_¡ª__]m____)/UE (_____k$__f¡®_0£¤_e""__]_9_N_¡®1__)>> 
endobj 
and I was unable to open it with PDF Box. 

Unfortunately, I can't share this PDF with you due to customer privacy and I was unable to find a tool that allow to create such a PDF. 

This kind of encryption is useless I think, but it's probably intersting to support it anyway. Browsers and Adobe Reader have no problem to open it.","Decomposed Bugs:  
B: A PDF with a specific Encryption Dictionary cannot be opened with PDF Box. The Encryption Dictionary is as follows:  
32 0 obj  
<</O (___\f__I_&_¨¦ ^¡ã_>5N,\\q¨¨_#O¡ë2__\b_5__j;_P)/EFF/StdCF/P -1852/R 5/OE (Q0_z_¨¨^_____¨¦_nP}___]_.y_¨²_c¨²_^)/U (_T7_Hib__\t|____U¡é__Nb¨ª¨¤>_@_ ___¡êX_¡®¨²-Uz_L<0_)/EncryptMetadata false/V 5/Length 256/CF<</StdCF<</AuthEvent/EFOpen/Length 32/CFM/AESV3>>>>/StmF/Identity/Filter/Standard/StrF/Identity/Perms (_;_¡ª__]m____)/UE (_____k$__f¡®_0£¤_e""__]_9_N_¡®1__)>>  
endobj.  

B: The encryption used in the PDF is described as ""useless"" but is supported by browsers and Adobe Reader. It may be worth supporting this encryption in PDF Box for compatibility.",Major,FALSE,Over-decomposition
Error when starting Apache Unomi when offline,https://issues.apache.org/jira/browse/UNOMI-75,"Happened when I started unomi while being offline (wifi stopped) 

2017-01-27 15:16:50,399 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/parameterized-types to bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT 
2017-01-27 15:16:50,400 | ERROR | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Unable to start blueprint container for bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT 
org.xml.sax.SAXParseException: src-resolve: Cannot resolve the name 'ptp:ParameterizedInt' to a 'simpleType definition' component. 
at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)[:] 
at org.apache.xerces.util.ErrorHandlerWrapper.error(Unknown Source)[:] 
at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAbstractTraverser.reportSchemaError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAttributeTraverser.traverseNamedAttr(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAttributeTraverser.traverseLocal(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAbstractTraverser.traverseAttrsAndAttrGrps(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.processComplexContent(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.traverseComplexTypeDecl(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.traverseLocal(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseNamedElement(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseGlobal(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.traverseSchemas(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.parseSchema(Unknown Source)[:] 
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadSchema(Unknown Source)[:] 
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:] 
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:] 
at org.apache.xerces.jaxp.validation.XMLSchemaFactory.newSchema(Unknown Source)[:] 
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.createSchema(NamespaceHandlerRegistryImpl.java:637)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.doGetSchema(NamespaceHandlerRegistryImpl.java:458)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.getSchema(NamespaceHandlerRegistryImpl.java:443)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:343)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1] 
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:] 
at java.lang.Thread.run(Thread.java:745)[:1.8.0_111] 
2017-01-27 15:16:50,439 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/beans to bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT 
2017-01-27 15:16:50,440 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/parameterized-types to bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT 
After some more research this is really a problem with Aries Blueprint and Apache CXF, here are the relevant tickets : 

https://issues.apache.org/jira/browse/ARIES-1540 

and 

https://issues.apache.org/jira/browse/CXF-7183 

We could use the first fix for the current release but we will need to somehow upgrade CXF again when we switch to Karaf 4.","Decomposed Bugs:  
B: The issue occurs when starting Unomi while offline (WiFi stopped), causing a failure in the blueprint container for bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT due to an unresolved XML schema component 'ptp:ParameterizedInt'.  
B: The issue is related to Aries Blueprint and Apache CXF, as indicated by the relevant tickets: https://issues.apache.org/jira/browse/ARIES-1540 and https://issues.apache.org/jira/browse/CXF-7183.  
B: The current release could use the fix from ARIES-1540, but an upgrade to CXF will be required when switching to Karaf 4.",Major,FALSE,Over-decomposition
base class warning in SAXException.hpp copy constructor,https://issues.apache.org/jira/browse/XERCESC-1162,"I have been getting multiple copies of this warning: 

include/xercesc/sax/SAXException.hpp: In copy constructor 
`xercesc_2_5::SAXException::SAXException(const xercesc_2_5::SAXException&)': 
include/xercesc/sax/SAXException.hpp:195: warning: base class `class 
xercesc_2_5::XMemory' should be explicitly initialized in the copy constructor 

I have to fix the code in SAXException.hpp by changing this: 

SAXException(const SAXException& toCopy) : 

fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager)) 
, fMemoryManager(toCopy.fMemoryManager) 
{ 
} 

into this: 

SAXException(const SAXException& toCopy) : XMemory(), 

fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager)) 
, fMemoryManager(toCopy.fMemoryManager) 
{ 
} 

once XMemory() is declared as a base class, all warnings are gone. I've seen 
this in 2.5.0 as well. Below are the compiler flags that I have set which 
should help you recreate this bug: 

-g3 -I. -I./include -isystem ./libs/crystalize/Linux/include -I. - 
I./include -isystem ./libs/crystalize/Linux/include -D_linux_ -D_x86_ - 
DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT -I./libs/Database - 
I./libs/Database/libs/xerces/Linux/include isystem ./libs/sybase/sybase 
12.5.1/Linux/include I./libs/xerces/Linux/include -Wall -W -pedantic -Wno 
long-long Wpointer-arith -Wcast-qual -Wwrite-strings -Wconversion -Wstrict 
prototypes -Wmissing-prototypes -Woverloaded-virtual -Wchar-subscripts - 
Wparentheses Wpointer-arith -Winline -Wnon-virtual-dtor -Wreorder -fno-gnu 
keywords -Wctor-dtor-privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar - 
Wno-float-equal Wdisabled-optimization -Wswitch -Wpointer-arith -pipe -Wno 
cast-qual Wno-unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage 
length=0 -DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64 - 
D_linux_ -D_x86_ -DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT - 
I./libs/utilities/include -I./libs/AST_Common -I./libs/AST_Common/AST/enums - 
I./libs/Database -I./libs/Database/libs/boost/Linux - 
I./libs/Database/libs/omni/Linux/include I./libs/Database/libs/sybase/sybase 
12.5.1/Linux/include -I./libs/Database/libs/xerces/Linux/include - 
isystem ./libs/sybase/sybase-12.5.1/Linux/include - 
I./libs/xerces/Linux/include -Wall -W -pedantic -Wno-long-long -Wpointer-arith - 
Wcast-qual Wwrite-strings -Wconversion -Wstrict-prototypes -Wmissing 
prototypes Woverloaded-virtual -Wchar-subscripts -Wparentheses -Wpointer 
arith Winline -Wnon-virtual-dtor -Wreorder -fno-gnu-keywords -Wctor-dtor 
privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar -Wno-float-equal - 
Wdisabled-optimization Wswitch -Wpointer-arith -pipe -Wno-cast-qual -Wno 
unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage-length=0 - 
DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64 

Let me know if you need more information. Thank you. 

-Vrajesh","Decomposed Bugs:  
B: The copy constructor `xercesc_2_5::SAXException::SAXException(const xercesc_2_5::SAXException&)` in `include/xercesc/sax/SAXException.hpp` does not explicitly initialize the base class `xercesc_2_5::XMemory`, causing a compiler warning.  
B: The code in `SAXException.hpp` needs to be modified to explicitly initialize the base class `XMemory` in the copy constructor. The current implementation is:  
```cpp  
SAXException(const SAXException& toCopy) :  
fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager))  
, fMemoryManager(toCopy.fMemoryManager)  
{  
}  
```  
It should be changed to:  
```cpp  
SAXException(const SAXException& toCopy) : XMemory(),  
fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager))  
, fMemoryManager(toCopy.fMemoryManager)  
{  
}  
```  
B: The compiler flags used to reproduce the warning are:  
```  
-g3 -I. -I./include -isystem ./libs/crystalize/Linux/include -I. -I./include -isystem ./libs/crystalize/Linux/include -D_linux_ -D_x86_ -DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT -I./libs/Database -I./libs/Database/libs/xerces/Linux/include -isystem ./libs/sybase/sybase-12.5.1/Linux/include -I./libs/xerces/Linux/include -Wall -W -pedantic -Wno-long-long -Wpointer-arith -Wcast-qual -Wwrite-strings -Wconversion -Wstrict-prototypes -Wmissing-prototypes -Woverloaded-virtual -Wchar-subscripts -Wparentheses -Wpointer-arith -Winline -Wnon-virtual-dtor -Wreorder -fno-gnu-keywords -Wctor-dtor-privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar -Wno-float-equal -Wdisabled-optimization -Wswitch -Wpointer-arith -pipe -Wno-cast-qual -Wno-unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage-length=0 -DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64 -D_linux_ -D_x86_ -DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT -I./libs/utilities/include -I./libs/AST_Common -I./libs/AST_Common/AST/enums -I./libs/Database -I./libs/Database/libs/boost/Linux -I./libs/Database/libs/omni/Linux/include -I./libs/Database/libs/sybase/sybase-12.5.1/Linux/include -I./libs/Database/libs/xerces/Linux/include -isystem ./libs/sybase/sybase-12.5.1/Linux/include -I./libs/xerces/Linux/include -Wall -W -pedantic -Wno-long-long -Wpointer-arith -Wcast-qual -Wwrite-strings -Wconversion -Wstrict-prototypes -Wmissing-prototypes -Woverloaded-virtual -Wchar-subscripts -Wparentheses -Wpointer-arith -Winline -Wnon-virtual-dtor -Wreorder -fno-gnu-keywords -Wctor-dtor-privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar -Wno-float-equal -Wdisabled-optimization -Wswitch -Wpointer-arith -pipe -Wno-cast-qual -Wno-unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage-length=0 -DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64  
```",Major,FALSE,Incorrect Interpretation of Solutions
svnmerge.py migration tool(s) do not guarantee proper svn:mergeinfo range ordering,https://issues.apache.org/jira/browse/SVN-3302,"Per http://svn.haxx.se/dev/archive-2008-10/0280.shtml, 
svnmerge-migrate-history.py (and probably svnmerge-migrate-history-remotely.py, 
too) do not ensure that the svn:mergeinfo properties they create have their 
rangelists sorted properly, which can result in the versioning of bogus property 
value that Subversion can't use.","Decomposed Bugs:  
B: svnmerge-migrate-history.py does not ensure that the svn:mergeinfo properties it creates have their rangelists sorted properly, which can result in the versioning of bogus property values that Subversion can't use.  
B: svnmerge-migrate-history-remotely.py likely has the same issue as svnmerge-migrate-history.py, where it does not ensure that the svn:mergeinfo properties it creates have their rangelists sorted properly, which can result in the versioning of bogus property values that Subversion can't use.",Critical,TRUE,
byte[].encodeBase64() incorrectly introduces line breaks,https://issues.apache.org/jira/browse/GROOVY-2878,"Groovy's encodeBase64 inserts 0x0A chars (LF) into long strings to break lines. This is contrary to my reading of RFC4648: 

{codec} 
3.1. Line Feeds in Encoded Data 

MIME [4] is often used as a reference for base 64 encoding. However, 
MIME does not define ""base 64"" per se, but rather a ""base 64 Content- 
Transfer-Encoding"" for use within MIME. As such, MIME enforces a 
limit on line length of base 64-encoded data to 76 characters. MIME 
inherits the encoding from Privacy Enhanced Mail (PEM) [3], stating 
that it is ""virtually identical""; however, PEM uses a line length of 
64 characters. The MIME and PEM limits are both due to limits within 
SMTP. 

Implementations MUST NOT add line feeds to base-encoded data unless 
the specification referring to this document explicitly directs base 
encoders to add line feeds after a specific number of characters.{codec} 
This has resulted in incorrect behaviour in Grails also. However the author notes that some groovy applications may rely on this functionality currently, so this could be a breaking change for some. 

It would be better to be correct IMO. There is no clue in groovy docs that this introduces line breaks in this MIME/SMTP specific way.","Decomposed Bugs:  
B: Groovy's encodeBase64 inserts 0x0A chars (LF) into long strings to break lines, which is contrary to RFC4648, which states that implementations MUST NOT add line feeds to base-encoded data unless explicitly directed.  
B: The incorrect behavior in Groovy's encodeBase64 has resulted in incorrect behavior in Grails.  
B: Some Groovy applications may rely on the current behavior of encodeBase64 inserting line breaks, which could make fixing this a breaking change for those applications.  
B: There is no mention in Groovy's documentation that encodeBase64 introduces line breaks in a MIME/SMTP-specific way.",Major,FALSE,Over-decomposition
Cleanup suspect coding practices in the org.apache.derby.impl.tools.ij package.,https://issues.apache.org/jira/browse/DERBY-6195,Similar to DERBY-6177.,B: Similar to DERBY-6177.,Minor,TRUE,
ArrayIndexOutOfBoundsException: 9 parsing RTF,https://issues.apache.org/jira/browse/TIKA-1192,"When trying to parse an RTF file I'm getting the following exception. I am not able to attach the file for privacy reasons: 

java.lang.ArrayIndexOutOfBoundsException: 9 
TextExtractor.java:872 org.apache.tika.parser.rtf.TextExtractor.processControlWord 
TextExtractor.java:566 org.apache.tika.parser.rtf.TextExtractor.parseControlWord 
TextExtractor.java:492 org.apache.tika.parser.rtf.TextExtractor.parseControlToken 
TextExtractor.java:459 org.apache.tika.parser.rtf.TextExtractor.extract 
TextExtractor.java:448 org.apache.tika.parser.rtf.TextExtractor.extract 
RTFParser.java:56 org.apache.tika.parser.rtf.RTFParser.parse 
(Unknown Source) sun.reflect.NativeMethodAccessorImpl.invoke0 
NativeMethodAccessorImpl.java:57 sun.reflect.NativeMethodAccessorImpl.invoke 
DelegatingMethodAccessorImpl.java:43 sun.reflect.DelegatingMethodAccessorImpl.invoke 
Method.java:606 java.lang.reflect.Method.invoke 
Reflector.java:93 clojure.lang.Reflector.invokeMatchingMethod 
Reflector.java:28 clojure.lang.Reflector.invokeInstanceMethod 
tika_parser.clj:20 rtf-parser.tika-parser/parse 
form-init2921349737948661927.clj:1 rtf-parser.tika-parser/eval4200 
Compiler.java:6619 clojure.lang.Compiler.eval 
Compiler.java:6582 clojure.lang.Compiler.eval 
core.clj:2852 clojure.core/eval 
main.clj:259 clojure.main/repl[fn] 
main.clj:259 clojure.main/repl[fn] 
main.clj:277 clojure.main/repl[fn] 
main.clj:277 clojure.main/repl 
RestFn.java:1096 clojure.lang.RestFn.invoke 
interruptible_eval.clj:56 clojure.tools.nrepl.middleware.interruptible-eval/evaluate[fn] 
AFn.java:159 clojure.lang.AFn.applyToHelper 
AFn.java:151 clojure.lang.AFn.applyTo 
core.clj:617 clojure.core/apply 
core.clj:1788 clojure.core/with-bindings* 
RestFn.java:425 clojure.lang.RestFn.invoke 
interruptible_eval.clj:41 clojure.tools.nrepl.middleware.interruptible-eval/evaluate 
interruptible_eval.clj:171 clojure.tools.nrepl.middleware.interruptible-eval/interruptible-eval[fn] 
core.clj:2330 clojure.core/comp[fn] 
interruptible_eval.clj:138 clojure.tools.nrepl.middleware.interruptible-eval/run-next[fn] 
AFn.java:24 clojure.lang.AFn.run 
ThreadPoolExecutor.java:1145 java.util.concurrent.ThreadPoolExecutor.runWorker 
ThreadPoolExecutor.java:615 java.util.concurrent.ThreadPoolExecutor$Worker.run 
Thread.java:724 java.lang.Thread.run","Decomposed Bugs:  
B: When trying to parse an RTF file, a `java.lang.ArrayIndexOutOfBoundsException: 9` exception occurs at `TextExtractor.java:872` in the `processControlWord` method.  
B: The exception occurs in the `parseControlWord` method at `TextExtractor.java:566`.  
B: The exception occurs in the `parseControlToken` method at `TextExtractor.java:492`.  
B: The exception occurs in the `extract` method at `TextExtractor.java:459`.  
B: The exception occurs in the `extract` method at `TextExtractor.java:448`.  
B: The exception occurs in the `parse` method at `RTFParser.java:56`.",Major,FALSE,Over-analysis
strange Digester parsing error,https://issues.apache.org/jira/browse/DIGESTER-27,"While testing our application we ran into a strange Digester parse issue. 
It looks like the Digester sometimes forgets to parse a value in the xml. Here 
the situation: 

1. If we fire testscript A that doesnot comply to the schema we set on the 
Digester then we get a parsing error as expected. The error is that field Z in 
the xml was not valid. 
2. We fire testscript B which should return an answer. The first time we fire 
it the Digester doesnot map field Z (which now has a valid value) to the java 
class as defined in the rule file. 
3. We fire testscript B again unchanged and now field Z is mapped by the 
Digester to the correct attribute in the corresponding java class. 

If at point 1 we dont fire testscript A (with the invalid value for attribute Z) 
but say C or any other this doesnot occur and we get the reply we expect...... 

It seems like that after a call which results in a SAXException due to an 
invalid value in the XML according to the attached schema the next call fails 
to parse the xml correctly to the java object defined in the rule file. The 
third call however (which is exactly the same as the second) succeeds. 

Any idea's? 

Regards, 
Lars Vonk","Decomposed Bugs:  
B: The Digester sometimes forgets to parse a value in the XML. If testscript A, which does not comply to the schema set on the Digester, is fired, it results in a parsing error as expected. The error is that field Z in the XML was not valid.  
B: After firing testscript A, firing testscript B (which should return an answer) results in the Digester not mapping field Z (which now has a valid value) to the Java class as defined in the rule file.  
B: Firing testscript B again unchanged results in the Digester correctly mapping field Z to the corresponding attribute in the Java class.  
B: If testscript A is not fired, but instead testscript C or any other script is fired, the issue does not occur, and the expected reply is received.  
B: It appears that after a call resulting in a SAXException due to an invalid value in the XML according to the attached schema, the next call fails to parse the XML correctly to the Java object defined in the rule file. The third call, which is identical to the second, succeeds.",Major,FALSE,Over-decomposition
Wrong XAException return code when broker timeout is hit,https://issues.apache.org/jira/browse/ARTEMIS-591,"By creating testcases for checking behavior of transaction timeout I've hit an issue of wrong error code being returned when broker transaction timeout is hit before TM transaction timeout expires. 
It uses XAER_PROTO instead of RBTIMEOUT. 

This issue does not cause data inconsistency. 

Scenario: 

ejb sends a message to a queue 
processing inside of the ejb takes long time 
TM transaction timeout is set big enough to not hit the timeout 
jms broker internal transaction timeout is smaller than time needed for processing ejb method 
jms broker txn timeout occurs - broker local txn is rolled back 
txn is removed from list of broker's local in-process transactions 
TM calls XAResource.end 
the call returns XAException.XAER_PROTO 
That's current implementation returns XAER_PROTO in this scenario but RBTIMEOUT would be more appropriate. 

From discussion with Narayana developers, RM should return the most specific error return code as possible. In this scenario it's RBTIMEOUT. 

Other notes from TM dev point of view: 

""[XA_RBTIMEOUT] 
The work represented by this transaction branch took too long."" 
per XA spec page 39. 

The more complex question is, at what point can the resource manager forget about that branch (and therefore return NOTA to subsequent calls)? 

The XA spec says ""After the transaction manager calls xa_end(), it should no longer consider the calling thread associated with that resource manager (although it must consider the resource manager part of the transaction branch when it prepares the branch.)"" 
which implies the branch is still considered live at that point, a view corroborated by: 

""[XA_RB_] 
The resource manager has dissociated the transaction branch from the thread of control and has marked rollback-only the work performed on behalf of _xid."" 
Exception being thrown 

WARN [com.arjuna.ats.jta] (Thread-0 
(ActiveMQ-client-global-threads-1468293951)) ARJUNA016056: 
TransactionImple.delistResource - caught exception during delist : 
XAException.XAER_PROTO: javax.transaction.xa.XAException 
at 
org.apache.activemq.artemis.core.protocol.core.impl.ActiveMQSessionContext.xaEnd(ActiveMQSessionContext.java:346) 
at 
org.apache.activemq.artemis.core.client.impl.ClientSessionImpl.end(ClientSessionImpl.java:1115) 
at 
org.apache.activemq.artemis.ra.ActiveMQRAXAResource.end(ActiveMQRAXAResource.java:112) 
at 
org.apache.activemq.artemis.service.extensions.xa.ActiveMQXAResourceWrapperImpl.end(ActiveMQXAResourceWrapperImpl.java:81) 
at 
com.arjuna.ats.internal.jta.transaction.arjunacore.TransactionImple.delistResource(TransactionImple.java:897) 
at 
org.jboss.jca.core.connectionmanager.listener.TxConnectionListener$TransactionSynchronization.beforeCompletion(TxConnectionListener.java:1063) 
at 
org.jboss.jca.core.connectionmanager.transaction.TransactionSynchronizer.invokeBefore(TransactionSynchronizer.java:438) 
at 
org.jboss.jca.core.connectionmanager.transaction.TransactionSynchronizer.beforeCompletion(TransactionSynchronizer.java:376) 
at 
org.jboss.as.txn.service.internal.tsr.JCAOrderedLastSynchronizationList.beforeCompletion(JCAOrderedLastSynchronizationList.java:130) 
at 
com.arjuna.ats.internal.jta.resources.arjunacore.SynchronizationImple.beforeCompletion(SynchronizationImple.java:76) 
at 
com.arjuna.ats.arjuna.coordinator.TwoPhaseCoordinator.beforeCompletion(TwoPhaseCoordinator.java:371) 
at 
com.arjuna.ats.arjuna.coordinator.TwoPhaseCoordinator.end(TwoPhaseCoordinator.java:91) 
at com.arjuna.ats.arjuna.AtomicAction.commit(AtomicAction.java:162) 
at 
com.arjuna.ats.internal.jta.transaction.arjunacore.TransactionImple.commitAndDisassociate(TransactionImple.java:1200) 
at 
com.arjuna.ats.internal.jta.transaction.arjunacore.BaseTransaction.commit(BaseTransaction.java:126) 
at 
com.arjuna.ats.jbossatx.BaseTransactionManagerDelegate.commit(BaseTransactionManagerDelegate.java:89) 
at 
org.jboss.as.ejb3.inflow.MessageEndpointInvocationHandler.afterDelivery(MessageEndpointInvocationHandler.java:71) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:498) 
at 
org.jboss.as.ejb3.inflow.AbstractInvocationHandler.handle(AbstractInvocationHandler.java:60) 
at 
org.jboss.as.ejb3.inflow.MessageEndpointInvocationHandler.doInvoke(MessageEndpointInvocationHandler.java:135) 
at 
org.jboss.as.ejb3.inflow.AbstractInvocationHandler.invoke(AbstractInvocationHandler.java:73) 
at 
org.jboss.as.test.jbossts.crashrec.jms.mdb.JMSCrashMessageDrivenBean$$$endpoint1.afterDelivery(Unknown 
Source) 
at 
org.apache.activemq.artemis.ra.inflow.ActiveMQMessageHandler.onMessage(ActiveMQMessageHandler.java:321) 
at 
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl.callOnMessage(ClientConsumerImpl.java:932) 
at 
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl.access$400(ClientConsumerImpl.java:47) 
at 
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl$Runner.run(ClientConsumerImpl.java:1045) 
at 
org.apache.activemq.artemis.utils.OrderedExecutorFactory$OrderedExecutor$ExecutorTask.run(OrderedExecutorFactory.java:100) 
at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
at java.lang.Thread.run(Thread.java:745)","Decomposed Bugs:  
B: The broker transaction timeout returns the wrong error code (XAER_PROTO) when it occurs before the TM transaction timeout expires, instead of the more appropriate RBTIMEOUT.  
B: The current implementation does not return the most specific error code (RBTIMEOUT) as recommended by the XA specification when the broker transaction timeout occurs before the TM transaction timeout.  
B: The XA specification states that the resource manager should return the most specific error code possible, which in this scenario is RBTIMEOUT, but the current implementation returns XAER_PROTO.  
B: The XA specification implies that the transaction branch is still considered live after xa_end() is called, but the resource manager dissociates the branch and marks it rollback-only, leading to the XAER_PROTO exception.  
B: The exception XAER_PROTO is thrown during delistResource in TransactionImple, which is not consistent with the expected behavior of returning RBTIMEOUT when the broker transaction timeout occurs.",Major,FALSE,Over-decomposition
CouchDB fails to bind to IPv6 address on Windows,https://issues.apache.org/jira/browse/COUCHDB-1032,"I'm trying to bind to IPv6 address :: but couchDB didn't start. 

I've tried with 
bind_address = :: 
and with the specific IPv6 address. 

LOG level setted do debug ########################### 
Erlang R14A (erts-5.8) [source] [smp:2:2] [rq:2] [async-threads:0] 

Eshell V5.8 (abort with ^G) 
1> Apache CouchDB 1.0.1 (LogLevel=debug) is starting. 
Configuration Settings [""../etc/couchdb/default.ini"", 
""../etc/couchdb/local.ini""]: 
[admins] **LINE REMOVED** 
[admins] **LINE REMOVED** 
[attachments] compressible_types=""text/*, application/javascript, application/json, application/xml"" 
[attachments] compression_level=""8"" 
[couch_httpd_auth] auth_cache_size=""50"" 
[couch_httpd_auth] authentication_db=""_users"" 
[couch_httpd_auth] authentication_redirect=""/_utils/session.html"" 
[couch_httpd_auth] require_valid_user=""false"" 
[couch_httpd_auth] **LINE REMOVED** 
[couch_httpd_auth] timeout=""600"" 
[couchdb] database_dir=""../var/lib/couchdb"" 
[couchdb] delayed_commits=""true"" 
[couchdb] max_attachment_chunk_size=""4294967296"" 
[couchdb] max_dbs_open=""100"" 
[couchdb] max_document_size=""4294967296"" 
[couchdb] os_process_timeout=""5000"" 
[couchdb] uri_file=""../var/lib/couchdb/couch.uri"" 
[couchdb] util_driver_dir=""../lib/couch-1.0.1/priv/lib"" 
[couchdb] view_index_dir=""../var/lib/couchdb"" 
[daemons] auth_cache="" 

{couch_auth_cache, start_link, []} 
"" 
[daemons] db_update_notifier="" 

{couch_db_update_notifier_sup, start_link, []} 
"" 
[daemons] external_manager="" 

{couch_external_manager, start_link, []} 
"" 
[daemons] httpd="" 

{couch_httpd, start_link, []} 
"" 
[daemons] query_servers="" 

{couch_query_servers, start_link, []} 
"" 
[daemons] stats_aggregator="" 

{couch_stats_aggregator, start, []} 
"" 
[daemons] stats_collector="" 

{couch_stats_collector, start, []} 
"" 
[daemons] uuids="" 

{couch_uuids, start, []} 
"" 
[daemons] view_manager="" 

{couch_view, start_link, []} 
"" 
[httpd] allow_jsonp=""false"" 
[httpd] authentication_handlers="" 

{couch_httpd_oauth, oauth_authentication_handler} 
, 

{couch_httpd_auth, cookie_authentication_handler} 
, 

{couch_httpd_auth, default_authentication_handler} 
"" 
[httpd] bind_address=""::"" 
[httpd] default_handler="" 

{couch_httpd_db, handle_request} 
"" 
[httpd] max_connections=""2048"" 
[httpd] port=""5984"" 
[httpd] secure_rewrites=""true"" 
[httpd] vhost_global_handlers=""_utils, _uuids, _session, _oauth, _users"" 
[httpd_db_handlers] _changes="" 

{couch_httpd_db, handle_changes_req} 
"" 
[httpd_db_handlers] _compact="" 

{couch_httpd_db, handle_compact_req} 
"" 
[httpd_db_handlers] _design="" 

{couch_httpd_db, handle_design_req} 
"" 
[httpd_db_handlers] _temp_view="" 

{couch_httpd_view, handle_temp_view_req} 
"" 
[httpd_db_handlers] _view_cleanup="" 

{couch_httpd_db, handle_view_cleanup_req} 
"" 
[httpd_design_handlers] _info="" 

{couch_httpd_db, handle_design_info_req} 
"" 
[httpd_design_handlers] _list="" 

{couch_httpd_show, handle_view_list_req} 
"" 
[httpd_design_handlers] _rewrite="" 

{couch_httpd_rewrite, handle_rewrite_req} 
"" 
[httpd_design_handlers] _show="" 

{couch_httpd_show, handle_doc_show_req} 
"" 
[httpd_design_handlers] _update="" 

{couch_httpd_show, handle_doc_update_req} 
"" 
[httpd_design_handlers] _view="" 

{couch_httpd_view, handle_view_req} 
"" 
[httpd_global_handlers] /="" 

{couch_httpd_misc_handlers, handle_welcome_req, <<\""Welcome\"">>} 
"" 
[httpd_global_handlers] _active_tasks="" 

{couch_httpd_misc_handlers, handle_task_status_req} 
"" 
[httpd_global_handlers] _all_dbs="" 

{couch_httpd_misc_handlers, handle_all_dbs_req} 
"" 
[httpd_global_handlers] _config="" 

{couch_httpd_misc_handlers, handle_config_req} 
"" 
[httpd_global_handlers] _log="" 

{couch_httpd_misc_handlers, handle_log_req} 
"" 
[httpd_global_handlers] _oauth="" 

{couch_httpd_oauth, handle_oauth_req} 
"" 
[httpd_global_handlers] _replicate="" 

{couch_httpd_misc_handlers, handle_replicate_req} 
"" 
[httpd_global_handlers] _restart="" 

{couch_httpd_misc_handlers, handle_restart_req} 
"" 
[httpd_global_handlers] _session="" 

{couch_httpd_auth, handle_session_req} 
"" 
[httpd_global_handlers] _stats="" 

{couch_httpd_stats_handlers, handle_stats_req} 
"" 
[httpd_global_handlers] _utils="" 

{couch_httpd_misc_handlers, handle_utils_dir_req, \""../share/couchdb/www\""} 
"" 
[httpd_global_handlers] _uuids="" 

{couch_httpd_misc_handlers, handle_uuids_req} 
"" 
[httpd_global_handlers] favicon.ico="" 

{couch_httpd_misc_handlers, handle_favicon_req, \""../share/couchdb/www\""} 
"" 
[log] file=""../var/log/couchdb/couch.log"" 
[log] include_sasl=""true"" 
[log] level=""debug"" 
[query_server_config] reduce_limit=""true"" 
[query_servers] javascript=""./couchjs.exe ../share/couchdb/server/main.js"" 
[replicator] max_http_pipeline_size=""10"" 
[replicator] max_http_sessions=""10"" 
[stats] rate=""1000"" 
[stats] samples=""[0, 60, 300, 900]"" 
[uuids] algorithm=""sequential"" 
Failure to start Mochiweb: eafnosupport 
[error] [<0.106.0>] {error_report,<0.34.0>, 
{<0.106.0>,crash_report, 
[[{initial_call,{mochiweb_socket_server,init,['Argument__1']}}, 
{pid,<0.106.0>} 
, 
{registered_name,[]} 
, 
{error_info,{exit,eafnosupport, 
[ 

{gen_server,init_it,6} 
, 
{proc_lib,init_p_do_apply,3} 
]}}, 
{ancestors,[couch_secondary_services,couch_server_sup, <0.35.0>]} 
, 
{messages,[]} 
, 
{links,[<0.89.0>]} 
, 
{dictionary,[]} 
, 
{trap_exit,true} 
, 
{status,running} 
, 
{heap_size,1597} 
, 
{stack_size,24} 
, 
{reductions,352} 
], 
[]]}} 

=CRASH REPORT==== 20-Jan-2011::09:39:55 === 
crasher: 
initial call: mochiweb_socket_server:init/1 
pid: <0.106.0> 
registered_name: [] 
exception exit: eafnosupport 
in function gen_server:init_it/6 
ancestors: [couch_secondary_services,couch_server_sup,<0.35.0>] 
messages: [] 
links: [<0.89.0>] 
dictionary: [] 
trap_exit: true 
status: running 
heap_size: 1597 
stack_size: 24 
reductions: 352 
neighbours: 
[error] [<0.89.0>] {error_report,<0.34.0>, 
{<0.89.0>,supervisor_report, 
[{supervisor,{local,couch_secondary_services}}, 
{errorContext,start_error}, 
{reason,eafnosupport}, 
{offender,[{pid,undefined}, 
{name,httpd}, 
{mfargs,{couch_httpd,start_link,[]}}, 
{restart_type,permanent}, 
{shutdown,1000}, 
{child_type,worker}]}]}} 

=SUPERVISOR REPORT==== 20-Jan-2011::09:39:55 === 
Supervisor: {local,couch_secondary_services} 
Context: start_error 
Reason: eafnosupport 
Offender: [{pid,undefined}, 
{name,httpd}, 
{mfargs,{couch_httpd,start_link,[]}}, 
{restart_type,permanent}, 
{shutdown,1000}, 
{child_type,worker}] 

[error] [<0.81.0>] {error_report,<0.34.0>, 
{<0.81.0>,supervisor_report, 
[{supervisor,{local,couch_server_sup}}, 
{errorContext,start_error} 
, 
{reason,shutdown} 
, 
{offender, 
[ 

{pid,undefined}, 
{name,couch_secondary_services}, 
{mfargs,{couch_server_sup,start_secondary_services,[]}}, 
{restart_type,permanent}, 
{shutdown,infinity}, 
{child_type,supervisor}]}]}} 

=SUPERVISOR REPORT==== 20-Jan-2011::09:39:55 === 
Supervisor: {local,couch_server_sup} 
Context: start_error 
Reason: shutdown 
Offender: [{pid,undefined} 
, 
{name,couch_secondary_services} 
, 
{mfargs,{couch_server_sup,start_secondary_services,[]}}, 
{restart_type,permanent} 
, 
{shutdown,infinity} 
, 
{child_type,supervisor} 
] 

=CRASH REPORT==== 20-Jan-2011::09:39:55 === 
crasher: 
initial call: application_master:init/4 
pid: <0.34.0> 
registered_name: [] 
exception exit: {bad_return, 
{{couch_app,start, 
[normal, 
[""../etc/couchdb/default.ini"", 
""../etc/couchdb/local.ini""]]}, 
{'EXIT', 
badmatch,{error,shutdown, 
[ 

{couch_server_sup,start_server,1}, 
{application_master,start_it_old,4}]}}}} 
in function application_master:init/4 
ancestors: [<0.33.0>] 
messages: [{'EXIT',<0.35.0>,normal}] 
links: [<0.33.0>,<0.6.0>] 
dictionary: [] 
trap_exit: true 
status: running 
heap_size: 610 
stack_size: 24 
reductions: 422 
neighbours: 

=INFO REPORT==== 20-Jan-2011::09:39:55 === 
application: couch 
exited: {bad_return,{{couch_app,start, 
[normal, 
[""../etc/couchdb/default.ini"", 
""../etc/couchdb/local.ini""]]}, 
{'EXIT',badmatch,{error,shutdown, 
[{couch_server_sup,start_server,1} 
, 
{application_master,start_it_old,4} 
]}}}} 
type: temporary 
1>","Decomposed Bugs:  
B: CouchDB fails to start when binding to the IPv6 address `::` with the error `eafnosupport`.  
B: The configuration setting `bind_address = ::` in CouchDB does not work as expected, causing the server to fail to start.  
B: The error logs indicate a failure in starting Mochiweb with the error `eafnosupport` when attempting to bind to an IPv6 address.  
B: The supervisor report shows a start error with the reason `eafnosupport` when attempting to start the `httpd` service with an IPv6 address.  
B: The crash report indicates that the `couch_secondary_services` supervisor failed to start due to the `eafnosupport` error.  
B: The `couch_server_sup` supervisor reported a shutdown error due to the failure of `couch_secondary_services` to start.  
B: The application `couch` exited with a `bad_return` error due to the failure of `couch_server_sup` to start the server.",Critical,FALSE,Over-analysis
Access Denied Exceptions fill up the logs with tracebacks that give no additional information,https://issues.apache.org/jira/browse/SLING-1727,"All errors in AbstractSlingPostOperation are logged with full tracebacks, however AccessDeniedExceptions all happen on the Save operation and so the traceback just fills the log up without providing any extra information 

The traceback should happen at debug level, with a one line message at info level. 

currently the traceback is 

03.09.2010 11:02:27.298 ERROR [0:0:0:0:0:0:0:1%0 [1283508147295] POST /test/authztest/node1283508146/childnode.html HTTP/1.1] org.apache.sling.servlets.post.impl.operations.ModifyOperation Exception during response processing. javax.jcr.AccessDeniedException: /test/authztest/node1283508146/childnode/user2-1283508146: not allowed to add or modify item 
at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:411) 
at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1097) 
at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920) 
at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109) 
at $Proxy11.save(Unknown Source) 
at org.apache.sling.servlets.post.AbstractSlingPostOperation.run(AbstractSlingPostOperation.java:125) 
at org.apache.sling.servlets.post.impl.SlingPostServlet.doPost(SlingPostServlet.java:242) 
at org.apache.sling.api.servlets.SlingAllMethodsServlet.mayService(SlingAllMethodsServlet.java:148) 
at org.apache.sling.api.servlets.SlingSafeMethodsServlet.service(SlingSafeMethodsServlet.java:344) 
at org.apache.sling.api.servlets.SlingSafeMethodsServlet.service(SlingSafeMethodsServlet.java:375) 
at org.apache.sling.engine.impl.request.RequestData.service(RequestData.java:523) 
at org.apache.sling.engine.impl.SlingMainServlet.processRequest(SlingMainServlet.java:427) 
at org.apache.sling.engine.impl.filter.RequestSlingFilterChain.render(RequestSlingFilterChain.java:48) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:64) 
at org.apache.sling.engine.impl.debug.RequestProgressTrackerLogFilter.doFilter(RequestProgressTrackerLogFilter.java:59) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.batch.RequestEventsFilter.doFilter(RequestEventsFilter.java:96) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.files.pool.ContentPoolFilter.doFilter(ContentPoolFilter.java:78) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.privacy.RestPrivacyFilter.doFilter(RestPrivacyFilter.java:81) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.persistence.TransactionManagerFilter.doFilter(TransactionManagerFilter.java:95) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.cluster.ClusterTrackingFilter.doFilter(ClusterTrackingFilter.java:87) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:313) 
at org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:207) 
at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502) 
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389) 
at org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:64) 
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181) 
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765) 
at org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:111) 
at org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:64) 
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) 
at org.mortbay.jetty.Server.handle(Server.java:324) 
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535) 
at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:880) 
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747) 
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218) 
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) 
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409) 
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)`","Decomposed Bugs:  
B: All errors in AbstractSlingPostOperation are logged with full tracebacks, which fills the log without providing extra information. AccessDeniedExceptions occur on the Save operation, and the traceback should happen at debug level with a one-line message at info level.  
B: The current traceback for AccessDeniedException is logged at ERROR level, which is unnecessary and clogs the log. The traceback should be logged at DEBUG level instead.  
B: A one-line message should be logged at INFO level for AccessDeniedException to provide sufficient context without the full traceback.",Major,FALSE,Over-analysis
Unable to create profile with consent info,https://issues.apache.org/jira/browse/UNOMI-238,"Hi, 
I'm not able to create a profile including its consents properties. 
If I try to add the ""consents"" property to the profile, Unomi returns a 500 status code. Looking at the docs, I've seen that the only way to add consents is by posting an event on a given session. 

I currently have millions of profiles to be imported and adding their consent during profile creation would be awesome.","Decomposed Bugs:  
B: Unomi returns a 500 status code when trying to add the ""consents"" property to a profile during profile creation.  
B: The only way to add consents is by posting an event on a given session, as per the documentation.  
B: There is a need to support adding consents during profile creation for importing millions of profiles efficiently.",Critical,FALSE,Over-decomposition
Build failure on FreeBSD with CMake,https://issues.apache.org/jira/browse/XERCESC-2109,"{{{ 
19:20:26 cd /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build/src && /usr/bin/CC -DHAVE_CONFIG_H=1 -DXERCES_BUILDING_LIBRARY=1 -D_FILE_OFFSET_BITS=64 -D_THREAD_SAFE=1 -Dxerces_c_EXPORTS -I/usr/local/include -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build/src -isystem /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/stage/include -Wall -Wcast-align -Wcast-qual -Wctor-dtor-privacy -Wextra -Wformat=2 -Wimplicit-atomic-properties -Wmissing-declarations -Wno-long-long -Woverlength-strings -Woverloaded-virtual -Wredundant-decls -Wreorder -Wswitch-default -Wunused-variable -Wwrite-strings -Wno-variadic-macros -fstrict-aliasing -msse2 -O3 -DNDEBUG -fPIC -pthread -std=gnu++14 -o CMakeFiles/xerces-c.dir/xercesc/util/Base64.cpp.o -c /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp 
19:20:26 /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp:149:14: error: use of undeclared identifier 'XERCES_SIZE_MAX' 
19:20:26 else if (XERCES_SIZE_MAX - inputLength < 2) 

{ 19:20:26 ^ 19:20:26 1 error generated. }} 
}","Decomposed Bugs:  
B: The build process fails with an error due to the use of an undeclared identifier 'XERCES_SIZE_MAX' in the file `/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp` at line 149.  
B: The identifier 'XERCES_SIZE_MAX' is not declared, causing the compilation to fail with the error: `error: use of undeclared identifier 'XERCES_SIZE_MAX'`.",Major,FALSE,Over-analysis
Directory Studio doesn't use the SASL confidentiality layer after negotiating its use,https://issues.apache.org/jira/browse/DIRSTUDIO-1220,"There is an issue connecting to an OpenLDAP server configured with olcSaslSecProps: noplain,noanonymous,minssf=1 

i.e. The server requires some form of transport encryption. Having a different issue with StartTLS (DIRSTUDIO-1219), I tried relying on the SASL confidentiality layer that SASL's GSSAPI mechanism can provide, to meet the requirement for encryption. I have chosen ""No encryption"" i.e. no SSL or StartTLS, in the Network Parameters, and then GSSAPI authentication method and Quality of Protection: Authentication with integrity and privacy protection in the SASL settings. 

When connecting to the server, what I can see happening when looking at the network traffic with Wireshark is: 

Client obtains a Kerberos service ticket for the LDAP server and passes it in the bind request for SASL GSSAPI authentication 
Server replies with a bind response, continuing SASL GSSAPI authentication, result code 14 (SASL bind in progress), with a 4 byte message wrapped using GSS_Wrap. The 4 bytes are 0x06 0x01 0x00 0x00 - referring to RFC4752, the first byte indicates the server supports ""Integrity protection"" and/or ""Confidentiality protection"" but not ""No security layer"", as expected. 
Client replies with a bind request, continuing SASL GSSAPI authentication, with a 4 byte message wrapped using GSS_Wrap. The 4 bytes are 0x04 0x01 0x00 0x00 - again referring to RFC4752, the first byte indicates the client has selected ""Confidentiality protection"". 
Server replies with a bind response with result code 0 (success). 
Client sends a search request with base DN: """", scope: base, filter: (objectClass=), for attributes: subschemaSubentry, **with no confidentiality protection*. This is the point where the client violates the protocol described in RFC4752 - after negotiating confidentiality protection, the client needs to actually use it! 
Server interprets the lack of confidentiality protection as an error and immediately drops the connection (this makes sense from the server's POV as it could indicate an attempted man-in-the-middle attack) 
Client immediately re-connects to the server, *doesn't bother to bind at all* and then issues more search requests on the base object, cn=Subschema, etc. 
An error message appears in Directory Studio ""Error while opening connection 
- Missing schema location in RootDSE, using default schema"" - this is presumably because the connection isn't bound, and the server limits what it will disclose to un-bound clients. 

Directory Studio can't browse the directory at all because it's not properly bound. 
As you can see, there's possibly two issues here - definitely an issue with the SASL GSSAPI mechanism, and possibly also an issue with the reconnect logic.","Decomposed Bugs:  
B: The client violates the protocol described in RFC4752 by sending a search request with no confidentiality protection after negotiating confidentiality protection during SASL GSSAPI authentication.  
B: The server interprets the lack of confidentiality protection as an error and immediately drops the connection, which is expected behavior to prevent potential man-in-the-middle attacks.  
B: The client reconnects to the server without binding and issues search requests on the base object, cn=Subschema, etc., leading to an unbound connection.  
B: Directory Studio displays an error message ""Error while opening connection - Missing schema location in RootDSE, using default schema"" due to the unbound connection, preventing browsing of the directory.  
B: There is an issue with the reconnect logic in Directory Studio, as it does not attempt to bind again after reconnecting to the server.",Major,FALSE,Over-decomposition
/ImageMask true does not work. Patch included.,https://issues.apache.org/jira/browse/PDFBOX-1445,"I have the following pdf... 

10 0 obj 
<< 
/Type /Page 
/MediaBox [ 0 0 612.0 792.0 ] 
/Parent 3 0 R 
/Resources << /XObject << /Obj4 4 0 R /Obj5 5 0 R /Obj6 6 0 R /Obj7 7 0 R >> /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ] >> 
/Contents [ 8 0 R 9 0 R ] 
>> 
endobj 

Which then draws 4 images. The first one is a ""base"" and then rest are image masks 

9 0 obj 
<< /Filter /FlateDecode /Length 121 >> 
stream 
q 
612.00 0 0 792.00 0.00 0.00 cm 
/Obj4 Do 
Q 
q 
0.129 g 
524.16 0 0 556.80 48.00 127.68 cm 
/Obj5 Do 
Q 
q 
0.302 g 
220.80 0 0 398.40 48.00 286.08 cm 
/Obj6 Do 
Q 
q 
0.204 g 
524.16 0 0 469.44 48.00 185.28 cm 
/Obj7 Do 
Q 
endstream 
endobj 

4 0 obj 
<< /Type /XObject /Subtype /Image /Width 1275 /Height 1650 /BitsPerComponent 8 
/ColorSpace /DeviceGray /Filter [ /FlateDecode /DCTDecode ] /Length 50485 >> 
stream 
endstream 
endobj 

5 0 obj 
<< /Type /XObject /Subtype /Image /Width 2184 /Height 2320 /BitsPerComponent 1 
/ImageMask true /Filter /CCITTFaxDecode /DecodeParms << /K -1 /Columns 2184 >> 
/Length 15580 >> 
stream 

etc ... 

The current code simply treats the imagemask as an image. Since this is just a 1 bit image it has no Alpha channel it overwrites the existing image and we simply get the last image drawn. 

In 

org.apache.pdfbox.util.operator.pagedrawer.Invoke.java 

method 

public void process(PDFOperator operator, List<COSBase> arguments) throws IOException 

after 

if (awtImage == null) 

{ LOG.warn(""getRGBImage returned NULL""); return;//TODO PKOCH } 
If you add the following code it fixes the problem. I can not provide the sample doc due to privacy reasons. 

/** 

Spec 8.9.6.2 
If ImageMask is true then the image is one bit. Black means draw the current colour and white means use the colour on the current image (ie Mask). 
Convert the map to an image with an Alpha channel so we can lay it on top 
*/ 
if(image.getImageMask()) 
{ 
Color currentColour = drawer.getGraphicsState().getStrokingColor().getJavaColor(); 
final int onColour = 0xff000000 | currentColour.getRGB(); 
BufferedImage bia = new BufferedImage(awtImage.getWidth(),awtImage.getHeight(),BufferedImage.TYPE_INT_ARGB); 
for(int y=0;y<awtImage.getHeight();y++) 
Unknown macro: { for(int x=0;x<awtImage.getWidth();x++) { bia.setRGB(x, y, (awtImage.getRGB(x, y) & 0x00ffffff) == 0xffffff ? 0x00ffffff : onColour); } } 
awtImage = bia; 
}","Decomposed Bugs:  
B: The current code treats the imagemask as an image, but since it is a 1-bit image with no Alpha channel, it overwrites the existing image, resulting in only the last image being drawn.  
B: In `org.apache.pdfbox.util.operator.pagedrawer.Invoke.java`, the method `public void process(PDFOperator operator, List<COSBase> arguments) throws IOException` needs to handle imagemasks correctly by converting the 1-bit image to an image with an Alpha channel to allow proper layering.  
B: The fix involves adding code to check if `image.getImageMask()` is true, then creating a new `BufferedImage` with an Alpha channel and setting the appropriate colors based on the current color and the mask values.",Major,FALSE,Over-analysis
build-couchdb build fails in SpiderMonkey on Debian Lenny 32-bit,https://issues.apache.org/jira/browse/COUCHDB-938,"I've followed the steps on couch.io for building without dependency hell on Debian Lenny 32-bit. However, this is a linux-vserver which runs on a 64-bit Ubuntu 10.04 host system. Maybe the configure script is making the wrong assumptions about the word size? 

(07:29:17) frank [mika]:~/git/build-couchdb# file /bin/bash 
/bin/bash: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.8, stripped 

git clone git://github.com/couchone/build-couchdb 
cd build-couchdb 
git submodule init 
git submodule update 
rake 

... 

--------- ## 
Platform. ## 
--------- ## 
hostname = frank 
uname -m = x86_64 
uname -r = 2.6.32-25-vserver 
uname -s = Linux 
uname -v = #44~ppa2-Ubuntu SMP Wed Oct 27 21:12:19 UTC 2010 

... 

c++ -o jsapi.o -c -I./dist/system_wrappers_js -include /root/git/build-couchdb/dependencies/js_src/config/gcc_hidden.h -DOSTYPE=\""Linux2.6\"" -DOSARCH=Linux -DEXPORT_JS_API -I/root/git/build-couchdb/dependencies/js_src -I. -I./dist/include -I./dist/include/nsprpub -I/root/git/build-couchdb/dependencies/js_src -fPIC -fno-rtti -fno-exceptions -Wall -Wpointer-arith -Woverloaded-virtual -Wsynth -Wno-ctor-dtor-privacy -Wno-non-virtual-dtor -Wcast-align -Wno-invalid-offsetof -Wno-variadic-macros -Wno-long-long -pedantic -fno-strict-aliasing -pthread -pipe -DNDEBUG -DTRIMMED -O3 -fstrict-aliasing -DMOZILLA_CLIENT -include ./js-confdefs.h -Wp,-MD,.deps/jsapi.pp /root/git/build-couchdb/dependencies/js_src/jsapi.cpp 
In file included from /root/git/build-couchdb/dependencies/js_src/nanojit/nanojit.h:277, 
from /root/git/build-couchdb/dependencies/js_src/jsbuiltins.h:45, 
from /root/git/build-couchdb/dependencies/js_src/jsapi.cpp:59: 
/root/git/build-couchdb/dependencies/js_src/nanojit/Containers.h:164: error: integer constant is too large for 'long' type 
make[1]: *** [jsapi.o] Error 1 
make[1]: Leaving directory `/tmp/tracemonkey_build20101107-10375-oeuo6o' 
make: *** [default] Error 2 
git checkout HEAD configure.in 
git clean -df . 
Removing configure 
rake aborted! 
Command failed with status (2): [make ...] 

(See full trace by running task with --trace)","Decomposed Bugs:  
B: The configure script may be making incorrect assumptions about the word size when building on a 32-bit Linux-vserver running on a 64-bit Ubuntu 10.04 host system.  
B: The build process fails with the error: ""integer constant is too large for 'long' type"" in the file `/root/git/build-couchdb/dependencies/js_src/nanojit/Containers.h:164`.  
B: The build process fails with the error: ""make[1]: *** [jsapi.o] Error 1"" and ""make: *** [default] Error 2"".  
B: The `rake` command aborts with the error: ""Command failed with status (2): [make ...]"" when attempting to build CouchDB.",Major,FALSE,Over-analysis
ValidField component creates javascript referencing handle_invalid_field() method that no longer exists,https://issues.apache.org/jira/browse/TAPESTRY-649,"Here is the html output from the <body>. I have a @FormBorder component 
which includes a form that encloses the actual form inputs. The handle_invalid_field() function does not get created. 

<script type=""text/javascript"" 
src=""/app?digest=b4909c59529064c46eb8843b65911500&path=%2Forg% 
2Fapache%2Ftapestry%2Fform%2FForm.js&service=asset""></script> 
<script type=""text/javascript""><!-- 

function validate_name(event) 
{ 
var field = document.Form.name; 

if (field.value.length == 0) 

{ handle_invalid_field(event, field, ""You must enter a value for Name.""); return; } 
} 

// --></script> 

<div class=""page""> 

<div class=""maindiv""> 

<div class=""topnav""> 
<div class=""globalnav""><span class=""links""><a href=""#"">Contact Us</a| <a href=""#"">Sitemap</a| <a href=""#"">Search</a| <a href=""#"">Join Us</a></span></div> 
<div class=""logo""><a href=""#""><img src=""images/logo_ulifeline.gif"" width=""219"" height=""52"" alt="""" border=""0"" /></a></div> 
<div class=""mitmed""><img src=""images/logo_mitmed.gif"" width=""139"" height=""21"" alt="""" /></div> 
<img src=""images/topwave.png"" width=""798"" height=""60"" alt="""" class=""bottomwave"" /> 
</div> 

<div class=""c-content""> 

<div class=""leftcol""> 
<p class=""navbutton""><a href=""#"" class=""button"">Log Out</a></p> 
<!-- <div class=""stylebuttonout""><div class=""stylebuttonin""><a href=""#"">Log Out</a></div></div--> 
<div class=""leftnav""> 
<table class=""navigation"" cellpadding=""0"" cellspacing=""0""> 
<tr><td><a href=""/app?component=%24AdminBorder.%24DirectLink&page=admin%2FAddSpecialty&service=direct"">Ulifeline Administrator Home</a></td></tr> 
<tr><td><a href=""/app?component=%24AdminBorder.%24DirectLink_0&page=admin%2FAddSpecialty&service=direct"" id=""nav_specialties"">Specialties</a></td></tr> 

</table> 
</div> 
</div> 

<div class=""contentarea""> 

<h1 id=""pagetitle"">Add Specialty</h1> 

<form method=""post"" action=""/app"" name=""Form"" id=""Form""> 
<div><input type=""hidden"" name=""formids"" value=""Hidden,Hidden_0,name,Checkbox,Submit""/> 
<input type=""hidden"" name=""component"" value=""formBorder.$Form""/> 
<input type=""hidden"" name=""page"" value=""admin/AddSpecialty""/> 
<input type=""hidden"" name=""service"" value=""direct""/> 
<input type=""hidden"" name=""submitmode"" value=""""/> 
<input type=""hidden"" name=""Hidden"" value=""0""/> 
<input type=""hidden"" name=""Hidden_0"" value=""0""/> 
</div> 
<table class=""form""> 

<tr> 
<th><label for=""name"">Name</label></th> 
<td><input type=""text"" name=""name"" id=""name""/></td> 
</tr> 
<tr> 
<th>Active</th> 
<td><input type=""checkbox"" name=""Checkbox"" id=""Checkbox""/></td> 
</tr> 

</table> 

<div class=""formsubmit""> 
<input type=""submit"" name=""Submit"" value=""Add Specialty"" id=""Submit"" class=""button""/> 
</div> 
</form> 

</div> 

</div> 

<hr /> 

<div class=""footer""> 
<span class=""left""> 
<span class=""right""> 
_Lifeline<br /> 
<a href=""#"">Terms of Use</a| <a href=""#"">Privacy Statement</a> 
</span> 
University Specific Contact Info goes here. Lorem ipsum dolor sit amet:<br /> 
Phone: 555-555-1234 Online: <a href=""#"">www.somwebaddress.edu</a> 
</span> 
</div> 

</div> 

</div> 

<script language=""JavaScript"" type=""text/javascript""><!-- 
Tapestry.register_form('Form'); 

Tapestry.onsubmit('Form', validate_name); 

Tapestry.set_focus('name'); 

// --></script></body>","Decomposed Bugs:  
B: The `handle_invalid_field()` function is not created, which is required for the validation logic in the `validate_name()` function.  
B: The `validate_name()` function in the script does not have the `handle_invalid_field()` function defined, causing validation to fail when the ""Name"" field is empty.  
B: The HTML output includes a form with a `@FormBorder` component, but the validation logic for the ""Name"" field is incomplete due to the missing `handle_invalid_field()` function.",Major,FALSE,Over-analysis
"Canonical-ize hostnames for Hive metastore, and HS2 servers.",https://issues.apache.org/jira/browse/HIVE-17218,"Currently, the HiveMetastoreClient and HiveConnection do not canonical-ize the hostnames of the metastore/HS2 servers. In deployments where there are multiple such servers behind a VIP, this causes a number of inconveniences: 

The client-side configuration (e.g. hive.metastore.uris in hive-site.xml) needs to specify the VIP's hostname, and cannot use a simplified CNAME, in the thrift URL. If the hive.metastore.kerberos.principal is specified using _HOST, one sees GSS failures as follows: 
hive --hiveconf hive.metastore.kerberos.principal=hive/_HOST@GRID.MYTH.NET --hiveconf hive.metastore.uris=""thrift://simplified-hcat-cname.grid.myth.net:56789"" 
... 
Exception in thread ""main"" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient 
at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:542) 
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) 
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) 
... 
This is because _HOST is filled in with the CNAME, and not the canonicalized name. 

Oozie workflows that use HCat <credential> have to always use the VIP hostname, and can't use _HOST-based service principals, if the CNAME differs from the VIP name. 
If the client-code simply canonical-ized the hostnames, it would enable the use of both simplified CNAMEs, and _HOST in service principals.","Decomposed Bugs:  
B: The HiveMetastoreClient and HiveConnection do not canonical-ize the hostnames of the metastore/HS2 servers, causing client-side configuration (e.g., hive.metastore.uris in hive-site.xml) to require specifying the VIP's hostname instead of a simplified CNAME in the thrift URL.  

B: When hive.metastore.kerberos.principal is specified using _HOST, GSS failures occur because _HOST is filled in with the CNAME instead of the canonicalized name, leading to exceptions such as:  
```  
Exception in thread ""main"" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient  
at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:542)  
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)  
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)  
```  

B: Oozie workflows that use HCat <credential> must always use the VIP hostname and cannot use _HOST-based service principals if the CNAME differs from the VIP name.  

B: If the client-code canonical-ized the hostnames, it would enable the use of both simplified CNAMEs and _HOST in service principals.",Major,FALSE,Incorrect Interpretation of Solutions
Unable to start karaf instance on fresh installation - elasticsearch error : java.lang.IllegalStateException: Received message from unsupported version: [5.2.2] minimal compatible version is: [5.6.0],https://issues.apache.org/jira/browse/UNOMI-196,"Services do not go live on a fresh installation. 

 
1. running ""bin/karaf"" results in karaf loading and then ""Initializing Unomi..."" which never finished.

2. Looking into log:display - it says: ""Unable to initialize bean elasticSearchPersistenceServiceImpl""

2018-08-23 12:55:58,615 | INFO | FelixStartLevel | RegionsPersistenceImpl | 49 - org.apache.karaf.region.persist - 3.0.8 | Loading region digraph persistence
2018-08-23 12:55:58,899 | INFO | FelixStartLevel | SecurityUtils | 30 - org.apache.sshd.core - 0.14.0 | BouncyCastle not registered, using the default JCE provider
2018-08-23 12:55:59,110 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Starting JMX OSGi agent
2018-08-23 12:55:59,157 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering MBean with ObjectName [osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9] for service with service.id [15]
2018-08-23 12:55:59,194 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.ServiceStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=serviceState,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,196 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.wiring.BundleWiringStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=wiringState,version=1.1,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,196 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.BundleStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=bundleState,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.PackageStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=packageState,version=1.5,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.service.cm.ConfigurationAdminMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.FrameworkMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=framework,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,371 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | EventAdmin support enabled, servlet events will be postet to topics.
2018-08-23 12:55:59,386 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | LogService support enabled, log events will be created.
2018-08-23 12:55:59,406 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | Pax Web started
2018-08-23 12:55:59,839 | INFO | pool-6-thread-1 | Server | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | jetty-8.1.19.v20160209
2018-08-23 12:55:59,935 | INFO | pool-6-thread-1 | AbstractConnector | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Started SelectChannelConnector@0.0.0.0:8181
2018-08-23 12:55:59,936 | INFO | pool-6-thread-1 | JettyServerImpl | 79 - org.ops4j.pax.web.pax-web-jetty - 3.2.9 | Pax Web available at [0.0.0.0]:[8181]
2018-08-23 12:55:59,964 | INFO | pool-6-thread-1 | JettyServerImpl | 79 - org.ops4j.pax.web.pax-web-jetty - 3.2.9 | Pax Web available at [0.0.0.0]:[9443]
2018-08-23 12:55:59,997 | INFO | pool-6-thread-1 | SslContextFactory | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Enabled Protocols [SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2] of [SSLv2Hello, SSLv3, TLSv1, TLSv1.1, TLSv1.2]
2018-08-23 12:55:59,999 | INFO | pool-6-thread-1 | AbstractConnector | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Started SslSelectChannelConnector@0.0.0.0:9443
2018-08-23 12:56:00,077 | INFO | FelixStartLevel | ContextLoaderListener | 106 - org.springframework.osgi.extender - 1.2.1 | Starting [org.springframework.osgi.extender] bundle v.[1.2.1]
2018-08-23 12:56:00,188 | INFO | FelixStartLevel | ExtenderConfiguration | 106 - org.springframework.osgi.extender - 1.2.1 | No custom extender configuration detected; using defaults...
2018-08-23 12:56:00,199 | INFO | FelixStartLevel | TimerTaskExecutor | 101 - org.apache.servicemix.bundles.spring-context - 3.2.17.RELEASE_1 | Initializing Timer
2018-08-23 12:56:00,833 | INFO | pache.cxf.osgi]) | HttpServiceFactoryImpl | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | Binding bundle: [org.apache.cxf.cxf-rt-transports-http [133]] to http service
2018-08-23 12:56:01,359 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer)]
2018-08-23 12:56:01,418 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer)]
2018-08-23 12:56:01,436 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:01,764 | WARN | FelixStartLevel | OSGiScriptEngineManager | 184 - com.hazelcast - 3.4.2 | Found ScriptEngineFactory candidate for com.sun.script.javascript.RhinoScriptEngineFactory, but cannot load class! -> java.lang.ClassNotFoundException: com.sun.script.javascript.RhinoScriptEngineFactory not found by com.hazelcast [184]
2018-08-23 12:56:01,827 | INFO | FelixStartLevel | XmlConfigLocator | 184 - com.hazelcast - 3.4.2 | Loading configuration /home/unomi/unomi-1.2.0-incubating/etc/hazelcast.xml from System property 'hazelcast.config'
2018-08-23 12:56:01,827 | INFO | FelixStartLevel | XmlConfigLocator | 184 - com.hazelcast - 3.4.2 | Using configuration file at /home/unomi/unomi-1.2.0-incubating/etc/hazelcast.xml
2018-08-23 12:56:02,118 | INFO | FelixStartLevel | DefaultAddressPicker | 184 - com.hazelcast - 3.4.2 | [LOCAL] [cellar] [3.4.2] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2018-08-23 12:56:02,128 | INFO | FelixStartLevel | DefaultAddressPicker | 184 - com.hazelcast - 3.4.2 | [LOCAL] [cellar] [3.4.2] Picked Address[127.0.0.1]:5701, using socket ServerSocket[addr=/0.0.0.0,localport=5701], bind any local is true
2018-08-23 12:56:02,313 | INFO | FelixStartLevel | OperationService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Backpressure is disabled
2018-08-23 12:56:02,316 | INFO | FelixStartLevel | BasicOperationScheduler | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Starting with 2 generic operation threads and 2 partition operation threads.
2018-08-23 12:56:02,369 | INFO | FelixStartLevel | system | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Hazelcast 3.4.2 (20150326 - f6349a4) starting at Address[127.0.0.1]:5701
2018-08-23 12:56:02,370 | INFO | FelixStartLevel | system | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Copyright (C) 2008-2014 Hazelcast.com
2018-08-23 12:56:02,371 | INFO | FelixStartLevel | Node | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Creating TcpIpJoiner
2018-08-23 12:56:02,372 | INFO | FelixStartLevel | LifecycleService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5701 is STARTING
2018-08-23 12:56:02,496 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Connecting to /127.0.0.1:5703, timeout: 0, bind-any: true
2018-08-23 12:56:02,497 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Could not connect to: /127.0.0.1:5703. Reason: SocketException[Connection refused to address /127.0.0.1:5703]
2018-08-23 12:56:02,497 | INFO | .cached.thread-2 | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5703 is added to the blacklist.
2018-08-23 12:56:02,498 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Connecting to /127.0.0.1:5702, timeout: 0, bind-any: true
2018-08-23 12:56:02,498 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Could not connect to: /127.0.0.1:5702. Reason: SocketException[Connection refused to address /127.0.0.1:5702]
2018-08-23 12:56:02,499 | INFO | .cached.thread-2 | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5702 is added to the blacklist.
2018-08-23 12:56:03,499 | INFO | FelixStartLevel | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2]

Members [1]

{ Member [127.0.0.1]:5701 this }
2018-08-23 12:56:03,520 | INFO | FelixStartLevel | LifecycleService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5701 is STARTED
2018-08-23 12:56:03,582 | INFO | FelixStartLevel | InternalPartitionService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Initializing cluster partition table first arrangement...
2018-08-23 12:56:03,681 | INFO | rint Extender: 2 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,682 | INFO | rint Extender: 3 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,682 | INFO | rint Extender: 1 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 3 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 1 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 2 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,927 | INFO | FelixStartLevel | BundleWatcher | 193 - org.apache.unomi.lifecycle-watcher - 1.2.0.incubating | Bundle watcher initialized.
2018-08-23 12:56:04,119 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/default.json, loading...
2018-08-23 12:56:04,119 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/campaign.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/campaignevent.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/event.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/goal.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/personaSession.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/profile.json, loading...
2018-08-23 12:56:04,121 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/propertyType.json, loading...
2018-08-23 12:56:04,121 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/rule.json, loading...
2018-08-23 12:56:04,123 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/scoring.json, loading...
2018-08-23 12:56:04,125 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/segment.json, loading...
2018-08-23 12:56:04,129 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/session.json, loading...
2018-08-23 12:56:04,156 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Connecting to ElasticSearch persistence backend using cluster name contextElasticSearch and index name context...
2018-08-23 12:56:06,682 | ERROR | FelixStartLevel | ServiceRecipe | 15 - org.apache.aries.blueprint.core - 1.6.2 | Error retrieving service from ServiceRecipe[name='elasticSearchPersistenceService']
org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:738)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate2(BeanRecipe.java:848)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:811)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.RefRecipe.internalCreate(RefRecipe.java:62)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:106)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:285)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalGetService(ServiceRecipe.java:252)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalCreate(ServiceRecipe.java:149)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:186)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:724)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:411)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1]
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:]
at java.lang.Thread.run(Thread.java:748)[:1.8.0_181]
Caused by: NoNodeAvailableException[None of the configured nodes are available: [{#transport#-1}

{o4uAdKg_QpCsuv2dzXKFyw}{localhost}{127.0.0.1:9300}]]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
[TRIMMED 50K CHAR LIMIT]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498)[:1.8.0_181]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:299)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:980)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:736)[15:org.apache.aries.blueprint.core:1.6.2]
... 35 more
2018-08-23 12:56:06,711 | WARN | FelixStartLevel | BeanRecipe | 15 - org.apache.aries.blueprint.core - 1.6.2 | Object to be destroyed is not an instance of UnwrapperedBeanHolder, type: null
2018-08-23 12:56:06,715 | ERROR | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Unable to start blueprint container for bundle org.apache.unomi.persistence-elasticsearch-core/1.2.0.incubating
org.osgi.service.blueprint.container.ComponentDefinitionException: org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:310)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalGetService(ServiceRecipe.java:252)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalCreate(ServiceRecipe.java:149)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:186)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:724)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:411)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1]
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:]
at java.lang.Thread.run(Thread.java:748)[:1.8.0_181]
Caused by: org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:738)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate2(BeanRecipe.java:848)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:811)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.RefRecipe.internalCreate(RefRecipe.java:62)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:106)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:285)[15:org.apache.aries.blueprint.core:1.6.2]
... 27 more
Caused by: NoNodeAvailableException[None of the configured nodes are available: [{#transport#-1}

{o4uAdKg_QpCsuv2dzXKFyw}
{localhost}

{127.0.0.1:9300}
]]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodA","Decomposed Bugs:  
B: Running ""bin/karaf"" results in Karaf loading and then ""Initializing Unomi..."" which never finishes.  
B: The log shows an error: ""Unable to initialize bean elasticSearchPersistenceServiceImpl.""  
B: The ElasticSearch persistence service fails to connect to the configured nodes, resulting in a NoNodeAvailableException.  
B: The Blueprint container for bundle org.apache.unomi.persistence-elasticsearch-core fails to start due to the inability to initialize the elasticSearchPersistenceServiceImpl bean.  
B: The Hazelcast service encounters connection issues, with nodes being added to the blacklist due to connection refusals.  
B: The Karaf Cellar bundles (config, bundle, features) are waiting for dependencies, including GroupManager and ClusterManager, which are not available.",Major,FALSE,Over-analysis
Need a 32×32 icon for Java options,https://issues.apache.org/jira/browse/NETBEANS-284,"The ContainerRegistration for Java options, defined in options.java/src/org/netbeans/modules/options/java/resources/package-info.java, is currently using a 20¡Á20 iconBase, whereas other options icons are 32¡Á32. This makes the icon look smaller than the others in NetBeans > Preferences. See the attached screenshot.","Decomposed Bugs:  
B: The ContainerRegistration for Java options, defined in options.java/src/org/netbeans/modules/options/java/resources/package-info.java, is currently using a 20×20 iconBase, whereas other options icons are 32×32.  
B: The smaller 20×20 icon makes the Java options icon look smaller than the others in NetBeans > Preferences.",Minor,FALSE,Over-decomposition
Hadoop configurations on the classpath seep into the S3 file system configs,https://issues.apache.org/jira/browse/FLINK-10383,"The S3 connectors are based on a self-contained shaded Hadoop. By design, they should only use config value from the Flink configuration. 

However, because Hadoop loads implicitly configs from the classpath, existing ""core-site.xml"" files can interfere with the configuration in ways intransparent for the user. We should ensure such configs are not loaded.","Decomposed Bugs:  
B: The S3 connectors are based on a self-contained shaded Hadoop and should only use config values from the Flink configuration.  
B: Hadoop implicitly loads configs from the classpath, which can cause existing ""core-site.xml"" files to interfere with the configuration in ways that are intransparent for the user.  
B: We should ensure that configs from the classpath, such as ""core-site.xml"", are not loaded.",Major,FALSE,Over-decomposition
Bandar Togel Terbaik,https://issues.apache.org/jira/browse/AAR-9319,"Bandar Togel Hadiah 4D 10 Juta Terbaik & Terpercaya | Daftar di BO Togel Terpercaya bet 100 perak 2021 
[Bandar Togel Hadiah 4D 10 Juta Terpercaya|#] terhadap Zaman sekarang ini kami telah perlu pandai pandai di dalam melacak penghasilan sampingan, apabila selama ini kamu sekedar mengandalkan penghasilan primer saja gara-gara telah berlimpah peluang yang terbuka sementara ini dan keliru satunya ialah bersama bermain di Bandar Togel Hadiah 4D 10 Juta Terbesar Dan Terpercaya Kepada kamu sekalian. Layaknya yang kamu ketahui sendiri bahwa permainan tebak angka ini udah berlimpah pengaruhi nasib seseorang jadi jutawan sekedar bersama kapital yang kecil saja, Pasaran Togel Singapore jadi tidak benar satu pasaran yang paling tak terhitung dimainkan oleh bettor sementara ini dan cocok bagi kamu yang dambakan mulai terjun di dalam global pertogelan. 

Pastinya kamu seluruh telah tau apa tersebut Togel Online sebab memang permainan BO Togel Hadiah 4d 10 juta satu ini telah terlampau populer berasal dari pernah sampai sekarang ini, nah disini kita inginkan merekomendasikan sebagian Bandar Singapore Pools yang tak terhitung dimainkan oleh para Togellers yang tersedia di Internet. Kudu diketahui bahwa sebelum kita merekomendasikan seluruh Web site Bandar Togel Hadiah 4d 10 Juta terpercaya di bawah ini udah kita coba mainkan terlebih dahulu sebelum memberitahukannya kepada kamu seluruh, bagi kamu yang tengah melacak Web Togel Terpercaya maka sanggup mempertimbangkan untuk bermain di tidak benar satu web site ini. 

Daftar Bandar Togel Online Resmi Terpercaya 2021 
Laman ini menghadirkan Daftar Bandar Togel Hadiah 4D 10 Juta Terpercaya tahun ini yang tersedia di kawasan Asia, khususnya wilayah Indonesia. Kumpulan Bandar Togel Hadiah 4D 10 Juta Terpercaya ini, akan dibahas satu persatu secara singkat dan memahami, supaya tiap tiap betor yang menonton akan jelas segera julukan web togel terpercaya yang sanggup digabung untuk daftar akun togel formal dan dijadikan partner bertaruh toto togel online. Yang tentu dan bukan barangkali mengecewakan kamu terhadap waktu ini adalah, seluruh Bandar yang kita rekomendasikan disini merupakan Bandar Togel Terpercaya hadiah terbesar. Pasti saja tersebut akan memicu untuk terhadap waktu bergabung, sebab bukan seluruh Bandar sekarang ini udah jadi Bandar Togel Hadiah 4D 10 Juta layaknya yang kita referensikan disini. 

Sehabis mempunyai account member bandar Permainan Toto Hadiah 4D 10 Juta Terpercaya Indonesia, lakukanlah pengisian game credit user id. Mampu bersama dengan transfer lewat atm, m banking, e-banking bank lokal terbesar layaknya Bca, Bni, Bri, Mandiri, CIMB Niaga ataupun deposit togel via pulsa Telkomsel. Dapat juga mengikuti kecanggihan teknologi era now bersama kenakan pelaksanaan dompet online, e-wallet layaknya Gopay, Ovo, DANA sertar Linkaja!. Agen togel deposit ovo selalu siap online 24jam melayani segala keperluan kesibukan permainan judi togel depo termurah. Pokoknya ga bakal rugi deh main di bandar togel deposit 10 ribu karna layanannya kondusif, cepat dan nyaman. 

Bandar Togel Hadiah 4D 10 Juta Terbesar Dan Terpercaya 
Pada Zaman sekarang ini kita sudah harus pintar _ pintar dalam mencari penghasilan sampingan, apabila selama ini anda hanya mengandalkan penghasilan utama saja karena sudah banyak peluang yang terbuka saat ini dan salah satunya ialah dengan bermain di [BO TOGEL HADIAH 4D 10 JUTA TERBESAR DAN TERPERCAYA|#] kepada anda sekalian. Seperti yang anda ketahui sendiri bahwa permainan tebak angka ini sudah banyak merubah nasib seseorang menjadi jutawan hanya dengan modal yang kecil saja, Pasaran Togel Singapore menjadi salah satu pasaran yang paling banyak dimainkan oleh bettor saat ini dan cocok bagi anda yang ingin mulai terjun dalam dunia pertogelan. 

Pastinya anda semua sudah tau apa itu Togel Online karena memang permainan BO Togel Hadiah 4D 10 juta satu ini sudah sangat populer dari dulu hingga sekarang ini, nah disini kami ingin merekomendasikan beberapa Bandar Singapore Pools yang banyak dimainkan oleh para Togellers yang ada di Internet. Perlu diketahui bahwa sebelum kami merekomendasikan semua Situs Bandar Togel Hadiah 4D 10 Juta terpercaya di bawah ini sudah kami coba mainkan terlebih dahulu sebelum memberitahukannya kepada anda semua, bagi anda yang sedang mencari Website Togel Terpercaya maka bisa mempertimbangkan untuk bermain di salah satu situs ini. 

Cara Daftar Akun di Situs BO Togel Terpercaya di Indonesia 24jam 
Untuk bermain permainan togel di Togel Situs Bandar Togel Terpercaya. Tentunya kalian harus memiliki akun untuk melakukan berbagai jenis transaksi dalam melakukan betting di situs togel resmi toto. Karena kalau kita tidak punya akun, Tentunya kita tidak dapat melakukan betting, deposit, maupun withdraw. Disini kami sediakan langkah-langkah khusus untuk para bettor yang akan bermain di Togel 5 Bandar Togel Terpercaya sebagia berikut : 

Pastikan anda berumur 18 Tahun lebih 
Kunjungi situs 5 Bandar Togel Terpercaya 
Klik tombol Pendaftaran / Daftar 
Isi Biodata Diri dengan baik dan benar 
Isi Username akun 
isi password sesuai selera anda 
Masukkan email yang aktif 24jam 
Masukkan no handphone yang dapat dihubungi 
Pilih Jenis pembayaran (Bank Lokal maupun E-wallet) 
Isi atas nama rekening tsb 
Masukan nomor rekening anda 
Selesai. 


Sebelum melakukan pendaftaran. Pastikan jaringan internet yang kamu gunakan cepat dan tentunya stabil. Karena untuk bermain togel online. Kamu harus mempunyai koneksi yang bagus untuk bermain maupun daftar akun togel di [Daftar Bandar Togel Terpercaya|#]. Maka dari itulah kami sarankan agar menggunkan koneksi Wi-Fi ketika bermain togel online di Situs Bandar Togel Hadiah 4d 10 Juta Terbesar. 

4 Negara Pasaran Bandar Togel Resmi Dan Terbaik Di Dunia 
Di dalam perjudian togel, tentunya tidak akan menarik apabila tidak memasang taruhan pada pasaran togel yang resmi dan terbaik di dunia. Sebab bila kita memasang pada pasaran togel sembarangan atau tidak legal di dunia. Pastinya kemenangan yang akan diraih akan sangat sulit atau bisa dikatakan itu adalah pasaran togel settingan. Sehingga apapun angka yang akan kita pasangkan maka kekalahan yang akan didapatkan. 

Lalu seperti apakah negara-negara yang menjadi pasaran togel resmi dan terbaik di dunia menurut organisasi resmi seperti WLA (World Lottery Association) dan APLA (Asia Pacific on the World Lottery Association). Tentunya seluruh bettor togel sekarang ini ingin mengetahui atau penasaran dengan nama-nama pasaran togel terbaik tersebut bukan?. 

Mari langsung saja admin dari blog seputaran permainan togel online ini akan menyebutkan 4 saja negara yang menjadi pasaran togel resmi dan terbaik di dunia menurut APLA dan WLA sebagai berikut : 

1. Pasaran Toto Macau 
Toto macau adalah pasaran togel terbaik dan resmi di dunia yang sekarang ini menjadi pasaran nomor 1 di indonesia. Dimana pada pasaran togel toto macau ini. Memiliki 3 mode jenis betting terbaik yang memudahkan seluruh bettor untuk melakukan taruhan. Bisa memilih mode dengan hadiah full terbesar ataupun mode diskon yang memiliki hadiah standart namun sewaktu melakukan taruhan modal yang dikeluarkan tidak terlalu banyak karena memiliki diskon atau potongan. 

Namun toto macau sendiri dipilih bukan karena hal sepele itu saja. Ada berbagai kelebihan dan keuntungan lainnya yang menjadikan pasaran tersebut nomor 1 di indonesia sekarang ini. Meskipun pasaran ini baru saja hadir sejak tahun 2015 silam dan paling muda dibandingkan dengan pasaran-pasaran togel lainnya seperti Hongkong (HK) , Sydney (SDY) ataupun Singapore (SGP). Tetapi pasaran ini memiliki hal yang tidak dimiliki oleh pasaran-pasaran togel tertua ataupun lawas itu. 

Biasanya pasaran togel umum atau pasaran togel dunia lainnya hanya memiliki jadwal perputaran angka result 1 kali dalam setiap harinya. Namun berbeda halnya dengan pasaran togel macau ini. Ia memiliki 4 kali perputaran angka yang live yang bisa dimainkan oleh seluruh bettor setiap harinya. Dengan hadiah yang besar dan memiliki hasil result terbanyak, tentunya hal tersebut menjadikan toto macau adalah pasaran nomor 1 di dunia sekarang ini karena tidak ada pasaran yang mengeluarkan angka sebanyak itu dalam 1 harinya. 

Selain itu ia memiliki situs resmi pengeluaran angka yang berbasis live streaming atau siaran langsung setiap jadwal atau jam tutup pasaran. Sehingga seluruh bettor di tanah air kita dapat menyaksikan secara langsung perputaran angka tersebut Oleh karena pasaran ini memiliki lisensi resmi dari pihak WLA dan APLA tentunya setiap pengeluaran angka itu di jamin aman 100% dan tidak ada kecurangan apapun. 

Buat seluruh bettor di luar sana yang masih kurang mengetahui jadwal perputaran angka atau jam result dari pasaran toto macau di Bandar Togel Hadiah 4D 10 Juta Terpercaya ini anda semua dapat melihatnya di bawah ini : 

Jam Tutup Pasaran 
12:59 WIB 
15:59 WIB 
18:59 WIB 
21:59 WIB 
Jam Tutup Pasaran 
13:15 WIB 
16:15 WIB 
19:15 WIB 
22:15 WIB 


Setiap perputaran atau bola gelinding yang akan di tampilkan pada permainan toto macau, Hanya akan mencari 4 bola terakhir yang masih berada pada meja. Sehingga kita dapat menyimpulkan dengan mudah bahwasannya angka apa saja yang akan menjadi result setiap perputaran. Dan untuk live streaming atau situs resmi toto macau dapat di lihat langsung pada website ataupun situs Bandar Togel Hadiah 4D 10 Juta Terpercaya. Pastikan anda melakukan taruhan pada pasaran terbaik ini dan menjadi salah seorang bettor ternama di indonesia tahun 2021. 

Setelah anda sudah yakin pada situs pilihan kalian, anda dapat langsung membuat akun togel online dengan melakukan pendaftaran pada Bandar Togel Hadiah 4D 10 Juta Terpercaya secara gratis. Cara daftar akun togel resmi online sangat mudah dilakukan oleh siapa saja. Yang terpenting anda bersedia memberikan beberapa data pribadi anda dan kemanan privacy nya terjamin 100%. Karna data-data tersebut sangat berguna bagi kelangsungan aktivitas judi togel uang asli di bo toto macau bet 100 diskon besar. Perangkat elektronik seperti computer atau smartphone dan koneksi internet yang stabil sangatlah dibutuhkan. Sebab itu semua merupakan alat 
Utama bila ingin bertaruh lotto secara daring. Setelah menyelesaikan proses registrasi, anda memiliki akses masuk ke semua pasaran togel online terlengkap di asia dan dunia. Ayo segera bikin akun toto dengan Bandar Togel Hadiah 4D 10 Juta Terpercaya sekarang juga! 

2. Pasaran Togel Hongkong 
Hongkong pools adalah nama pasaran togel terbaik ke-2 yang ditampilkan pada situs blog seputar permainan togel online ini. Dimana hongkong atau yang biasa dikenal dengan nama lain yaitu HK merupakan salah satu pasaran togel terlama dan terbaik di indonesia. Hampir seluruh bettor di indonesia mengenal dengan nama pasaran togel tersebut. 

Hongkong pools dulunya terkenal melalui via bandar darat togel di indonesia. Tidak seperti pasaran toto macau yang terkenal karena zaman sudah sangat modern seperti sekarang ini. Dulunya pasaran hongkong hanya dikenal oleh bettor-bettor lama maupun yang sudah tua saja. Karena dulunya pasaran togel yang aktif di indonesia hanya beberapa saja. Di antara salah satunya adalah pasaran togel hongkong sendiri. 

Pasaran yang sudah lama hadir dan memiliki lisensi atau sertifikat resmi dari pihak APLA dan WLA di dunia adalah Hongkong pools. Pasaran ini sangat diminati oleh seluruh bettor togel di indonesia karena memiliki jadwal result yang sangat cocok untuk dimainkan. Dimana jam result yang dimiliki oleh hongkong pools adalah 23.00 WIB yang merupakan adalah waktu senggang atau waktu santai untuk para masyarakat pekerja di indonesia. Sehingga dapat dimainkan tanpa harus terburu-buru. 

Tetapi jika anda semua malas atau enggan mencari satu-persatu website yang berada di internet sekarang ini. Karena ribet ataupun memakan waktu yang cukup lama. Tentunya pada situs blog togel online ini anda semua bisa memilih dari beberapa situs yang kami sajikan tersebut sebagai partner resmi anda semua dalam melakukan taruhan. Dikarenakan setiap website yang telah direferensikan pada situs ini dijamin aman dan terpercaya 100%. 

Untuk hadiah togel pada pasaran hongkong ini dulunya memiliki hadiah togel yang standar ataupun tergolong kecil. Sebab para bettor hanya dapat melakukan taruhan togel pada bandar darat di indonesia. Berbeda dengan pasaran toto macau yang memiliki hadiah togel terbesar dan spektakuler. Tentunya seiring berjalannya waktu pasaran togel pun berkembang mengikuti zaman yang modern. Dan sekarang kita bisa melakukan taruhan pada pasaran togel tersebut dengan 3 mode betting yang memiliki hadiah togel setimpal dengan pasaran-pasaran lainnya. 

3. Pasaran Togel Sydney 
Sydney pools atau biasa yang dikenal dengan nama lain oleh para bettor yaitu SDY merupakan salah satu pasaran togel resmi dan terbaik yang memiliki sertifikat dan reputasi terbaik di indonesia. Tidak seperti pasaran togel lainnya atau pasaran abal-abal di luar sana yang sewaktu-waktu bisa mengubah hasil result pada situs pengeluaran angka. Tentunya pada pasaran sydney di jamin 100% aman dan tidak ada kecurangan karena memiliki akses resmi dari pihak WLA dan APLA. 

Namun siapa sangka bahwasannya pasaran togel sydney ini menjadi salah satu pasaran togel terbaik di indonesia yang memiliki peminat judi terbanyak di indonesia. Pasaran yang berasal dari negara australia yang bertempatkan pada kota Sydney ini. memiliki jadwal result siang hari yaitu pukul 13.50 WIB dari situs resmi yaitu Bandar Togel Hadiah 4D 10 Juta Terpercaya. 

Pasaran togel sydney sama dengan pasaran hongkong ataupun singapore yang merupakan pasaran ter-lawas atau terlama di indonesia. Sehingga hampir seluruh bettor mengenal dengan pasaran yang satu ini dan di jamin 100% aman tidak adanya kecurangan apapun saat memutarkan angka. Dan pastinya pasaran yang satu ini tersedia pada seluruh website ataupun Bandar Togel Hadiah 4D 10 Juta Terpercaya Di indonesia. 

4. Pasaran Togel Singapore 
Pasaran togel terbaik ke-4 adalah singapore pools atau SGP yang dikenal oleh seluruh penjudi di tanah air indonesia. Pastinya seluruh masyarakat indonesia mengenal baik dengan pasaran yang satu ini. Baik kalangan muda maupun lanjut usia mengetahui betul bagaimana dari pasaran ini. Karena pasaran ini resmi dari negara SINGAPORE untuk perputaran lottery atau angka keluarannya. 

Selain ada akses resmi dari pihak APLA dan WLA tentunya ada berbagai organisasi resmi yang wajib dimiliki oleh tiap-tiap pasaran togel resmi dan terbaik di dunia. Sebab setiap perputaran togel yang di lakukan tersebut wajib memiliki pengamatan dari organisasi tersebut agar tidak terjadi kecurangan ataupun hal-hal yang tidak diinginkan. Dan dengan adanya organisasi tersebut hadiah togel online pun bisa menjadi sangat besar karena sponsor yang diberikan oleh pihak organisasi tersebut. 

Namun pasaran ini sempat menjadi kontroversi karena pada tahun 2020 silam pasaran ini libur cukup panjang dari bulan 4 2020 hingga akhir tahun 2021. Hal tersebut menjadikan pasaran singapore menjadi redup ataupun ditakuti oleh sebagian bettor togel di indonesia. Sebab semasa libur panjang dari negara tersebut banyak sekali munculnya website-website palsu yang mengaku-ngaku sebagai situs resmi Singapore. 

Tetapi sekarang ini Singapore atau SGP pools sudah kembali bersinar dan memberikan banyak sekali keuntungan untuk seluruh peminat judi togel di indonesia. Untuk hasil result dari pasaran ini hanya pada website Bandar Togel Hadiah 4D 10 Juta Terpercaya dan memiliki jadwal tayang yaitu 17.50 WIB untuk hari senin , rabu , kamis , sabtu dan minggu, sedangkan hari selasa dan jumat seperti biasa pasaran ini akan tutup alias libur. 

Related Keywords 

bandar togel terpercaya dan terlengkap 
bandar togel terpercaya 2020 
bandar togel online terpercaya dan berbayar 
bandar togel resmi indonesia 
bandar togel hadiah terbesar 
bandar togel hadiah prize 12345 
bandar togel resmi terbaru deposit 10 ribu 
bandar togel terbaik dan terbesar 2021 
bandar togel hadiah 4d 10 juta 
daftar bandar togel terbesar dan terpercaya 
5 bandar togel terpercaya 2021 
daftar bandar togel terbesar resmi deposit 10 ribu 
daftar bandar togel tertua 
semua bandar togel terbaik dan terpercaya no 1 
bandar lotre togel deposit via dana 
bandar togel terpercaya via gopay 
bandar togel terbesar 2021 
bandar togel 4D hadiah terbesar 
bandar togel terbaru dan terpercaya 2020 
bandar togel terbesar di dunia 
bandar togel resmi terpercaya 2021 
bandar togel terjitu di indonesia 
bandar togel terpercaya hadiah terbesar 
bandar togel terpercaya dan paling jitu 2021 
bandar togel terbesar di indonesia 
bandar togel terbaik di indonesia 
bandar togel terjitu dan terpercaya online 
Bandar Togel terpercaya indonesia via dana 
Bandar Togel terbesar via gopay 
Bandar Togel online 24 jam terpercaya 
Bandar Togel terpercaya 2020 
data togel terlengkap 2021 
Bandar Togel terbaik 
Bandar Togel termudah via ovo 
Bandar Togel terbaru 
Bandar Togel terbaru dan termurah via linkaja 
Bandar Togel terbesar dan terpercaya 
Bandar Togel terpercaya di indonesiabandar togel terpercaya dan terlengkap 
bandar togel terpercaya 2020 
bandar togel online terpercaya dan berbayar 
bandar togel formal indonesia 
bandar togel hadiah terbesar 
bandar togel hadiah prize 12345 
bandar togel formal terbaru deposit 10 ribu 
bandar togel paling baik dan terbesar 2021 
bandar togel hadiah 4d 10 juta 
daftar bandar togel terbesar dan terpercaya 
5 bandar togel terpercaya 2021 
daftar bandar togel terbesar formal deposit 10 ribu 
daftar bandar togel tertua 
seluruh bandar togel paling baik dan terpercaya no 1 
bandar lotre togel deposit via dana 
bandar togel terpercaya via gopay 
bandar togel terbesar 2021 
bandar togel 4d hadiah terbesar 
bandar togel terbaru dan terpercaya 2020 
bandar togel terbesar di global 
bandar togel formal terpercaya 2021 
bandar togel terjitu di indonesia 
bandar togel terpercaya hadiah terbesar 
bandar togel terpercaya dan paling jitu 2021 
bandar togel terbesar di indonesia 
bandar togel paling baik di indonesia 
bandar togel terjitu dan terpercaya online 
Bandar Togel terpercaya indonesia via dana 
Bandar Togel terbesar via gopay 
Bandar Togel online 24 jam terpercaya 
Bandar Togel terpercaya 2020 
knowledge togel terlengkap 2021 
Bandar Togel paling baik 
Bandar Togel termudah via ovo 
Bandar Togel terbaru 
Bandar Togel terbaru dan termurah via linkaja 
Bandar Togel terbesar dan terpercaya 
Bandar Togel terpercaya di indonesia 
Bandar Togel terpercaya hari ini","Decomposed Bugs:

B: The Kafka website is missing a link to the privacy policy: https://privacy.apache.org/policies/privacy-policy-public.html.  
B: The Kafka website is using Google Analytics.  
B: The Kafka website is using Google Fonts.  
B: The Kafka website is using scripts hosted on Cloudflare CDN.  
B: The Kafka website has embedded videos that don't have an image placeholder.  

---

B: We omitted to pass user_ctx down in fabric_view_all_docs. Since auth has happened beforehand this hasn't been an obvious issue, but it matters for the _users db as that reacts differently based on the user. Passing the user ctx down could fix it.  
B: Couchdb intentionally hides design documents in that database from non-admins and intentionally hides the user docs of other users. Passing the user ctx down could fix it.  

---

B: When the generated stub processes a response, it currently expects nodes to come in a specific order, and throws an exception if the order differs. If ""Name"" came in before ""IsPrivacyChanged"", exception is thrown. In the parse() methods of different result processing factories. A loop should be used, and any elements can appear in any order.  

---

B: For a PDF that worked fine under 1.2.1 the height value returned is negative and the wrong value (i.e. using Math.abs() won't fix it). Other PDFs work fine. PDF Debug shows ""Creator:Crystal Reports"" and ""Producer:PDF-XChange (XCPRO30.DLL v3.30.0064) (Windows 2k)"". It's stopping the user from moving off 1.2.1.  
B: When examining the 'Stream' items, the text is not what displays.",-,FALSE,Over-decomposition
ERROR: Unable to generate spec: read file info,https://issues.apache.org/jira/browse/COUCHDB-2741,"Trying to build 51b98a4, I'm getting this failure during the make install target: 

/usr/bin/make DESTDIR=/home/micah/debian/couchdb/couchdb-2.0/debian/couchdb install 
make[2]: Entering directory '/home/micah/debian/couchdb/couchdb-2.0' 
==> b64url (compile) 
==> cassim (compile) 
==> lager (compile) 
==> couch_log (compile) 
==> config (compile) 
==> chttpd (compile) 
==> couch (compile) 
==> couch_epi (compile) 
==> couch_index (compile) 
==> couch_mrview (compile) 
==> couch_replicator (compile) 
==> couch_plugins (compile) 
==> couch_event (compile) 
==> couch_stats (compile) 
==> ddoc_cache (compile) 
==> ets_lru (compile) 
==> meck (compile) 
==> fabric (compile) 
==> bear (compile) 
==> folsom (compile) 
==> global_changes (compile) 
==> goldrush (compile) 
==> ibrowse (compile) 
==> ioq (compile) 
==> jiffy (compile) 
==> khash (compile) 
==> mango (compile) 
==> mem3 (compile) 
==> mochiweb (compile) 
==> oauth (compile) 
==> rexi (compile) 
==> snappy (compile) 
==> setup (compile) 
==> rel (compile) 
==> couchdb-2.0 (compile) 
Installing CouchDB into /home/micah/debian/couchdb/couchdb-2.0/debian/couchdb//usr/lib/couchdb... 
==> rel (generate) 
ERROR: Unable to generate spec: read file info /usr/lib/erlang/man/man8/cups-deviced.8.gz failed 
ERROR: Unexpected error: rebar_abort 
ERROR: generate failed while processing /home/micah/debian/couchdb/couchdb-2.0/rel: rebar_abort 
Makefile:84: recipe for target 'install' failed 
this is my install.mk: 

# Licensed under the Apache License, Version 2.0 (the ""License""); you may not 
# use this file except in compliance with the License. You may obtain a copy of 
# the License at 
# 
# http://www.apache.org/licenses/LICENSE-2.0 
# 
# Unless required by applicable law or agreed to in writing, software 
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the 
# License for the specific language governing permissions and limitations under 
# the License. 
# 
# The contents of this file are auto-generated by configure 
# 
package_author_name = The Apache Software Foundation 
install_dir = /usr/lib/couchdb 

bin_dir = /usr/bin 
libexec_dir = /lib/x86_64-linux-gnu/couchdb 
doc_dir = /usr/share/doc/apache-couchdb/couchdb 
sysconf_dir = /etc/couchdb 
data_dir = /usr/share/couchdb 

database_dir = /var/lib 
view_index_dir = /var/lib 
log_file = /var/log/couch.log 

html_dir = /usr/share/doc/apache-couchdb/html 
pdf_dir = /usr/share/doc/apache-couchdb/pdf 
man_dir = /usr/share/man 
info_dir = /share/info 

with_fauxton = 1 
with_docs = 1 

user = micah","Decomposed Bugs:  
B: During the `make install` target, the build fails with the error: `ERROR: Unable to generate spec: read file info /usr/lib/erlang/man/man8/cups-deviced.8.gz failed`.  
B: The build process encounters an unexpected error: `ERROR: Unexpected error: rebar_abort`.  
B: The `generate` step fails while processing `/home/micah/debian/couchdb/couchdb-2.0/rel` with the error: `ERROR: generate failed while processing /home/micah/debian/couchdb/couchdb-2.0/rel: rebar_abort`.  
B: The `Makefile` at line 84 fails to execute the `install` target.",Major,FALSE,Over-analysis
LICENSE and NOTICE files are not correct,https://issues.apache.org/jira/browse/FLINK-10987,Flink's LICENSE and NOTICE files are not correct wrt http://www.apache.org/dev/licensing-howto.html. We need to update them before we can release 1.7.0.,"B: Flink's LICENSE file is not correct with respect to http://www.apache.org/dev/licensing-howto.html. It needs to be updated before releasing 1.7.0.  
B: Flink's NOTICE file is not correct with respect to http://www.apache.org/dev/licensing-howto.html. It needs to be updated before releasing 1.7.0.",Blocker,TRUE,