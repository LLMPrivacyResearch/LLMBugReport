NAME,LINK,Resolution,Priority,DESCRIPTION,RESOLUTION,STATUS,CREATED TIME,ROSOLVED TIME,FIRST COMMENT TIME,#COMMENTS,TIME 1,TIME 2
Website updates to satisfy Apache privacy policies,https://issues.apache.org/jira/browse/KAFKA-13868,Fixed,Critical,"The ASF has updated its privacy policy and all websites must be compliant.

The full guidelines can be found in https://privacy.apache.org/faq/committers.html

The Kafka website has a few issues, including:

It's missing a link to the privacy policy: https://privacy.apache.org/policies/privacy-policy-public.html
It's using Google Analytics
It's using Google Fonts
It's using scripts hosted on Cloudflare CDN
Embedded videos don't have an image placeholder
As per the email sent to the PMC, all updates have to be done by July 22.",Fixed,Resolved,5/3/2022 19:30:00,7/28/2022 7:05:00,7/13/2022 11:22:00,49,86,15.00
ApplicationMasterProtocolPBClientImpl.allocate fails with EOFException when RPC privacy is enabled,https://issues.apache.org/jira/browse/HADOOP-14062,Fixed,Critical,"When privacy is enabled for RPC (hadoop.rpc.protection = privacy), ApplicationMasterProtocolPBClientImpl.allocate sometimes (but not always) fails with an EOFException. I've reproduced this with Spark 2.0.2 built against latest branch-2.8 and with a simple distcp job on latest branch-2.8.

Steps to reproduce using distcp:

1. Set hadoop.rpc.protection equal to privacy
2. Write data to HDFS. I did this with Spark as follows:

sc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789"")).mkString(""|"")).toDF().repartition(100).write.parquet(""hdfs:///tmp/testData"")
3. Attempt to distcp that data to another location in HDFS. For example:

hadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy
I observed this error in the ApplicationMaster's syslog:

2016-12-19 19:13:50,097 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1482189777425_0004, File: hdfs://<namenode_host>:8020/tmp/hadoop-yarn/staging/<hdfs_user>/.staging/job_1482189777425_0004/job_1482189777425_0004_1.jhist
2016-12-19 19:13:51,004 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0
2016-12-19 19:13:51,031 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1482189777425_0004: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:22528, vCores:23> knownNMs=3
2016-12-19 19:13:52,043 INFO [RMCommunicator Allocator] org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking ApplicationMasterProtocolPBClientImpl.allocate over null. Retrying after sleeping for 30000ms.
java.io.EOFException: End of File Exception between local host is: ""<application_master_host>/<ip_addr>""; destination host is: ""<rm_host>"":8030; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)
	at org.apache.hadoop.ipc.Client.call(Client.java:1428)
	at org.apache.hadoop.ipc.Client.call(Client.java:1338)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy80.allocate(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy81.allocate(Unknown Source)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
Marking as ""critical"" since this blocks YARN users from encrypting RPC in their Hadoop clusters.",Fixed,Resolved,12/20/2016 0:35:00,3/10/2017 17:26:00,12/20/2016 4:42:00,58,80,80.00
Add privacy policy in web site,https://issues.apache.org/jira/browse/AMBARI-766,Fixed,Major,As the web site use google analytics privacy policy page must be added.,Fixed,Resolved,9/24/2012 8:20:00,10/3/2012 23:33:00,9/24/2012 8:21:00,4,9,9.00
Thrift Server 1 uses different QOP settings than RPC and Thrift Server 2 and can easily be misconfigured so there is no encryption when the operator expects it.,https://issues.apache.org/jira/browse/HBASE-17513,Fixed,Critical,"As of HBASE-14400 the setting hbase.thrift.security.qop was unified to behave the same as the general HBase RPC protection. However, this only happened for the Thrift2 server. The Thrift server found in the thrift package (aka Thrift Server 1) still hard codes the old configs of 'auth', 'auth-int', and 'auth-conf'.

Additionally, these Quality of Protection (qop) settings are used only by the SASL transport. If a user configures the HBase Thrift Server to make use of the HTTP transport (to enable doAs proxying e.g. for Hue) then a QOP setting of 'privacy' or 'auth-conf' won't get them encryption as expected.

We should

1) update hbase-thrift/src/main/.../thrift/ThriftServerRunner to rely on SaslUtil to use the same 'authentication', 'integrity', 'privacy' configs in a backward compatible way
2) also have ThriftServerRunner warn when both hbase.thrift.security.qop and hbase.regionserver.thrift.http are set, since the latter will cause the former to be ignored. (users should be directed to hbase.thrift.ssl.enabled and related configs to ensure their transport is encrypted when using the HTTP transport.)",Fixed,Resolved,1/23/2017 17:14:00,1/22/2018 17:06:00,1/23/2017 17:14:00,39,364,364.00
Missing localization strings in the installer,https://issues.apache.org/jira/browse/FLEX-34392,Fixed,Minor,"the following strings aren't localized
Select AIR and Flash Player
Select Flex Version
Select AIR Version
Select Flash Player Version
anonymous usage statics will be collected in accordance with our privacy policy.",Fixed,Resolved,7/1/2014 17:00:00,7/11/2014 0:45:00,6/22/2015 6:31:00,1,10,346.00
Use SaslUtil to set Sasl.QOP in 'Thrift',https://issues.apache.org/jira/browse/HBASE-19118,Fixed,Major,"In Configure the Thrift Gateway, it says ""set the property hbase.thrift.security.qop to one of the following three values: privacy, integrity, authentication"", which would lead to failure of starting up a thrift server.
In fact, the value of hbase.thrift.security.qop should be auth, auth-int, auth-conf, according to the documentation of Sasl.QOP",Fixed,Resolved,10/30/2017 4:12:00,11/2/2017 16:26:00,10/30/2017 4:25:00,37,3,3.00
Netbeans not allowed to access personnal files and folder under macOS,https://issues.apache.org/jira/browse/NETBEANS-5004,Fixed,Major,"With the new privacy and security features enforced macOS Catalina, Netbeans and any of its subprocesses are not allowed to access any of the personnal files and folders (e.g.: Downloads, Documents...).

Normally, a system prompt should ask the user when the application attempts to access such files, but here nothing happens. For instance, when I try to access the Document directory through the ""Open file..."" menu, this directory appears empty.

The bug seems to be related to the fact that the shell script used to launch Netbeans (/bin/sh [/Applications/NetBeans/Apache NetBeans) is  sandboxed.

Giving full disk access to Netbeans in system preferences did not solve the issue.

The system console error related to this bug is attached to this ticket.",Fixed,Resolved,11/10/2020 12:29:00,4/1/2021 12:48:00,11/12/2020 12:23:00,6,142,140.00
Convert all Lucene web properties to use the ASF CMS,https://issues.apache.org/jira/browse/LUCENE-2748,Fixed,Major,"The new CMS has a lot of nice features (and some kinks to still work out) and Forrest just doesn't cut it anymore, so we should move to the ASF CMS: http://apache.org/dev/cms.html",Fixed,Resolved,11/8/2010 20:45:00,6/11/2012 14:59:00,11/9/2010 18:18:00,27,581,580.00
PutHDFS will pass files to success when they were not successfully written with hadoop client misconfiguration,https://issues.apache.org/jira/browse/NIFI-1062,Fixed,Major,"PutHDFS will create an empty file, but the data it is attempting to write will fail with this stack trace, and the flow file gets routed to success.

2015-10-24 11:16:19,278 WARN [Thread-4674] org.apache.hadoop.hdfs.DFSClient DataStreamer Exception
java.lang.IllegalArgumentException: null
        at javax.security.auth.callback.NameCallback.<init>(NameCallback.java:90) ~[na:1.8.0_45]
        at com.sun.security.sasl.digest.DigestMD5Client.processChallenge(DigestMD5Client.java:324) ~[na:1.8.0_45]
        at com.sun.security.sasl.digest.DigestMD5Client.evaluateChallenge(DigestMD5Client.java:220) ~[na:1.8.0_45]
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.evaluateChallengeOrResponse(SaslParticipant.java:113) ~[na:na]
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:451) ~[na:na]
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getSaslStreams(SaslDataTransferClient.java:390) ~[na:na]
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:262) ~[na:na]
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:210) ~[na:na]
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:182) ~[na:na]
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1413) ~[na:na]
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1361) ~[na:na]
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588) ~[na:na]
2015-10-24 11:16:19,303 INFO [Timer-Driven Process Thread-2] o.apache.nifi.processors.hadoop.PutHDFS PutHDFS[id=3c30a474-86b4-45fc-b771-95d7a1b5054d] copied StandardFlowFileRecord[uuid=5a3deada-9739-474f-a83d-0447ad5aefd9,claim=StandardContentClaim [resourceClaim=StandardResourceClaim[id=1445694733273-1, container=default, section=1], offset=87, length=29],offset=0,name=x11.txt,size=29] to HDFS at /use/hdfs/x11.txt in 56 milliseconds at a rate of 511 bytes/sec",Fixed,Resolved,10/24/2015 15:22:00,11/11/2015 16:41:00,10/24/2015 15:24:00,13,18,18.00
"When a cube built successfully with around 170 measures, some queries cannot be executed",https://issues.apache.org/jira/browse/KYLIN-1518,Fixed,Major,"A cube with the same data model, the same cube definition except the number of measures. If the number is around 150, the same query can be executed successfully. The error log is as follows:

at org.apache.calcite.avatica.Helper.createException(Helper.java:41)
at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:112)
at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:130)
at org.apache.kylin.rest.service.QueryService.execute(QueryService.java:361)
at org.apache.kylin.rest.service.QueryService.queryWithSqlMassage(QueryService.java:276)
at org.apache.kylin.rest.service.QueryService.query(QueryService.java:118)
at org.apache.kylin.rest.service.QueryService$$FastClassByCGLIB$$4957273f.invoke(<generated>)
at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618)
at org.apache.kylin.rest.service.QueryService$$EnhancerByCGLIB$$ab2dbbe7.query(<generated>)
at org.apache.kylin.rest.controller.QueryController.doQueryWithCache(QueryController.java:191)
at org.apache.kylin.rest.controller.QueryController.query(QueryController.java:95)
at sun.reflect.GeneratedMethodAccessor169.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:213)
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:126)
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:96)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:617)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:578)
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80)
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:923)
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:852)
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:882)
at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:789)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:105)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:195)
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:266)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504)
at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074)
at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Index: 187, Size: 186
at java.util.ArrayList.rangeCheck(ArrayList.java:635)
at java.util.ArrayList.get(ArrayList.java:411)
at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:914)
at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:885)
at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112)
at org.apache.calcite.rex.RexProgramBuilder.registerInput(RexProgramBuilder.java:274)
at org.apache.calcite.rex.RexProgramBuilder.addProject(RexProgramBuilder.java:185)
at org.apache.calcite.rex.RexProgram.create(RexProgram.java:215)
at org.apache.calcite.rex.RexProgram.create(RexProgram.java:184)
at org.apache.kylin.query.relnode.OLAPProjectRel.implementEnumerable(OLAPProjectRel.java:219)
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:159)
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:155)
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:155)
at org.apache.kylin.query.relnode.OLAPToEnumerableConverter.implement(OLAPToEnumerableConverter.java:97)
at org.apache.calcite.adapter.enumerable.EnumerableRelImplementor.implementRoot(EnumerableRelImplementor.java:99)
at org.apache.calcite.adapter.enumerable.EnumerableInterpretable.toBindable(EnumerableInterpretable.java:92)
at org.apache.calcite.prepare.CalcitePrepareImpl$CalcitePreparingStmt.implement(CalcitePrepareImpl.java:1050)
at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:293)
at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:188)
at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:671)
at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:572)
at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:541)
at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:173)
at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:561)
at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:477)
at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:109)
... 75 more",Fixed,Resolved,3/22/2016 7:06:00,4/18/2016 10:15:00,3/22/2016 7:09:00,19,27,27.00
Statestore should garbage collect hung connections,https://issues.apache.org/jira/browse/IMPALA-1726,Fixed,Critical,"If a node is truly hung, the statestore may apparently wait forever to receive the heartbeat response. We need to check the TCP timeouts on the connections from the statestore to the subscriber.

Since the operating system can also interfere, we should periodically visit all heartbeat threads and see how long they've been in the heartbeat RPC for. I think we can forcibly close the socket in a GC thread if it's taken too long. The next time round should hit the TCP cnxn timeout (or be refused), and the subscriber should be marked as dead.",Fixed,Resolved,2/2/2015 18:41:00,3/17/2015 16:26:00,3/17/2015 0:05:00,4,43,0.00
Yarn NodeManager OOM Listener Fails Compilation on Ubuntu 18.04,https://issues.apache.org/jira/browse/YARN-8498,Fixed,Blocker,"While building this project, I ran into a few compilation errors here. The first one was in this file:

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/impl/oom_listener_main.c

At the very end, during the compilation of the OOM test, it fails again:
hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:256:7: error: ‘__WAIT_STATUS’ was not declared in this scope
__WAIT_STATUS mem_hog_status = {};

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:257:30: error: ‘mem_hog_status’ was not declared in this scope
__pid_t exited0 = wait(mem_hog_status);

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:275:21: error: expected ‘;’ before ‘oom_listener_status’
__WAIT_STATUS oom_listener_status = {};

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:276:30: error: ‘oom_listener_status’ was not declared in this scope
__pid_t exited1 = wait(oom_listener_status);",Fixed,Resolved,2/7/2019 7:49:00,2/7/2019 7:37:00,7/15/2018 6:03:00,28,207,207.00
Performance of DirectColorModel RGB bitmap images,https://issues.apache.org/jira/browse/XGC-71,Fixed,-,When RGB bitmaps are used there can be performance improvements made to mitigate some of the impact.,Fixed,Resolved,7/25/2012 9:14:00,7/25/2012 9:52:00,7/25/2012 9:52:00,1,0,0.00
activemq-client tests are failing with JDK17+,https://issues.apache.org/jira/browse/AMQ-9341,Fixed,Major,"activemq-client module doesn't build due to test failure:

Caused by: java.net.BindException: Can't assign requested address
        at java.base/sun.nio.ch.Net.connect0(Native Method)
        at java.base/sun.nio.ch.Net.connect(Net.java:579)
        at java.base/sun.nio.ch.Net.connect(Net.java:568)
        at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        at java.base/java.net.Socket.connect(Socket.java:633)
        at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:304)
        at org.apache.activemq.transport.tcp.TcpTransport.connect(TcpTransport.java:525) ",Fixed,Resolved,10/16/2023 15:57:00,11/1/2023 5:40:00,10/21/2023 13:32:00,6,16,11.00
Python DataFrame CSV load on large file is writing to console in Ipython,https://issues.apache.org/jira/browse/SPARK-14103,Fixed,Major,"I am using the spark from the master branch and when I run the following command on a large tab separated file then I get the contents of the file being written to the stderr

df = sqlContext.read.load(""temp.txt"", format=""csv"", header=""false"", inferSchema=""true"", delimiter=""\t"")
Here is a sample of output:

^M[Stage 1:>                                                          (0 + 2) / 2]16/03/23 14:01:02 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 2)
com.univocity.parsers.common.TextParsingException: Error processing input: Length of parsed input (1000001) exceeds the maximum number of characters defined in your parser settings (1000000). Identified line separator characters in the parsed content. This may be the cause of the error. The line separator in your parser settings is set to '\n'. Parsed content:
        Privacy-shake"",: a haptic interface for managing privacy settings in mobile location sharing applications       privacy shake a haptic interface for managing privacy settings in mobile location sharing applications  2010    2010/09/07              international conference on human computer interaction  interact                43331058        19371[\n]        3D4F6CA1        Between the Profiles: Another such Bias. Technology Acceptance Studies on Social Network Services       between the profiles another such bias technology acceptance studies on social network services 2015    2015/08/02      10.1007/978-3-319-21383-5_12    international conference on human-computer interaction  interact                43331058        19502[\n]

.......

.........

web snippets    2008    2008/05/04      10.1007/978-3-642-01344-7_13    international conference on web information systems and technologies    webist          44F29802        19489
06FA3FFA        Interactive 3D User Interfaces for Neuroanatomy Exploration     interactive 3d user interfaces for neuroanatomy exploration     2009                    internationa]
        at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:241)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:356)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:137)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:120)
        at scala.collection.Iterator$class.foreach(Iterator.scala:742)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foreach(CSVParser.scala:120)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foldLeft(CSVParser.scala:120)
        at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:212)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.aggregate(CSVParser.scala:120)
        at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
        at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
        at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
        at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:69)
        at org.apache.spark.scheduler.Task.run(Task.scala:82)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException
16/03/23 14:01:03 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
^M[Stage 1:>                                                          (0 + 1) / 2]


For a small sample (<10,000 lines) of the data, I am not getting any error. But as soon as I go above more than 100,000 samples, I start getting the error.

I don't think the spark platform should output the actual data to stderr ever as it decreases the readability.",Fixed,Resolved,3/23/2016 20:22:00,4/8/2016 7:28:00,3/23/2016 20:25:00,35,16,16.00
Missing menu items,https://issues.apache.org/jira/browse/ASFSITE-27,Fixed,Major,"The redesign has lost several links from the navigation:

Privacy Page
Incubator
Memorials
Conferences",Fixed,Resolved,7/16/2023 23:33:00,7/19/2023 22:42:00,7/19/2023 22:42:00,1,3,0.00
Site-to-Site Transit URI is inconsistent,https://issues.apache.org/jira/browse/NIFI-2028,Fixed,Major,"Site-to-Site client and server create provenance event at both end, and those have Transit URI in it to record how flow files are transferred. However, the URI formats are inconsistent among RAW vs HTTP.

Test result as follows:

These port numbers are configurable in nifi.properties
3080: Web API port (nifi.web.http.port)
3081: Site-to-Site RAW Socket port (nifi.remote.input.socket.port)
Before Fix

PUSH - RAW

Client - SEND:	nifi://localhost:3081/flow-file-uuid
Server - RECEIVE:	nifi://localhost:3081/flow-file-uuid
PULL - RAW

Client - RECEIVE:	nifi://localhost:3081flow-file-uuid
Server - SEND:	nifi://localhost:3081/flow-file-uuid
PUSH - HTTP

Client - SEND:	http://127.0.0.1:3080/nifi-api/flow-file-uuid
Server - RECEIVE:	nifi://127.0.0.1:57390
PULL - HTTP

Client - RECEIVE:	http://127.0.0.1:3080/flow-file-uuid
Server - SEND:	nifi://127.0.0.1:57673
Issues

PULL - RAW, Client - RECEIVE: lacks '/' in between port and flow-file uuid
RAW uses server's host and port on both end (by transit url prefix), HTTP should follow this rule
HTTP transit uri looks like REST endpoint but it is not a real endpoint. It should be an actual endpoint URI
RAW uses hostname, while HTTP uses IP address
After Fix

PUSH - RAW

Client - SEND:	nifi://localhost:3081/flow-file-uuid
Server - RECEIVE:	nifi://localhost:3081/flow-file-uuid
PULL - RAW

Client - RECEIVE:	nifi://localhost:3081/flow-file-uuid
Server - SEND:	nifi://localhost:3081/flow-file-uuid
PUSH - HTTP

Client - SEND:	http://localhost:3080/nifi-api/data-transfer/input-ports/$port-id/transactions/$tx-id/flow-files
Server - RECEIVE:	http://localhost:3080/nifi-api/data-transfer/input-ports/$port-id/transactions/$tx-id/flow-files
PULL - HTTP

Client - RECEIVE:	http://localhost:3080/nifi-api/data-transfer/output-ports/$port-id/transactions/$tx-id/flow-files
Server - SEND:	http://localhost:3080/nifi-api/data-transfer/output-ports/$port-id/transactions/$tx-id/flow-files",Fixed,Resolved,6/15/2016 6:50:00,8/2/2016 13:15:00,6/15/2016 7:21:00,27,48,48.00
compat with pandas 0.20.0,https://issues.apache.org/jira/browse/ARROW-879,Fixed,Blocker,pandas changes the location of an import for ``DatetimeTZDtype``: http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#reorganization-of-the-library-privacy-changes,Fixed,Resolved,4/23/2017 12:56:00,4/23/2017 22:37:00,4/23/2017 15:58:00,3,0,0.00
Unexpected subelement Name,https://issues.apache.org/jira/browse/AXIS2-1747,Fixed,Major,"When the generated stub processes response, it expects the nodes come in a specific order. If the order is different, exception is thrown.

The code is in parse() methods of different result processing factory.

For example, the code:
/// process the ""first"" element ""IsPrivacyChanged""
if (reader.isStartElement()
...
""IsPrivacyChanged"").equals(reader.getName())) {
read the value
}else
{
throw exception
}

///now it processes 2nd element.
if (reader.isStartElement()
...
""Name"").equals(reader.getName())) {
read the value
}else
{
throw exception
}

If ""Name"" came in before ""IsPrivacyChanged"", exception is thrown.

A loop should be used, and any elements can appear in any order.",Fixed,Resolved,11/20/2006 2:48:00,2/5/2007 7:32:00,11/20/2006 3:05:00,3,77,77.00
query fails when calcite parse sql,https://issues.apache.org/jira/browse/KYLIN-1539,Fixed,Major,"There are two cases of query failure when calcite parses sql. However, both are solved by the same solution, just restarting kylin server.",Fixed,Resolved,3/28/2016 9:52:00,4/18/2016 10:14:00,3/28/2016 9:52:00,18,21,21.00
System views design leads to bad user expirience.,https://issues.apache.org/jira/browse/IGNITE-12921,Fixed,Critical,"Current implementation of system views has broken system behavior which is related with querying system views.

Before 2.8 system views were available via SQL queries (if indexing is enabled). It did not depend on any configuration.

After implementation of IGNITE-12145 system views available only if SqlViewExporterSpi is passed to IgniteConfiguration.setSystemViewExporterSpi(). Now, if an user configures some SystemViewExporterSpi then provided user configuration will rewrite default configuration and SqlViewExporterSpi won't be initialized. As result it is impossible to query system views and any query to the views fails with exception. This behavior is not obvious for the user. See tests below.

The second problem is kind of design problem. System view is internal part of the system and should be available regardless of any exporter configuration (at least via SQL) such as it was implemented before 2.8 release.

My suggestion is the following: we should remove SqlViewExporterSpi and configure all views on indexing module initialization. SqlViewExporterSPI also doesn't make sense because:

it operates by some internal API (SchemaManager, GridKernalContext, IgniteH2Indexing).
it doesn't allow to end user to add any new system view.
Only thing that could be useful is a filtering. But it could be done with SQL.

Reproducer of broken behavior:

package org.apache.ignite.internal.processors.cache.metric;

import org.apache.ignite.cache.query.SqlFieldsQuery;
import org.apache.ignite.cluster.ClusterState;
import org.apache.ignite.configuration.DataRegionConfiguration;
import org.apache.ignite.configuration.DataStorageConfiguration;
import org.apache.ignite.configuration.IgniteConfiguration;
import org.apache.ignite.internal.IgniteEx;
import org.apache.ignite.spi.systemview.jmx.JmxSystemViewExporterSpi;
import org.apache.ignite.testframework.junits.common.GridCommonAbstractTest;
import org.junit.Test;

import java.util.HashSet;
import java.util.List;
import java.util.Set;

import static java.util.Arrays.asList;
import static org.apache.ignite.internal.processors.cache.index.AbstractSchemaSelfTest.queryProcessor;

public class SystemViewTest extends GridCommonAbstractTest {

    private static boolean useDefaultSpi;

    /** {@inheritDoc} */
    @Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception {
        IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName);

        cfg.setConsistentId(igniteInstanceName);

        cfg.setDataStorageConfiguration(new DataStorageConfiguration()
                .setDataRegionConfigurations(
                        new DataRegionConfiguration().setName(""in-memory"").setMaxSize(100L * 1024 * 1024))
                .setDefaultDataRegionConfiguration(
                        new DataRegionConfiguration()
                                .setPersistenceEnabled(true)));

        if (!useDefaultSpi) {
            // Configure user provided system view exporter SPI.
            cfg.setSystemViewExporterSpi(new JmxSystemViewExporterSpi());
        }

        return cfg;
    }

    /**
     * Will executed succefully.
     */
    @Test
    public void testSystemViewWithDefaultSpi() throws Exception {
        useDefaultSpi = true;

        doTestSystemView();
    }

    /**
     * Will fail with <code>Table ""VIEWS"" not found</code>.
     */
    @Test
    public void testSystemViewWithCustomSpi() throws Exception {
        useDefaultSpi = false;

        doTestSystemView();
    }

    private void doTestSystemView() throws Exception {
        try (IgniteEx ignite = startGrid()) {
            ignite.cluster().state(ClusterState.ACTIVE);

            Set<String> cacheNames = new HashSet<>(asList(""cache-1"", ""cache-2""));

            for (String name : cacheNames)
                ignite.getOrCreateCache(name);

            SqlFieldsQuery qry = new SqlFieldsQuery(""SELECT * FROM SYS.VIEWS"");

            List<List<?>> res = queryProcessor(ignite).querySqlFields(qry, true).getAll();

            res.forEach(item -> log.info(""VIEW FOUND: "" + item));
        }
    }

}",Fixed,Resolved,4/20/2020 16:09:00,9/18/2020 12:07:00,4/22/2020 13:04:00,10,151,149.00
TestFailpoints::test_failpoints crash in ARM build,https://issues.apache.org/jira/browse/IMPALA-11542,Fixed,Critical,"Saw the crash in https://jenkins.impala.io/job/ubuntu-16.04-from-scratch-ARM/13

In the ERROR log:

Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30000,server=y,suspend=n  
impalad: /home/ubuntu/native-toolchain/source/llvm/llvm-5.0.1-asserts.src-p3/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldELF.cpp:400: void llvm::RuntimeDyldELF::resolveAArch64Relocation(const llvm::SectionEntry&, uint64_t, uint64_t, uint32_t, int64_t): Assertion `isInt<33>(Result) && ""overflow check failed for relocation""' failed.
Minidump in thread [20013]exec-finstance (finst:1e4a0f56622f2a15:51c2970900000005) running query 1e4a0f56622f2a15:51c2970900000000, fragment instance 1e4a0f56622f2a15:51c2970900000005
Wrote minidump to /home/ubuntu/Impala/logs/ee_tests/minidumps/impalad/de4830d8-009d-47f4-f14bb68a-f0d8cd4c.dmp 
In the INFO log:

I0830 06:54:49.173234 11329 impala-beeswax-server.cc:516] query: Query {
  01: query (string) = ""SELECT STRAIGHT_JOIN *\n           FROM alltypes t1\n                  JOIN /*+broadcast*/ alltypesagg t2 ON t1.id = t2.id\n           WHERE t2.int_col < 1000"",
  03: configuration (list) = list<string>[10] {
    [0] = ""CLIENT_IDENTIFIE[...](273)"",
    [1] = ""TEST_REPLAN=1"",
    [2] = ""DISABLE_CODEGEN=False"",
    [3] = ""BATCH_SIZE=0"",
    [4] = ""NUM_NODES=0"",
    [5] = ""DISABLE_CODEGEN_ROWS_THRESHOLD=0"",
    [6] = ""MT_DOP=4"",
    [7] = ""ABORT_ON_ERROR=1"",
    [8] = ""DEBUG_ACTION=4:GETNEXT:MEM_LIMIT_EXCEEDED|COORD_BEFORE_EXEC_RPC:JITTER@100@0.3"",
    [9] = ""EXEC_SINGLE_NODE_ROWS_THRESHOLD=0"",
  },
  04: hadoop_user (string) = ""ubuntu"",
}
...
  74: client_identifier (string) = ""failure/test_failpoints.py::TestFailpoints::()::test_failpoints[protocol:beeswax|table_format:seq/snap/block|exec_option:{'test_replan':1;'batch_size':0;'num_nodes':0;'disable_codegen_rows_threshold':0;'disable_codegen':False;'abort_on_error':1;'exec_sing"",
...
I0830 06:54:49.173739 11329 Frontend.java:1877] 1e4a0f56622f2a15:51c2970900000000] Analyzing query: SELECT STRAIGHT_JOIN *
           FROM alltypes t1
                  JOIN /*+broadcast*/ alltypesagg t2 ON t1.id = t2.id
           WHERE t2.int_col < 1000 db: functional_seq_snap 
The client_identifier shows it's TestFailpoints::test_failpoints.",Fixed,Resolved,8/30/2022 12:34:00,12/1/2023 18:29:00,9/6/2022 1:47:00,7,458,451.00
Value::Record containing enums fail to validate when using namespaces in Schema,https://issues.apache.org/jira/browse/AVRO-3674,Fixed,Major,"Consider the following schema:

{
    ""type"": ""record"",
    ""name"": ""NamespacedMessage"",
    ""namespace"": ""com.domain"",
    ""fields"": [
        {
            ""type"": ""record"",
            ""name"": ""field_a"",
            ""fields"": [
                {
                    ""name"": ""enum_a"",
                    ""type"": {
                        ""type"": ""enum"",
                        ""name"": ""EnumType"",
                        ""symbols"": [
                            ""SYMBOL_1"",
                            ""SYMBOL_2""
                        ],
                        ""default"": ""SYMBOL_1""
                    }
                },
                {
                    ""name"": ""enum_b"",
                    ""type"": ""EnumType""
                }
            ]
        }
    ]
}
I might represent this in Rust using the following structs:

#[derive(Serialize)]
enum EnumType {
    #[serde(rename = ""SYMBOL_1"")]
    Symbol1,
    #[serde(rename = ""SYMBOL_2"")]
    Symbol2,
}

#[derive(Serialize)]
struct FieldA {
    enum_a: EnumType,
    enum_b: EnumType,
}

#[derive(Serialize)]
struct NamespacedMessage {
    field_a: FieldA,
}

let msg = NamespacedMessage {
    field_a: FieldA {
        enum_a: EnumType::Symbol2,
        enum_b: EnumType::Symbol1,
    },
};
and then serialize this into a `Value` using the following logic:

let mut ser = Serializer::default();
let test_value: Value = msg.serialize(&mut ser).unwrap();
After serializing into `test_value` I would expect that `test_value.validate(&schema)` yields True. However this is not the case.

I can work around it by removing the `namespace` definition from my schema which allows the validation to proceed.

I believe the cause of schema validation failure is [this lookup failing](https://github.com/apache/avro/blob/release-1.11.1-rc1/lang/rust/avro/src/types.rs#L370) when schemas are utilized as the `Value` will not have a namespace associated with it.

I believe this could be fixed by altering `validate_internal` to accept an optional namespace that is derived from the provided schema to `validate`. However, I'm not sure if this is an appropriate fix.",Fixed,Resolved,11/15/2022 14:33:00,11/18/2022 13:04:00,11/15/2022 14:40:00,23,3,3.00
user context not passed down in fabric_view_all_docs,https://issues.apache.org/jira/browse/COUCHDB-3232,Fixed,Major,"We omitted to pass user_ctx down in fabric_view_all_docs. Since auth has happened beforehand this hasn't been an obvious issue, but it matters for the _users db as that reacts differently based on the user. couchdb intentionally hides design documents in that database from non-admins and intentionally hides the user docs of other users.

passing the user ctx down fixes both issues.",Fixed,Resolved,11/11/2016 4:43:00,11/11/2016 23:41:00,11/11/2016 4:48:00,4,0,0.00
HBase server doesn't preserve SASL sequence number on the network,https://issues.apache.org/jira/browse/HBASE-22492,Fixed,Major,"When auth-conf is enabled on RPC, the server encrypt response in setReponse() using saslServer. The generated cryptogram included a sequence number manage by saslServer. But then, when the response is sent over the network, the sequence number order is not preserved.

The client receives reply in the wrong order, leading to a log message from DigestMD5Base:

sasl:1481  - DIGEST41:Unmatched MACs

Then the message is discarded, leading the client to a timeout.

I propose a fix here: https://github.com/sbarnoud/hbase-release/commit/ce9894ffe0e4039deecd1ed51fa135f64b311d41

It seems that any HBase 1.x is affected.

This part of code has been fully rewritten in HBase 2.x, and i haven't do the analysis on HBase 2.x which may be affected.

 

Here, an extract of client log that i added to help me to understand:

…

2019-05-28 12:53:48,644 DEBUG [Default-IPC-NioEventLoopGroup-1-32] NettyRpcDuplexHandler:80  - callId: 5846 /192.163.201.65:58870 -> dtltstap004.fr.world.socgen/192.163.201.72:16020

2019-05-28 12:53:48,651 INFO  [Default-IPC-NioEventLoopGroup-1-18] NioEventLoop:101  - SG: Channel ready to read 1315913615 unsafe 1493023957 /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020

2019-05-28 12:53:48,651 INFO  [Default-IPC-NioEventLoopGroup-1-18] SaslUnwrapHandler:78  - SG: after unwrap:46 -> 29 for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 seqNum 150

2019-05-28 12:53:48,652 DEBUG [Default-IPC-NioEventLoopGroup-1-18] NettyRpcDuplexHandler:192  - callId: 5801 received totalSize:25 Message:20 scannerSize:(null)/192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020

2019-05-28 12:53:48,652 INFO  [Default-IPC-NioEventLoopGroup-1-18] sasl:1481  - DIGEST41:Unmatched MACs

2019-05-28 12:53:48,652 WARN  [Default-IPC-NioEventLoopGroup-1-18] SaslUnwrapHandler:70  - Sasl error (probably invalid MAC) detected for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 saslClient @4ac31121 ctx @14fb001d msg @140313192718406 len 118 data:1c^G?^P?3??h?k??????""??x?$^_??^D;^]7^Es??Em?c?w^R^BL?????????x??omG?z?I???45}???dE?^\^S>D?^????/4f?^^?? ?^E????d?????????D?kM^@^A^@^@^@? readerIndex 118 writerIndex 118 seqNum 152
 We can see that the client unwraps the Sasl message with sequence number 152 before sequence number 151 and fails with the unmatched MAC.

 

I opened a case to Oracle because we should had an error (and not the message ignored). That's because the JDK doesn't controls integrity in the right way.

https://github.com/openjdk/jdk/blob/master/src/java.security.sasl/share/classes/com/sun/security/sasl/digest/DigestMD5Base.java

The actual JDK controls the HMac before the sequence number and hides the real error (bad sequence number) because SASL is stateful. The JDK should check FIRST the sequence number and THEN the HMac.

When (and if) the JDK will be patched, and accordingly to https://www.ietf.org/rfc/rfc2831.txt , we will get an exception in that case instead of having the message ignored.",Fixed,Resolved,5/29/2019 9:23:00,6/22/2019 1:10:00,5/29/2019 11:18:00,39,24,24.00
"Website ""site"" section links all inaccessible",https://issues.apache.org/jira/browse/KARAF-743,Fixed,Minor,"The links pointed to on the ""site"" page: http://karaf.apache.org/site.html are all broken; I'm not sure why as the site/trunk repository does indeed have pages for all the links on the site page. This also affects the Apache ""Privacy Policy"" link at the very bottom of all the Karaf website pages.",Fixed,Resolved,7/17/2011 14:23:00,7/18/2011 8:22:00,7/18/2011 8:19:00,2,1,0.00
Datanode#checkSecureConfig should allow SASL and privileged HTTP,https://issues.apache.org/jira/browse/HDFS-13081,Fixed,Major,"Datanode#checkSecureConfig currently check the following to determine if secure datanode is enabled. 

The server has bound to privileged ports for RPC and HTTP via SecureDataNodeStarter.
The configuration enables SASL on DataTransferProtocol and HTTPS (no plain HTTP) for the HTTP server.
Authentication of Datanode RPC server can be done either via SASL handshake or JSVC/privilege RPC port.
This guarantees authentication of the datanode RPC server before a client transmits a secret, such as a block access token.

Authentication of the HTTP server can also be done either via HTTPS/SSL or JSVC/privilege HTTP port. This guarantees authentication of datandoe HTTP server before a client transmits a secret, such as a delegation token.

This ticket is open to allow privileged HTTP as an alternative to HTTPS to work with SASL based RPC protection.
 
cc: cnauroth , daryn, jnpandey for additional feedback.",Fixed,Resolved,1/29/2018 20:15:00,2/28/2018 18:28:00,2/1/2018 19:24:00,27,30,27.00
RTF parser misses text content,https://issues.apache.org/jira/browse/TIKA-1713,Fixed,Major,"We have a lot of Outlook msg files that have RTF body content. Tika is not finding any text within these messages. It appears to be a mixture of RTF and HTML.

I've extracted an example RTF body (see attachment) for use with the following test case:

ByteArrayOutputStream bytes = new ByteArrayOutputStream()
rtfParser.parse(
        this.class.getResourceAsStream(""/problems/no-text.rtf""),
        new EmbeddedContentHandler(new BodyContentHandler(bytes)),
        new Metadata(), new ParseContext()
);
assertTrue(""Document is missing required text"", bytes.toByteArray().length > 0)",Fixed,Resolved,8/17/2015 13:38:00,5/29/2019 15:25:00,8/24/2015 14:16:00,4,1381,"1,374.00"
NiFi Registry fails to push flow to GIT using SSH,https://issues.apache.org/jira/browse/NIFI-11927,Fixed,Major,"I have been successfully running NiFi Registry version 1.20.0 (and others before it) for about a year now. Now I needed to switch server that Registry runs on, since the old one was outdated, and decided to set up NiFi Registry version 1.22.0.

The setup worked great and NiFi seems to have been running properly. However I recently noticed that it wasn't pushing anything to GIT. The changes are made, new versions are committed, they show up both in Registry GUI as well as the NiFi instance, but when I try to fetch some version it will not allow me giving me the following error:



I started investigating the issue and noticed that the GIT repository that NiFi Registry was using was several versions ahead of origin, no version got pushed to the GIT repo.

I investigated the logs and noticed following messages show up when I make new version in NiFi and commit it to registry:

2023-08-10 07:52:03,317 WARN [NiFi Registry Web Server-18] javax.persistence.spi javax.persistence.spi::No valid providers found.
2023-08-10 07:52:05,120 INFO [GitFlowMetaData Push thread] o.a.s.c.u.s.e.EdDSASecurityProviderRegistrar getOrCreateProvider(EdDSA) created instance of net.i2p.crypto.eddsa.EdDSASecurityProvider
2023-08-10 07:52:05,326 INFO [GitFlowMetaData Push thread] o.a.s.c.i.DefaultIoServiceFactoryFactory No detected/configured IoServiceFactoryFactory; using Nio2ServiceFactoryFactory
2023-08-10 07:52:05,351 ERROR [GitFlowMetaData Push thread] o.a.n.r.p.flow.git.GitFlowMetaData Failed to push commits to origin due to org.eclipse.jgit.api.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly
org.eclipse.jgit.api.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly
        at org.eclipse.jgit.api.PushCommand.call(PushCommand.java:147)
        at org.apache.nifi.registry.provider.flow.git.GitFlowMetaData.lambda$startPushThread$1(GitFlowMetaData.java:299)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
        at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.eclipse.jgit.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly
        at org.eclipse.jgit.transport.TransportGitSsh$SshPushConnection.<init>(TransportGitSsh.java:382)
        at org.eclipse.jgit.transport.TransportGitSsh.openPush(TransportGitSsh.java:159)
        at org.eclipse.jgit.transport.PushProcess.execute(PushProcess.java:127)
        at org.eclipse.jgit.transport.Transport.push(Transport.java:1384)
        at org.eclipse.jgit.api.PushCommand.call(PushCommand.java:137)
        ... 7 common frames omitted
Caused by: java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()'
        at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:189)
        at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:142)
        at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:99)
        at org.eclipse.jgit.transport.sshd.SshdSessionFactory.getSession(SshdSessionFactory.java:235)
        at org.eclipse.jgit.transport.sshd.SshdSessionFactory.getSession(SshdSessionFactory.java:1)
        at org.eclipse.jgit.transport.SshTransport.getSession(SshTransport.java:107)
        at org.eclipse.jgit.transport.TransportGitSsh$SshPushConnection.<init>(TransportGitSsh.java:358)
        ... 11 common frames omitted 
I think the line Caused by: java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()' is what's causing the issue with pushing to GIT. This does not show up on the old Registry instance when I push to that.

 

I checked the setup and compared it to the previous version running on old server. They have exactly the same configuration, save for new one running on Java 17 not 11 (although I tried running it on 11 too).

 

The SSH keys are valid and recognized by our Gitlab instance, I can push manually as the nifi user, there are literally no differences between the configuration on one server compared to another. I even checked the configuration inside .git folder in the flow_storage directory, it was the same as on the other server. The ~/.ssh folder for nifi user has similar data between two servers, both have keys and known hosts set up, with right permissions, etc.

I decided to try running version 1.20.0 of registry on new server, and it seems to have worked immediately, without any issue, it manages to push to GIT on its own, with no changes to config. I tested version 1.23 as well and it had the same issue, I haven't tested 1.21.0 though.

 

For more information, here is the structure I have:

├── nifi-registry-1.20.0
│   ├── bin
│   ├── conf
│   ├── docs
│   ├── ext
│   ├── lib
│   ├── logs
│   ├── run
│   └── work
├── nifi-registry-1.23.0
│   ├── bin
│   ├── conf
│   ├── docs
│   ├── ext
│   ├── lib
│   ├── logs
│   ├── run
│   └── work
└── nifi-registry-files
    ├── authorization-files
    │   ├── authorizations.xml
    │   ├── authorizers.xml
    │   ├── login-identity-providers.xml
    │   └── users.xml
    ├── certificate-files
    │   ├── keystore.jks
    │   └── truststore.jks
    ├── configuration-files
    │   ├── providers.xml
    │   └── registry-aliases.xml
    ├── database-drivers
    │   └── mariadb-java-client-2.7.4.jar
    ├── extension-bundles
    └── flow-storage
        └── Buckets... 
 

Contents of the providers.xml:

<providers>    
  <flowPersistenceProvider>     
    <class&amp;amp;gt;org.apache.nifi.registry.provider.flow.git.GitFlowPersistenceProvider</class&amp;amp;gt;
    <property name=""Flow Storage Directory"">/disk1/nifi-registry/nifi-registry-files/flow-storage</property>
    <property name=""Remote To Push"">origin</property>
    <property name=""Remote Access User""></property>
    <property name=""Remote Access Password""></property>
    <property name=""Remote Clone Repository""></property>
  </flowPersistenceProvider>    
  <extensionBundlePersistenceProvider>
    <class&amp;amp;gt;org.apache.nifi.registry.provider.extension.FileSystemBundlePersistenceProvider</class&amp;amp;gt;
    <property name=""Extension Bundle Storage Directory"">/disk1/nifi-registry/nifi-registry-files/extension-bundles</property>
  </extensionBundlePersistenceProvider>
</providers>
 

Contents of the .git/config file in the flow_storage directory:

[core]
  repositoryformatversion = 0
  filemode = true
  bare = false
  logallrefupdates = true
[http]
  sslVerify = false
  proxy = http://<PROXY_URL>:3128
[https]
  sslVerify = false
  proxy = http://<PROXY_URL>:3128
[user]
  name = nifi_user
  email = OMITTED_FOR_PRIVACY
[remote ""origin""]
  url = git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git
  fetch = +refs/heads/*:refs/remotes/origin/*
[branch ""master""]
  remote = origin
  merge = refs/heads/master ",Fixed,Resolved,8/10/2023 7:17:00,8/14/2023 19:21:00,8/14/2023 14:33:00,5,4,0.00
BasicHttpCacheStorage leaking variant keys in root response's variantMap,https://issues.apache.org/jira/browse/HTTPCLIENT-2284,Fixed,Minor,"BasicHttpCacheStorage has a memory leak in the root response's variantMap. When a variant cached entry is evicted due to CacheMap being too large, the root cache entry does not remove the variant key in its variantMap. This is a memory leak that can grow the variantMap indefinitely, or until the root entry get's evicted itself.

Simplified testcase:

A request is being sent from a client that contains a header ""x-my-variant"" with a hash of the current timestamp.
The server responds 200, with a cacheable response. The response Vary's on ""x-my-variant""
These requests repeat, causing:
The ""root"" CacheEntry to be kept in CacheMap
The oldest variant CacheEntry to be evicted due to the CacheMap size limit
However the ""root"" CacheEntry never removes the evicted variant keys from the variantMap ",Fixed,Resolved,7/20/2023 18:33:00,8/28/2023 7:51:00,7/20/2023 18:42:00,19,39,39.00
Update the ubuntu version in the build instruction,https://issues.apache.org/jira/browse/HADOOP-17635,Fixed,Major,"In BUILDING.txt

Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:
Ubuntu 14 is already EoL, should be updated to 18 or 20.",Fixed,Resolved,4/15/2021 10:55:00,4/19/2021 7:45:00,4/19/2021 1:26:00,1,4,0.00
[Threat Modeling] Drillbit may be spoofed by an attacker and this may lead to data being written to the attacker's target instead of Drillbit,https://issues.apache.org/jira/browse/DRILL-5582,Fixed,Minor,"Consider the scenario:
Alice has a drillbit (my.drillbit.co) with plain and kerberos authentication enabled containing important data. Bob, the attacker, attempts to spoof the connection and redirect it to his own drillbit (fake.drillbit.co) with no authentication setup.

When Alice is under attack and attempts to connect to her secure drillbit, she is actually authenticating against Bob's drillbit. At this point, the connection should have failed due to unmatched configuration. However, the current implementation will return SUCCESS as long as the (spoofing) drillbit has no authentication requirement set.

Currently, the drillbit <- to -> drill client connection accepts the lowest authentication configuration set on the server. This leaves unsuspecting user vulnerable to spoofing.",Fixed,Resolved,6/13/2017 6:31:00,10/20/2017 18:12:00,10/17/2017 23:21:00,7,129,3.00
org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl uses internal class com.sun.org.apache.xerces.internal.impl.dv.util.Base64,https://issues.apache.org/jira/browse/NETBEANS-5427,Fixed,Major,"Netbeans 12.3 fails to deploy on Tomcat when running under JDK 16

SEVERE [org.openide.util.RequestProcessor]: Error in RequestProcessor org.openide.nodes.AsynchChildrenSEVERE [org.openide.util.RequestProcessor]: Error in RequestProcessor org.openide.nodes.AsynchChildrenjava.lang.IllegalAccessError: class org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl (in unnamed module @0x145afd71) cannot access class com.sun.org.apache.xerces.internal.impl.dv.util.Base64 (in module java.xml) because module java.xml does not export com.sun.org.apache.xerces.internal.impl.dv.util to unnamed module @0x145afd71 at org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl.run(TomcatManagerImpl.java:533) at org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl.list(TomcatManagerImpl.java:372) at org.netbeans.modules.tomcat5.deploy.TomcatManager.modules(TomcatManager.java:718) at org.netbeans.modules.tomcat5.deploy.TomcatManager.getRunningModules(TomcatManager.java:539) at org.netbeans.modules.tomcat5.ui.nodes.TomcatWebModuleChildrenFactory.createKeys(TomcatWebModuleChildrenFactory.java:107) at org.openide.nodes.AsynchChildren.run(AsynchChildren.java:190) at org.openide.util.RequestProcessor$Task.run(RequestProcessor.java:1418) at org.netbeans.modules.openide.util.GlobalLookup.execute(GlobalLookup.java:45) at org.openide.util.lookup.Lookups.executeWith(Lookups.java:278)[catch] at org.openide.util.RequestProcessor$Processor.run(RequestProcessor.java:2033)",Fixed,Resolved,3/8/2021 14:46:00,5/5/2021 10:29:00,3/17/2021 10:55:00,8,58,49.00
FileExistsException: Destination .. already exists when DiskFileItem.write was given an existing file,https://issues.apache.org/jira/browse/FILEUPLOAD-293,Fixed,Major,"Since 1.4, where FILEUPLOAD-248 was shipped, passing an existing file to DiskFileItem.write will cause an FileExistsException with the message ""Destination FILE already exist"", this prevents us from upgrading to 1.4 from 1.3.3.

 

2019-02-20 01:12:56,504 http-nio-2990-exec-3 ERROR [|5ccb9b99-a96f-42ba-ad01-ac278516e1a4|] [IssueAttachmentsResource.privacy-safe] Error saving attachment
org.apache.commons.io.FileExistsException: Destination '/buildeng/bamboo-agent-home/xml-data/build-dir/CLOUDRELEASE-AGILEWD15421-FT18/jira-test-runner-jira/target/cargo/configurations/tomcat9x/temp/attachment-3404789743778163937.tmp' already exists
{{ at org.apache.commons.io.FileUtils.moveFile(FileUtils.java:3001)}}
{{ at org.apache.commons.fileupload.disk.DiskFileItem.write(DiskFileItem.java:405)}}
{{ at com.atlassian.plugins.rest.common.multipart.fileupload.CommonsFileUploadFilePart.write(CommonsFileUploadFilePart.java:49)}}
{{ at com.atlassian.jira.rest.v2.issue.IssueAttachmentsResource.getFileFromFilePart(IssueAttachmentsResource.java:175)}}
...

 

Apache Felix also ran into the same bug:
https://issues.apache.org/jira/browse/FELIX-6037 ",Fixed,Resolved,2/21/2019 6:42:00,2/24/2019 14:12:00,2/23/2019 15:55:00,23,3,1.00
Fix serialization of structs containing Fixed fields,https://issues.apache.org/jira/browse/AVRO-3631,Fixed,Major,"Consider the following minimal Avro Schema:

{
    ""type"": ""record"",
    ""name"": ""TestStructFixedField"",
    ""fields"": [
        {
            ""name"": ""field"",
            ""type"": {
                ""name"": ""field"",
                ""type"": ""fixed"",
                ""size"": 6
            }
        }
    ]
}
In Rust, I might represent this schema with the following struct:

#[derive(Debug, Serialize, Deserialize)]
struct TestStructFixedField {
    field: [u8; 6]
}
I would then expect to be able to use `apache_avro::to_avro_datum()` to convert an instance of `TestStructFixedField` into an `Vec<u8>` using an instance of `Schema` initialized from the schema listed above.

However, this fails because the `Value` produced by `apache_avro::to_value()` represents `field` as an `Value::Array<Value::Int>` rather than a `Value::Fixed<6, Vec<u8>` which does not pass schema validation.

I believe that there are two options to fix this:
1. Allow Value::Array<Vec<Value::Int>> to pass validation if the array has the expected length, and none of the contents of the array are out-of-range for u8. If we go down this route, the implementation of `to_avro_datum()` will have to take care of converting Value::Int to u8 when converting into bytes.
2. Update `apache_avro::to_value()` such that fixed length arrays are converted into `Value::Fixed<N, Vec<u8>>` rather than `Value::Array`.",Fixed,Resolved,9/28/2022 13:14:00,7/18/2024 13:38:00,9/28/2022 13:41:00,90,659,659.00
Enabling ranger plugin config should modify dependent configs,https://issues.apache.org/jira/browse/AMBARI-9626,Fixed,Major,"Changes required for enabling Ranger plugin
HDFS

Property	Value	File
dfs.permissions.enabled	true	hdfs-site.xml
HIVE

Property	Value	File
hive.security.authorization.enabled	true	hive-site.xml
hive.security.authorization.manager	com.xasecure.authorization.hive.authorizer.XaSecureHiveAuthorizerFactory	hiveserver2-site.xml
hive.security.authenticator.manager	org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator	hiveserver2-site.xml
hive.conf.restricted.list	Must contain all elements of hive.security.authorization.enabled, hive.security.authorization.manager,hive.security.authenticator.manager	hive-site.xml
HBASE

Property	Value	File
hbase.security.authorization	true	hbase-site.xml
hbase.coprocessor.master.classes	Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor and add if not present	hbase-site.xml
hbase.coprocessor.region.classes	Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor	hbase-site.xml
hbase.rpc.protection	privacy	hbase-site.xml
KNOX
Replace instances of AclsAuthz with XASecurePDPKnox in topology.xml

STORM

Property	Value	File
nimbus.authorizer	com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer	storm.yaml
Note that nimbus.authorizer should be added only when the cluster is already Kerberized; having this property in a non-Kerberized cluster causes Storm to fail.

Changes required for disabling Ranger plugin
HDFS

Property	Value	File
HIVE

Property	Value	File
hive.security.authorization.manager	org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory	hiveserver2-site.xml
hive.security.authenticator.manager	org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator	hiveserver2-site.xml
HBASE

Property	Value	File
hbase.coprocessor.master.classes	Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor	hbase-site.xml
hbase.coprocessor.region.classes	Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor	hbase-site.xml
hbase.rpc.protection	authentication	hbase-site.xml
KNOX
Replace instance of XASecurePDPKnox with AclsAuthz in topology.xml

STORM

Property	Value	File
nimbus.authorizer	backtype.storm.security.auth.authorizer.SimpleACLAuthorizer com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer	storm.yaml
Note that nimbus.authorizer should be added only when the cluster is already Kerberized; having this property in a non-Kerberized cluster causes Storm to fail.",Fixed,Resolved,2/13/2015 15:06:00,3/17/2015 18:31:00,2/15/2015 15:14:00,5,32,30.00
KnoxSSO broken on recent Chrome browsers (version > 80),https://issues.apache.org/jira/browse/KNOX-2387,Fixed,Major,"Google chrome changed the default behavior of SameSite parameter in Set-Cookie header from None to Lax. This causes partial breakage of Knox SSO. 

Details about Chrome browser feature - https://www.chromestatus.com/feature/5088147346030592

How it affects - https://support.okta.com/help/s/article/FAQ-How-Chrome-80-Update-for-SameSite-by-default-Potentially-Impacts-Your-Okta-Environment",Fixed,Resolved,6/11/2020 10:56:00,6/17/2020 10:50:00,6/17/2020 10:48:00,1,6,0.00
"Infinite loop on server disconnect",https://issues.apache.org/jira/browse/HTTPCORE-528,Fixed,Major,"I am seeing HTTP NIO client get into an infinite loop after the server disconnections the connection. See the log output attached; note some content removed for privacy.

 

Note that the HttpAsyncClient has returned the response and the application has completely processed it. The client maintains the connection, as expected, since the response included `Keep-Alive: timeout=5`. Five seconds after everything is complete, the server closes the TCP connection. The client reacts accordingly: the selector wakes up, does a read of -1 bytes, closes the session and sets a 1 second timeout to close the connection in.

 

The infinite loop occurs because the selector.select() call constantly returns in AbstractIOReactor.execute()

readyCount == 1, so events are processed

processEvent() notes the key is readable and calls:

  session.resetLastRead()

  readable(key);

Because resetLastRead() is constantly updated, the 1 second timeout is never reached and AbstractIOReactor.timeoutCheck() can never call sessionTimedOut() or close the connection.

 

Note the entire time this is happening, netstat shows the connection is in CLOSE_WAIT state. The infinite loop continues until the OS keepalive timeout is reached and the connection is cleaned by the OS.

I am not sure if this epoll / selector behavior is expected or not. However, I have replicated this issue in multiple environments. It seems like the async client should handle this by detecting the condition and closing the connection.

 

Other notes from this infinite loop state:

SSLIOSession.updateEventMask() never closes the session either since the state remains `CLOSING`

AbstractIODispatch.inputReady() does not  read any data from the connection since ssliosession.isAppInputReady() evaluates to false.

SelectionKeyImpl.interestOps remains 1 (`OP_READ`)",Fixed,Resolved,6/21/2018 23:56:00,6/26/2018 13:56:00,6/22/2018 11:56:00,8,5,4.00
Improve the way job history files are managed,https://issues.apache.org/jira/browse/MAPREDUCE-323,Fixed,Critical,"Today all the jobhistory files are dumped in one job-history folder. This can cause problems when there is a need to search the history folder (job-recovery etc). It would be nice if we group all the jobs under a user folder. So all the jobs for user amar will go in history-folder/amar/. Jobs can be categorized using various features like jobid, date, jobname etc but using username will make the search much more efficient and also will not result into namespace explosion.",Fixed,Resolved,11/17/2008 7:48:00,9/7/2011 8:35:00,11/8/2011 17:47:00,67,1024,62.00
Updated yajl-ruby ~> 1.3.1,https://issues.apache.org/jira/browse/SAMZA-1582,Fixed,Major,"Copying the reminder from Apache Security team:

        Sign in
gstein,

We found a potential security vulnerability in a repository which you have been granted security alert access.

!https://ci4.googleusercontent.com/proxy/LhcZ7iaFoInSfQy1r_J1HOWPMTtxYnupwChIFTSA_wTfxDY3HgcGigfxusXNNAZ63YSBgrW9Ng_0lnJNyuw-HZe8OlzcTvPaKX4OXHI=s0-d-e1-ft#https://avatars3.githubusercontent.com/u/47359?s=56&v=4        width=28,height=28!        apache/samza
Known high severity security vulnerability detected in {{yajl-ruby < 1.3.1}}defined in Gemfile.lock.
Gemfile.lock update suggested: yajl-ruby ~> 1.3.1.
Always verify the validity and compatibility of suggestions with your codebase.
 
Review vulnerable dependency
Only users who have been assigned access to security alerts will receive these notifications.
Unsubscribe · Email preferences · Terms · Privacy · Sign into GitHub
GitHub, Inc. 
88 Colin P Kelly Jr St. 
San Francisco, CA 94107

 {quote}",Fixed,Resolved,2/12/2018 18:29:00,4/27/2018 21:54:00,4/24/2018 1:21:00,2,74,3.00
Fix HBASE-22492 on branch-2 (SASL GapToken),https://issues.apache.org/jira/browse/HBASE-25586,Fixed,Major,"The issue is also exist on branch-2.

17:27:41.556 [pool-1-thread-8] WARN org.apache.hadoop.hbase.client.ScannerCallable - Ignore, probably already closed. Current scan: {""startRow"":""19999999"",""stopRow"":"""",""batch"":20,""cacheBlocks"":true,""totalColumns"":0,""maxResultSize"":""2097152"",""families"":{},""caching"":2147483647,""maxVersions"":1,""timeRange"":[""0"",""9223372036854775807""]} on table: cluster_test
javax.security.sasl.SaslException: Call to XXX/172.27.162.2:22101 failed on local exception: javax.security.sasl.SaslException: Gap token
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.ipc.IPCUtil.wrapException(IPCUtil.java:224)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:383)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)
	at org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:117)
	at org.apache.hadoop.hbase.ipc.Call.setException(Call.java:132)
	at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.cleanupCalls(NettyRpcDuplexHandler.java:203)
	at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.exceptionCaught(NettyRpcDuplexHandler.java:220)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273)
	at org.apache.hbase.thirdparty.io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:143)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:381)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: javax.security.sasl.SaslException: Gap token
	at com.sun.security.sasl.gsskerb.GssKrb5Base.checkMessageProp(GssKrb5Base.java:142)
	at com.sun.security.sasl.gsskerb.GssKrb5Base.unwrap(GssKrb5Base.java:81)
	at org.apache.hadoop.hbase.security.SaslUnwrapHandler.channelRead0(SaslUnwrapHandler.java:52)
	at org.apache.hadoop.hbase.security.SaslUnwrapHandler.channelRead0(SaslUnwrapHandler.java:33)
	at org.apache.hbase.thirdparty.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	... 20 common frames omitted
The is can be reproduced with the attached code. hbase.rpc.protection must be set to integrity or privacy.",Fixed,Resolved,2/18/2021 9:31:00,2/26/2021 13:07:00,2/26/2021 21:12:00,5,8,0.00
CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest is broken on Fedora 25.,https://issues.apache.org/jira/browse/MESOS-7049,Fixed,Major,"Test output:

[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest
[ RUN      ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest
../../src/tests/containerizer/cgroups_tests.cpp:1020: Failure
(statistics).failure(): Failed to parse perf sample: Failed to parse perf sample line '6186960975,,cycles,mesos_test,2000511515,100.00,3.093,GHz': Unexpected number of fields
../../src/tests/containerizer/cgroups_tests.cpp:193: Failure
(cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy
[  FAILED  ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest (2123 ms)
[----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest (2123 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:836: Failure
Failed
Tests completed with child processes remaining:
-+- 20455 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest
 \--- 20500 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest
[==========] 1 test from 1 test case ran. (2141 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest
Software versions:

[jpeach@jpeach src]$ uname -a
Linux jpeach.apple.com 4.9.6-200.fc25.x86_64 #1 SMP Thu Jan 26 10:17:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
[jpeach@jpeach src]$ perf -v
perf version 4.9.6.200.fc25.x86_64.g51a0
[jpeach@jpeach src]$ cat /etc/os-release
NAME=Fedora
VERSION=""25 (Workstation Edition)""
ID=fedora
VERSION_ID=25
PRETTY_NAME=""Fedora 25 (Workstation Edition)""
ANSI_COLOR=""0;34""
CPE_NAME=""cpe:/o:fedoraproject:fedora:25""
HOME_URL=""https://fedoraproject.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_BUGZILLA_PRODUCT=""Fedora""
REDHAT_BUGZILLA_PRODUCT_VERSION=25
REDHAT_SUPPORT_PRODUCT=""Fedora""
REDHAT_SUPPORT_PRODUCT_VERSION=25
PRIVACY_POLICY_URL=https://fedoraproject.org/wiki/Legal:PrivacyPolicy
VARIANT=""Workstation Edition""
VARIANT_ID=workstation
The test then fails to clean up, leaving stale processes and cgroups.",Fixed,Resolved,2/1/2017 23:49:00,4/28/2017 23:32:00,2/15/2017 23:26:00,3,86,72.00
NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/ColumnVector,https://issues.apache.org/jira/browse/RANGER-4178,Fixed,Critical,"Observed below error when enabled audit type as ORC format.

NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/ColumnVector

https://issues.apache.org/jira/browse/RANGER-1837

https://issues.apache.org/jira/browse/RANGER-3235

cc: rmani ",Fixed,Resolved,4/8/2023 12:52:00,8/25/2023 16:11:00,4/11/2023 13:07:00,18,139,136.00
Race between FeatureService and ConfigAdmin for resolving mvn: URLs?,https://issues.apache.org/jira/browse/KARAF-910,Fixed,Blocker,"I have an intermittent problem where my custom features.xml cannot be resolved. I use a tweaked etc/org.ops4j.pax.url.mvn.cfg file so the features.xml file is never present in $HOME/.m2/repo but is instead is resolved to local repo relative to the app:

org.ops4j.pax.url.mvn.defaultRepositories=file:${karaf.base}/system@snapshots,\
file:${karaf.home}/${karaf.default.repository}@snapshots,\
file:${karaf.base}/../../../.env/.m2/repo@snapshots

Sometimes when I start Karaf, I get this error (actual URL edited for privacy)

karaf@tsf> 2011-09-30 09:23:09,760 WARN [FeaturesServiceImpl.java:924] Unable to add features repository mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features at startup - o.a.k.f.i.FeaturesServiceImpl
java.lang.RuntimeException: URL [mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features] could not be resolved.
at org.ops4j.pax.url.mvn.internal.Connection.getInputStream(Connection.java:195) [na:na]
at org.ops4j.pax.url.mvn.internal.AetherBridgeConnection.getInputStream(AetherBridgeConnection.java:68) [na:na]
at org.apache.karaf.features.internal.FeatureValidationUtil.validate(FeatureValidationUtil.java:49) [na:na]
at org.apache.karaf.features.internal.FeaturesServiceImpl.validateRepository(FeaturesServiceImpl.java:199) [na:na]
at org.apache.karaf.features.internal.FeaturesServiceImpl.internalAddRepository(FeaturesServiceImpl.java:210) [na:na]
at org.apache.karaf.features.internal.FeaturesServiceImpl.start(FeaturesServiceImpl.java:922) [na:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [na:1.6.0_26]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) [na:1.6.0_26]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) [na:1.6.0_26]
at java.lang.reflect.Method.invoke(Method.java:597) [na:1.6.0_26]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:226) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:824) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:636) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:724) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:640) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:331) [org.apache.aries.blueprint:0.3.1]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:227) [org.apache.aries.blueprint:0.3.1]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [na:1.6.0_26]
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_26]
at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_26]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) [na:1.6.0_26]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206) [na:1.6.0_26]
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_26]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_26]
at java.lang.Thread.run(Thread.java:662) [na:1.6.0_26]

If I put a breakpoint in org.ops4j.pax.url.mvn.internal.Connection.getInputStream(), I can see that when it fails m_configuration.getDefaultRepositories() contains one repo ($HOME/.m2/repo) and when it succeeds m_configuration.getDefaultRepositories() contains the three repos I've specified in etc/org.ops4j.pax.url.mvn.cfg.

I interpret that to mean that sometimes the features resolution happens before Felix reads the files in etc/ and sometimes the features load afterward. Mostly I'm using the same startlevels as Karaf – my startup.properties file is identical to the following except for a few additions I made.

http://svn.apache.org/viewvc/karaf/trunk/assemblies/apache-karaf/src/main/filtered-resources/etc/startup.properties?revision=1176017&view=markup",Fixed,Resolved,9/30/2011 15:40:00,3/12/2012 13:33:00,1/10/2011 1:18:00,20,164,427.00
SimpleRpcServer is broken,https://issues.apache.org/jira/browse/HBASE-27097,Fixed,Blocker,"Concerns about SimpleRpcServer are not new, and not new to 2.5.  @chenxu noticed a problem on HBASE-23917 back in 2020. After some simple evaluations it seems quite broken.

When I run an async version of ITLCC against a 2.5.0 cluster configured with hbase.rpc.server.impl=SimpleRpcServer, the client almost immediately stalls because there are too many in flight requests. The logic to pause with too many in flight requests is my own. That's not important. Looking at the server logs it is apparent that SimpleRpcServer is quite broken. Handlers suffer frequent protobuf parse errors and do not properly return responses to the client. This is what stalls my test client. Rather quickly all available request slots are full of requests that will have to time out on the client side.

Exceptions have three patterns but they all have in common SimpleServerRpcConnection#process. It seems likely the root cause is mismatched expectations or bugs in connection buffer handling in SimpleRpcServer/SimpleServerRpcConnection versus downstream classes that process and parse the buffers. It also seems likely that changes were made to downstream classes like ServerRpcConnection expecting NettyRpcServer's particulars without updating SimpleServerRpcConnection and/or SimpleRpcServer. That said, this is just a superficial analysis.

1) ""Protocol message end-group tag did not match expected tag""

 2022-06-07T16:44:04,625 WARN  [Reader=5,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.
    at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.invalidEndTag(InvalidProtocolBufferException.java:129) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
    at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.checkLastTagWas(CodedInputStream.java:4034) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
    at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4275) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9981) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9910) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14190) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15304) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14651) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:420) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
    at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:317) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
    at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.mergeFrom(ProtobufUtil.java:2638) ~[hbase-client-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
2) ""Protocol message tag had invalid wire type.""

2022-06-07T16:44:04,705 WARN  [Reader=6,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException$InvalidWireTypeException: Protocol message tag had invalid wire type.
	at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.invalidWireType(InvalidProtocolBufferException.java:134) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:527) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.GeneratedMessageV3.parseUnknownField(GeneratedMessageV3.java:320) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10700) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10620) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11481) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11475) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9981) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9910) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14190) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15304) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14651) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:420) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:317) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.mergeFrom(ProtobufUtil.java:2638) ~[hbase-client-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:829) ~[?:?]
3) ""While parsing a protocol message, the input ended unexpectedly in the middle of a field.""

2022-06-07T16:44:04,885 WARN  [Reader=9,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.
	at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:107) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readRawLittleEndian64(CodedInputStream.java:4478) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readFixed64(CodedInputStream.java:4167) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:511) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.GeneratedMessageV3.parseUnknownField(GeneratedMessageV3.java:320) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10700) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10620) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11481) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11475) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9981) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9910) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14190) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15304) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14651) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:420) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:317) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0]
	at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.mergeFrom(ProtobufUtil.java:2638) ~[hbase-client-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:829) ~[?:?]",Fixed,Resolved,6/7/2022 23:57:00,7/12/2022 18:18:00,6/7/2022 23:58:00,35,35,35.00
add -Wall to compile log4cxx will get many warning,https://issues.apache.org/jira/browse/LOGCXX-14,Fixed,Minor,"When I build my project using log4cxx with -Wall ,it will get many warning. I think that why don't you use -Wall in log4cxx to check source code. it's very horrible to see these warning !",Fixed,Resolved,9/8/2004 7:35:00,5/17/2007 19:30:00,3/16/2005 7:59:00,7,981,792.00
WSDL2Java fails with imported schemas in WSDL with file not found.,https://issues.apache.org/jira/browse/AXIS2-796,Fixed,Major,"When using WSDL2Java as ant task schemas that are imported as relative and contained in the same directory as the WSDL cause the task to fail. It finds them early on and then looks in a different directory later on according to the output. This same WSDL and xsd's will work fine in the Eclipse AxisCodegen plugin.

I believe this is possibly due to the basedir not being set correctly. The Eclipse tool specifically sets the basedir early on in the configuration whereas WSDL2Java does not.

Stack trace produced.

wsdl2java:
[delete] Deleting directory D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\target\generated-sources\java
[java] Retrieving schema at 'CustomerHeaderData.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/'.
[java] Retrieving schema at 'CustomerMessages.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/'.
[java] Retrieving schema at 'CustomerData.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/CustomerMessages.xsd'.
[java] org.apache.axis2.wsdl.codegen.CodeGenerationException: Error parsing WSDL
[java] at org.apache.axis2.wsdl.codegen.CodeGenerationEngine.<init>(CodeGenerationEngine.java:94)
[java] at org.apache.axis2.wsdl.WSDL2Code.main(WSDL2Code.java:32)
[java] at org.apache.axis2.wsdl.WSDL2Java.main(WSDL2Java.java:21)
[java] Caused by: org.apache.axis2.AxisFault: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified); nested exception is:
[java] java.lang.RuntimeException: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified)
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.populateService(WSDL2AxisServiceBuilder.java:243)
[java] at org.apache.axis2.wsdl.codegen.CodeGenerationEngine.<init>(CodeGenerationEngine.java:87)
[java] ... 2 more
[java] Caused by: java.lang.RuntimeException: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified)
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1916)
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1929)
[java] at org.apache.ws.commons.schema.SchemaBuilder.handleImport(SchemaBuilder.java:1714)
[java] at org.apache.ws.commons.schema.SchemaBuilder.handleXmlSchemaElement(SchemaBuilder.java:126)
[java] at org.apache.ws.commons.schema.XmlSchemaCollection.read(XmlSchemaCollection.java:250)
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.getXMLSchema(WSDL2AxisServiceBuilder.java:959)
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.copyExtensibleElements(WSDL2AxisServiceBuilder.java:1067)
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.populateService(WSDL2AxisServiceBuilder.java:221)
[java] ... 3 more
[java] Caused by: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified)
[java] at org.apache.ws.commons.schema.XmlSchemaCollection.read(XmlSchemaCollection.java:221)
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1911)
[java] ... 10 more
[java] Java Result: 1
[java] Exception in thread ""main""",Fixed,Resolved,6/2/2006 22:02:00,8/10/2006 12:41:00,7/19/2006 13:29:00,3,69,22.00
Include documentation for previous version on the website,https://issues.apache.org/jira/browse/HELIX-270,Fixed,Major,"The documentation on the website is for 0.6.2 but as far as I can see, this version is still under heavy development.

I can't find a link to the documentation for the 0.6.1 release. I can build the website locally, though I don't get the latest doc fixes for 0.6.1, and that's much less convenient anyway.

Could you provide separate links to the documentation of the currently released version, previous versions (in the future) and dev version?

Same for the javadoc: the online one is 0.6.2-snapshot I think (you should probably display a version number on the javadoc headers).

Thanks!",Fixed,Resolved,10/11/2013 10:44:00,11/15/2013 22:57:00,10/18/2013 20:35:00,5,35,28.00
axis is vulnerable to XXE,https://issues.apache.org/jira/browse/AXIS-471,Fixed,-,"See note from ""Gregory Steuck"" <greg@nest.cx>. He posted an advisory @
http://groups.google.com/groups?selm=apn8hv%2421a9%241%40FreeBSD.csie.NCTU.edu.tw&oe=UTF-8&output=gplain
as well.

============================================================================
Hi Davanum,

I sent a similar message to Sam Ruby yesterday but never got a reply.
You seem to be actively committing changes to Axis, so you may be in a
better position to address the problem.

Unfortunately Axis (at least 1.0) is vulnerable to XXE attack as
described in my post below.

Thanks
Greg
----BEGIN PGP SIGNED MESSAGE----
Hash: SHA1

Gregory Steuck security advisory #1, 2002

Overview:
XXE (Xml eXternal Entity) attack is an attack on an application that parses
XML input from untrusted sources using incorrectly configured XML parser.
The application may be coerced to open arbitrary files and/or TCP connections.

Legal Notice:
This Advisory is Copyright (c) 2002 Gregory Steuck.
You may distribute it unmodified.
You may not modify it and distribute it or distribute parts
of it without the author's written permission.

Disclaimer:
The information in this advisory is believed to be true though
it may be false.
The opinions expressed in this advisory and program are my own and
not of any company. The usual standard disclaimer applies,
especially the fact that Gregory Steuck is not liable for any damages
caused by direct or indirect use of the information or functionality
provided by this advisory or program. Gregory Steuck bears no
responsibility for content or misuse of this advisory or program or
any derivatives thereof.
Anything in this document may change without notice.

Details:
External entity references allow embedding data outside the main file into
an XML document. In the DTD, one declares the external reference with the
following syntax:
<!ENTITY name SYSTEM ""URI"">

XML processor behavior as specified is
http://www.w3.org/TR/REC-xml#include-if-valid:

""When an XML processor recognizes a reference to a parsed entity, in
order to validate the document, the processor must include its
replacement text. If the entity is external, and the processor is not
attempting to validate the XML document, the processor may, but need
not, include the entity's replacement text...""

Now assume that the XML processor parses data originating from a source under
attacker control. Most of the time the processor will not be validating,
but it MAY include the replacement text thus initiating an unexpected
file open operation, or HTTP transfer, or whatever system ids the XML
processor knows how to access.

Suspect systems:
The buzz on the street is ""web services"". They accept XML encoded
data over the network, sometimes from untrusted clients. So, the
prime targets are SOAP and XMLRPC implementations. Yet, there are
many more XML based protocols and vulnerability does not necessary
lie with the servers. Pick any ""XML based network protocol"" and
try to apply the attack methodology.

Suggested fix:
Most XML parsers allow their user to explicitly specify external
entity handler. In case of untrusted XML input it is best to prohibit
all external general entities.

Successful exploitation may yield:

DoS on the parsing system by making it open, e.g.
file:///dev/random | file:///dev/urandom | file://c:/con/con
TCP scans using HTTP external entities (including behind firewalls
since application servers often have world view different
from that of the attacker)
Unauthorized access to data stored as XML files on the parsing
system file system (of course the attacker still needs a way to
get these data back)
DoS on other systems (if parsing system is allowed to establish
TCP connections to other systems)
NTLM authentication material theft by initiating UNC file access to
systems under attacker control (far fetched?)
Doomsday scenario: A widely deployed and highly connected application
vulnerable to this attack may be used for DDoS.
Products review:
Several SOAP and XMLRPC implementation were found vulnerable. I will
be contacting their respective authors directly. It will be up to
those authors to publish the patches and/or advisories.

The following implementations were found NOT vulnerable and the reasons
contributing to their resistance were researched.

Java:
Apache XML-RPC server is NOT vulnerable in the default configuration
due to its use of MinML parser which doesn't support external entities.
Yet should be vulnerable if used with a full blown parser like Xerces
or Crimson. To make it invulnerable in all configurations it needs to
explicitly setup an EntityResolver that aborts having found external
entities.

Marquée XML-RPC also uses MinML and thus is NOT vulnerable.

XMLRPC-J uses freeDOM that only supports Minimal XML which
lacks entity references (http://www.docuverse.com/smldev/minxml.jsp)

WebLogic 6.1sp3 SOAP implementation was NOT found vulnerable. It
appears to be using a parser that ignores entities altogether. Ignorance
is bliss...

Python:
Python 2.2 SimpleXMLRPCServer does NOT seem to be vulnerable. It can use
multiple different parsers:

xmllib.XMLParser is the default one shipped with Python. It
doesn't implement processing of doctype definition and thus doesn't
understand external entities defined in there
ExpatParser is used when expat python-expat is installed,
it understands the references but seems to replace them with
empty strings unconditionally. This negates the attack.
SGMLOP parser, judging by comments in its source doesn't recognize
external entities
FastParser was not available for inspection
Acknowledgments:
Even though the issue was discovered and researched independently I
cannot claim to be the first one to realize the risks associated with
XML external entities. E.g. RFC 2518 discusses the issue in section
17.7 Implications of XML External Entities.

----BEGIN PGP SIGNATURE----
Version: GnuPG v1.0.6 (OpenBSD)
Comment: Processed by Mailcrypt 3.5.6 and Gnu Privacy Guard <http://www.gnupg.org/>

iEYEARECAAYFAj2/FZkACgkQCxVCvY31obB6vQCbBlV+v0jDRQQ7GcNxYRtajtAf
FxUAnRCDfjLy2692iGF3Ewmxzo/VXYmz
=t4QF
----END PGP SIGNATURE----
============================================================================",Fixed,Resolved,10/31/2002 6:39:00,2/24/2004 0:25:00,10/31/2002 6:57:00,1,481,481.00
PDF parsing to XHTML results in tika attempting to write invalid HTML characters.,https://issues.apache.org/jira/browse/TIKA-2955,Fixed,Major,"Hi, I am trying to parse: 314.pdf

what is happening when I try to convert it to XHTML is my XML parser fails because:

14:35:12.876 [main] ERROR com.funnelback.common.filter.TikaFilterProvider - Unable to filter stream with document type '.pdf'
org.xml.sax.SAXException: net.sf.saxon.trans.XPathException: Illegal HTML character: decimal 147
 at net.sf.saxon.event.ReceivingContentHandler.endElement(ReceivingContentHandler.java:538) ~[Saxon-HE-9.9.0-2.jar:?]
 at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.sax.SecureContentHandler.endElement(SecureContentHandler.java:256) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.sax.SafeContentHandler.endElement(SafeContentHandler.java:274) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.sax.XHTMLContentHandler.endDocument(XHTMLContentHandler.java:229) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.parser.pdf.AbstractPDF2XHTML.endDocument(AbstractPDF2XHTML.java:556) ~[tika-parsers-1.19.1.jar:1.19.1]
 at org.apache.pdfbox.text.PDFTextStripper.writeText(PDFTextStripper.java:267) ~[pdfbox-2.0.12.jar:2.0.12]
 at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:117) ~[tika-parsers-1.19.1.jar:1.19.1]
 at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:172) ~[tika-parsers-1.19.1.jar:1.19.1]
 at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.19.1.jar:1.19.1]
 at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:143) ~[tika-core-1.19.1.jar:1.19.1]
 at 
[removed section of trace]
Caused by: net.sf.saxon.trans.XPathException: Illegal HTML character: decimal 147
 at net.sf.saxon.serialize.HTMLEmitter.writeEscape(HTMLEmitter.java:379) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.serialize.XMLEmitter.characters(XMLEmitter.java:662) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.serialize.HTMLEmitter.characters(HTMLEmitter.java:441) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.serialize.HTMLIndenter.characters(HTMLIndenter.java:216) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.event.SequenceNormalizer.characters(SequenceNormalizer.java:183) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.event.ReceivingContentHandler.flush(ReceivingContentHandler.java:646) ~[Saxon-HE-9.9.0-2.jar:?]
 at net.sf.saxon.event.ReceivingContentHandler.endElement(ReceivingContentHandler.java:526) ~[Saxon-HE-9.9.0-2.jar:?]
 ... 43 more
It looks like tika is asking the XML library to handle chracter 147 ie 0x93 which is not allowed in HTML.

This saxon XML library is not happy with that, I think the default java one doesn't complain when given the invalid character though, however tika is probably wrong to write out that character when writing XHTML.",Fixed,Resolved,10/3/2019 6:12:00,10/10/2019 3:20:00,10/3/2019 22:24:00,22,7,7.00
ConcurrentModificationException thrown from inside camel splitter,https://issues.apache.org/jira/browse/CAMEL-6771,Fixed,Major,"We use camel 2.11.1 running on the oracle 1.7 jvm for linux.

I have a route that looks like this. It reads in files and puts them on a seda queue with 8 concurrent consumers.

The SpatialInterpolationPojo reads each file is read and split into two messages X and Y.
The MyAggregator uses X and Y together and outputs a combined message A.B
The MySplitterPojo splits A.B into two messages A and B
from(""file://somefile"") 
    .to(""seda:filteraccept?concurrentConsumers=8""); 

from(""seda:filteraccept?concurrentConsumers=8"") 
    .split() 
    .method(new SpatialInterpolationPojo(), ""split"") 
    .to(""direct:wind-aggregator""); 

from(""direct:wind-aggregator"") 
    .aggregate(packageCorrelationId(), new MyAggregator()) 
    .completionPredicate(header(FIELD_AGGREGATION_COMPLETE).isNotNull()) 
    .split() 
    .method(new MySplitterPojo()) 
    .to(""seda:output""); 
The MySplitterPojo simply returns List<Message> containing two messages that come from data in the input message body. We copy the body headers to the result messages.

It is thread safe, it has no state, ie there are no object fields that are modified.

The method is like this it is edited for clarity/privacy:

public class MySplitterPojo {

 public List<Message> splitMessage( 
            @Headers Map<String, Object> headers, 
            @Body CombinedObject body) { 
    
    DefaultMessage a = new DefaultMessage(); 
    a.setBody(body.getA()); 
    a.setHeaders(new HashMap<String, Object>(headers)); 
            
    DefaultMessage b = new DefaultMessage(); 
    b.setBody(body.getB()); 
    b.setHeaders(new HashMap<String, Object>(headers)); 
  
    ArrayList<Message> result = new ArrayList<Message>(2); 
    result.add(a); 
    result.add(b); 
    
    return result; 
 } 
}
When we run this route we very occasionally get the exception below. You can see that it is entirely within camel, it appears to be trying to copy the map stored under the exchange property Exchange.AGGREGATION_STRATEGY which is a camel internal property key.

By inspection of the message I can see that Exchange has just come out of the WindVectorAggregator.

This seems like it must be a camel bug to me. Any ideas?

15 Sep 2013 23:06:47,140[Camel (camel-1) thread #21 - seda://filteraccept] WARN AggregateProcessor Error processing aggregated exchange. Exchange[Message: { Trondheim, NO=WindVector [u=-5.92894983291626, v=7.060009002685547], ... }]. Caused by: [java.util.ConcurrentModificationException - null] 
java.util.ConcurrentModificationException 
        at java.util.HashMap$HashIterator.nextEntry(Unknown Source) 
        at java.util.HashMap$EntryIterator.next(Unknown Source) 
        at java.util.HashMap$EntryIterator.next(Unknown Source) 
        at java.util.HashMap.putAllForCreate(Unknown Source) 
        at java.util.HashMap.<init>(Unknown Source) 
        at org.apache.camel.processor.MulticastProcessor.setAggregationStrategyOnExchange(MulticastProcessor.java:1011) 
        at org.apache.camel.processor.Splitter.process(Splitter.java:95) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:86) 
        at org.apache.camel.processor.aggregate.AggregateProcessor$1.run(AggregateProcessor.java:495) 
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) 
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source) 
        at java.util.concurrent.FutureTask.run(Unknown Source) 
        at org.apache.camel.util.concurrent.SynchronousExecutorService.execute(SynchronousExecutorService.java:62) 
        at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) 
        at org.apache.camel.processor.aggregate.AggregateProcessor.onSubmitCompletion(AggregateProcessor.java:487) 
        at org.apache.camel.processor.aggregate.AggregateProcessor.onCompletion(AggregateProcessor.java:471) 
        at org.apache.camel.processor.aggregate.AggregateProcessor.doAggregation(AggregateProcessor.java:325) 
        at org.apache.camel.processor.aggregate.AggregateProcessor.process(AggregateProcessor.java:229) 
        at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122) 
        at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:60) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:571) 
        at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:504) 
        at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:213) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RecipientList.sendToRecipientList(RecipientList.java:151) 
        at org.apache.camel.component.bean.MethodInfo$1.doProceed(MethodInfo.java:285) 
        at org.apache.camel.component.bean.MethodInfo$1.proceed(MethodInfo.java:251) 
        at org.apache.camel.component.bean.BeanProcessor.process(BeanProcessor.java:161) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:117) 
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:80) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122) 
        at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:60) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:122) 
        at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:298) 
        at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:117) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:571) 
        at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:504) 
        at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:213) 
        at org.apache.camel.processor.Splitter.process(Splitter.java:98) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:117) 
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:80) 
        at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
        at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
        at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:294) 
        at org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:203) 
        at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:150) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) 
        at java.lang.Thread.run(Unknown Source)",Fixed,Resolved,9/20/2013 6:16:00,9/29/2013 10:34:00,9/23/2013 17:35:00,5,9,6.00
proxy-user not working for Spark on k8s in cluster deploy mode,https://issues.apache.org/jira/browse/SPARK-39399,Fixed,Major,"As part of https://issues.apache.org/jira/browse/SPARK-25355 Proxy user support was added for Spark on K8s. But the PR only added proxy user argument on the spark-submit command. The actual functionality of authentication using the proxy user is not working in case of cluster deploy mode.

We get AccessControlException when trying to access the kerberized HDFS through a proxy user. 

Spark-Submit:
$SPARK_HOME/bin/spark-submit \
--master <K8S_APISERVER> \
--deploy-mode cluster \
--name with_proxy_user_di \
--proxy-user <username> \
--class org.apache.spark.examples.SparkPi \
--conf spark.kubernetes.container.image=<SPARK3.2_with_hadoop3.1_image> \
--conf spark.kubernetes.driver.limit.cores=1 \
--conf spark.executor.instances=1 \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
--conf spark.kubernetes.namespace=<namespace_name> \
--conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \
--conf spark.eventLog.enabled=true \
--conf spark.eventLog.dir=hdfs://<hdfs_cluster>/scaas/shs_logs \

--conf spark.kubernetes.file.upload.path=hdfs://<hdfs_cluster>/tmp \

--conf spark.kubernetes.container.image.pullPolicy=Always \
$SPARK_HOME/examples/jars/spark-examples_2.12-3.2.0-1.jar
Driver Logs:

++ id -u
+ myuid=185
++ id -g
+ mygid=0
+ set +e
++ getent passwd 185
+ uidentry=
+ set -e
+ '[' -z '' ']'
+ '[' -w /etc/passwd ']'
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -z ']'
+ '[' -z ']'
+ '[' -n '' ']'
+ '[' -z x ']'
+ SPARK_CLASSPATH='/opt/hadoop/conf::/opt/spark/jars/*'
+ '[' -z x ']'
+ SPARK_CLASSPATH='/opt/spark/conf:/opt/hadoop/conf::/opt/spark/jars/*'
+ case ""$1"" in
+ shift 1
+ CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=<addr> --deploy-mode client --proxy-user proxy_user --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.SparkPi spark-internal
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0-1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of successful kerberos logins and latency (milliseconds)""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of failed kerberos logins and latency (milliseconds)""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""GetGroups""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since startup""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since last successful login""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
22/04/26 08:54:38 DEBUG SecurityUtil: Setting hadoop.security.token.service.use_ip to true
22/04/26 08:54:38 DEBUG Shell: Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
    at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:469)
    at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:440)
    at org.apache.hadoop.util.Shell.<clinit>(Shell.java:517)
    at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
    at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
    at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:102)
    at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:86)
    at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315)
    at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:303)
    at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1827)
    at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:709)
    at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:659)
    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:570)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/04/26 08:54:38 DEBUG Shell: setsid exited with exit code 0
22/04/26 08:54:38 DEBUG Groups:  Creating new Groups object
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: backing jks path initialized to file:/etc/security/bind.jceks
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: initialized local file as '/etc/security/bind.jceks'.
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: the local file does not exist.
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Usersearch baseDN: dc=<dc>
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Groupsearch baseDN: dc=<dc>
22/04/26 08:54:38 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.LdapGroupsMapping; cacheTimeout=300000; warningDeltaMs=5000
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login commit
22/04/26 08:54:38 DEBUG UserGroupInformation: using local user:UnixPrincipal: 185
22/04/26 08:54:38 DEBUG UserGroupInformation: Using user: ""UnixPrincipal: 185"" with name 185
22/04/26 08:54:38 DEBUG UserGroupInformation: User entry: ""185""
22/04/26 08:54:38 DEBUG UserGroupInformation: Reading credentials from location set in HADOOP_TOKEN_FILE_LOCATION: /mnt/secrets/hadoop-credentials/..2022_04_26_08_54_34.1262645511/hadoop-tokens
22/04/26 08:54:39 DEBUG UserGroupInformation: Loaded 3 tokens
22/04/26 08:54:39 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE)
22/04/26 08:54:39 DEBUG UserGroupInformation: PrivilegedAction as:proxy_user (auth:PROXY) via 185 (auth:SIMPLE) from:org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
22/04/26 08:54:39 DEBUG FileSystem: Loading filesystems
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar
22/04/26 08:54:39 DEBUG FileSystem: Looking for FS supporting hdfs
22/04/26 08:54:39 DEBUG FileSystem: looking for configuration option fs.hdfs.impl
22/04/26 08:54:39 DEBUG FileSystem: Looking in service filesystems for implementation class
22/04/26 08:54:39 DEBUG FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
22/04/26 08:54:39 DEBUG DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
22/04/26 08:54:39 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>/tmp/spark-upload-bf713a0c-166b-43fc-a5e6-24957e75b224/spark-examples_2.12-3.0.1.jar
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
22/04/26 08:54:39 DEBUG RetryUtils: multipleLinearRandomRetry = null
22/04/26 08:54:39 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@4a325eb9
22/04/26 08:54:39 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@2577d6c8
22/04/26 08:54:40 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
22/04/26 08:54:40 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path: [/usr/java/packages/lib, /usr/lib64, /lib64, /lib, /usr/lib]
22/04/26 08:54:40 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
22/04/26 08:54:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/04/26 08:54:40 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
22/04/26 08:54:40 DEBUG DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.transfer.protection = authentication,privacy, configured class dfs.data.transfer.saslproperties.resolver.class = class org.apache.hadoop.security.SaslPropertiesResolver
22/04/26 08:54:40 DEBUG Client: The ping interval is 60000 ms.
22/04/26 08:54:40 DEBUG Client: Connecting to <server>/<ip>:8020
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796)
22/04/26 08:54:40 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE22/04/26 08:54:40 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector.class)
22/04/26 08:54:40 DEBUG SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
22/04/26 08:54:40 DEBUG SaslRpcClient: client isn't using kerberos
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720)
22/04/26 08:54:40 WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG Client: closing ipc connection to <server>/<ip>:8020: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
    at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:757)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
    at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:813)
    at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558)
    at org.apache.hadoop.ipc.Client.call(Client.java:1389)
    at org.apache.hadoop.ipc.Client.call(Client.java:1353)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
    at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.base/java.lang.reflect.Method.invoke(Unknown Source)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
    at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1576)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1591)
    at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)
    at org.apache.hadoop.fs.Globber.doGlob(Globber.java:270)
    at org.apache.hadoop.fs.Globber.glob(Globber.java:149)
    at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2067)
    at org.apache.spark.util.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:318)
    at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:273)
    at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:271)
    at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
    at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
    at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
    at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
    at org.apache.spark.util.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:271)
    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:364)
    at scala.Option.map(Option.scala:230)
    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:364)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
    at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:165)
    at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
    at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)
    at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
    at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:614)
    at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:410)
    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)
    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:796)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796)
    ... 53 more  
 

The reason for no delegation token found is that the proxy user UGI doesn't have any credentials/tokens ( tokenSize:: 0 ) 

22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:<hdfs>, Ident: (token for proxyUser: HDFS_DELEGATION_TOKEN owner=proxyUser, renewer=proxyUser, realUser=superuser/test@test.com, issueDate=1651165129518, maxDate=1651769929518, sequenceNumber=180516, masterKeyId=601)
22/04/28 16:59:37 DEBUG Token: Cannot find class for token kind HIVE_DELEGATION_TOKEN
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 08 73 68 72 70 72 61 73 61 04 68 69 76 65 1e 6c 69 76 79 2f 6c 69 76 79 2d 69 6e 74 40 43 4f 52 50 44 45 56 2e 56 49 53 41 2e 43 4f 4d 8a 01 80 71 1c 71 b5 8a 01 80 b9 35 79 b5 8e 15 cd 8e 03 6e
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: kms-dt, Service: <ip>:9292, Ident: (kms-dt owner=proxyUser, renewer=proxyUser, realUser=superuser, issueDate=1651165129566, maxDate=1651769929566, sequenceNumber=181197, masterKeyId=1152)
22/04/28 16:59:37 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE)
22/04/28 16:59:37 DEBUG UserGroupInformation: createProxyUser: from:org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
22/04/28 16:59:37 DEBUG UserGroupInformation: proxy user created, ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE)  subject::Subject:
    Principal: proxyUser
    Principal: 185 (auth:SIMPLE)
 tokenSize:: 0 
22/04/28 16:59:38 DEBUG AbstractNNFailoverProxyProvider: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE)  tokensize:: 0
22/04/28 16:59:38 DEBUG HAUtilClient: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE)  tokenSize::0
22/04/28 16:59:38 DEBUG AbstractDelegationTokenSelector: kindName:: HDFS_DELEGATION_TOKEN  service:: ha-hdfs:<hdfs> tokens size:: 0
22/04/28 16:59:38 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>:8020/tmp/spark-upload-10582dde-f07c-4bf7-a611-5afbdd12ff6c/spark-examples_2.12-3.0.1.jar 
 

Please refer to the last 4 comments on https://issues.apache.org/jira/browse/SPARK-25355.",Fixed,Resolved,6/7/2022 10:15:00,3/8/2023 3:35:00,6/7/2022 10:26:00,7,274,274.00
Update contribution guide w/ suggestions for GitHub merge button,https://issues.apache.org/jira/browse/BEAM-3264,Fixed,P2,"Today our guide for committers has instructions for rebasing, adjusting commits, etc.

I think with the use of gitbox, we should encourage using the GitHub button, keeping a merge commit when possibly, but editing the default message since it references useless information like the transient branch that was merged.",Fixed,Resolved,11/28/2017 4:09:00,12/14/2017 21:52:00,12/8/2017 6:26:00,2,16,6.00
Remove HDP 3.0 stack from Ambari,https://issues.apache.org/jira/browse/AMBARI-22873,Fixed,Critical,Remove HDP-3.0 stack definition from Ambari. Management Packs will replace stacks in Ambari 3.0.,Fixed,Resolved,1/29/2018 20:25:00,2/1/2018 2:35:00,N/A,0,3,0.00
The OpenJPA web site must post the privacy policy,https://issues.apache.org/jira/browse/OPENJPA-844,Fixed,Major,"During the last few board meetings, the usage of Google Analytics to
track the usage of our web sites was discussed. While this is not a
problem per se, Google requires in its Terms and Conditions that all
sites using it must post a privacy policy (to be exact, paragraph 7 of
the Analytics Terms and Conditions at
http://www.google.com/analytics/tos.html states that ""You must post a
privacy policy and that policy must provide notice of your use of a
cookie that collects anonymous traffic data."")

The legal-discuss group, together with the Jackrabbit PMC and a number
of individuals has pursued this issue and drafted up a privacy policy
for Jackrabbit, which is available at
http://jackrabbit.apache.org/privacy-policy.html

The board would like to thank you for this and appreciates the effort
and diligence that went into it.

If your PMC is collecting information through Google Analytics (ATM
there are at least 18 PMCs using it; you know who you are), we expect
you to set up a privacy policy along the lines of the Jackrabbit PMC and
add a note to your next board report after you have done so.

Thank you for your cooperation.

For the Apache board
Henning",Fixed,Closed,12/23/2008 16:47:00,3/8/2010 15:22:00,3/8/2010 15:22:00,1,440,0.00
typing issue in defaultDeniedProperties (org.apache.unomi.privacy.cfg),https://issues.apache.org/jira/browse/UNOMI-55,Fixed,Major,"Tested CXS 1.1.0 #685:

open /etc/org.apache.unomi.privacy.cfg
the property ""linedInId"" has a typing issue - the ""k"" is missing
-> please add (linkedInId)",Fixed,Closed,9/2/2016 14:59:00,9/20/2016 8:14:00,N/A,0,18,0.00
Email privacy issues in comment notification,https://issues.apache.org/jira/browse/ROL-650,Fixed,Minor,"Notifications of new comments are sent with the commenter's email address as the FROM address.

Having the poster's address as the FROM address seems at first glance to be a ""good thing"", however it means that any returns such as mailbox full, out of office messages etc. go back to that ""untrusted"" address. Also, if the address is a known email spammer, the comment will be posted but the weblog owner may not get the notification. The first issue is a bit of a privacy issue, since a site may decide not to publish contributors' email addresses, only to have the sent out through this route to unknow 3rd parties.",Fixed,Closed,1/28/2005 13:03:00,2/26/2005 22:20:00,1/28/2005 13:26:00,4,29,29.00
Trademarks / privacy policy footer displays broken,https://issues.apache.org/jira/browse/SUREFIRE-1844,Fixed,Trivial,"The footer which is at the end of Surefire's documentation pages, such as this one, have a broken display (at least in Firefox 81 and Google Chrome 85). The horizontal alignment is incorrect, causing the sentence to start outside the visible area and its end to overlap with the ""Privacy policy"" link, as can be seen in the screenshot below:",Fixed,Closed,9/22/2020 21:30:00,9/23/2020 11:34:00,9/23/2020 6:20:00,13,1,0.00
iOS 6 - deal with new Privacy functionality in Contacts (ABAddressBook:: ABAddressBookCreateWithOptions),https://issues.apache.org/jira/browse/CB-902,Fixed,Critical,"Currently crashes if the user does not have AddressBook permission on iOS 6.

The user will get a popup dialog similar to the Geolocation permissions dialog. When creating an address book, we should handle the condition where the app does not have permission, and the address book returned is NULL.",Fixed,Closed,6/12/2012 8:51:00,10/8/2012 12:27:00,6/12/2012 8:55:00,16,118,118.00
Content is getting public to web search engine no privacy,https://issues.apache.org/jira/browse/OFBIZ-4360,Fixed,Major,"all content hosted on ofbiz trees is getting public throuth a general through this link
myhost:8080/ecommerce/control/ViewSimpleContent?dataResourceId=10170",Fixed,Closed,8/3/2011 12:18:00,12/20/2014 9:46:00,12/20/2014 8:09:00,3,1235,0.00
Fix HBase RPC protection documentation,https://issues.apache.org/jira/browse/HBASE-14400,Fixed,Critical,"HBase configuration 'hbase.rpc.protection' can be set to 'authentication', 'integrity' or 'privacy'.
""authentication means authentication only and no integrity or privacy; integrity implies
authentication and integrity are enabled; and privacy implies all of
authentication, integrity and privacy are enabled.""

However hbase ref guide incorrectly suggests in some places to set the value to 'auth-conf' instead of 'privacy'. Setting value to 'auth-conf' doesn't provide rpc encryption which is what user wants.

This jira will fix:

documentation: change 'auth-conf' references to 'privacy'
SaslUtil to support both set of values (privacy/integrity/authentication and auth-conf/auth-int/auth) to be backward compatible with what was being suggested till now.
change 'hbase.thrift.security.qop' to be consistent with other similar configurations by using same set of values (privacy/integrity/authentication).",Fixed,Closed,9/10/2015 19:50:00,9/16/2015 1:56:00,9/10/2015 20:42:00,23,6,6.00
"Obfuscate commentors email addresses when they select ""Notify me by email of new comments""",https://issues.apache.org/jira/browse/ROL-1455,Fixed,Major,"Currently, if a commentor leaves a comment and selects the option to be notified of new comments, they and all other users who selected to be notified receive the email notification with all of their email addresses rendered in the ""To"" line. Need to obfuscate their email adresses for privacy purposes.

",Fixed,Closed,6/20/2007 16:48:00,8/21/2007 13:53:00,6/20/2007 16:58:00,3,62,62.00
Impala website is missing mandatory elements,https://issues.apache.org/jira/browse/IMPALA-11899,Fixed,Major,"The Apache project website checker lists the following problems about impala.apache.org:

missing link to the Apache Privacy Policy
missing copyright notice
see https://whimsy.apache.org/site/project/impala (the link may require committer or PMC privileges)
These entries should be simple to add.",Fixed,Closed,2/6/2023 15:49:00,2/10/2023 13:59:00,2/10/2023 16:17:00,2,4,0.00
CookieSpecBase.domainMatch() leaks cookies to 3rd party domains,https://issues.apache.org/jira/browse/HTTPCLIENT-467,Fixed,Major,"The change committed for #32833
<http://issues.apache.org/bugzilla/show_bug.cgi?id=32833> is buggy; it doesn't
match browser behavior and in fact leaks cookies to third party domains.

To see, try the following:

CookieSpecBase cspec = new CookieSpecBase();
Cookie cookie = new Cookie("".hotmail.com"",""foo"",""bar"",""/"",Integer.MAX_VALUE,false);
cspec.match(""iwanttostealcookiesfromhotmail.com"",80,""/"",false,cookie);

It will return true. Testing in Firefox1.0.4 and IE6 show no such similar
leakage for similar cases. (Indeed, it'd be a headline-making privacy bug if
they were to do this.)

Those browsers do, in my limited testing, behave as desired by the filer of
#32833: a cookie of domain value '.mydomain.com' will be returned to exact host
'mydomain.com' (. However, the fix that was suggested was overbroad.

I suggest instead for CookieSpecBase.domainMatch():

public boolean domainMatch(final String host, final String domain)

{ // BUGGY: matches a '.service.com' cookie to hosts like 'enemyofservice.com' // return host.endsWith(domain) // || (domain.startsWith(""."") && host.endsWith(domain.substring(1))); // BETTER: RFC2109, plus matches a '.service.com' cookie to exact host 'service.com' return host.equals(domain) || (domain.startsWith(""."") && (host.endsWith(domain) || host.equals(domain.substring(1)))); }",Fixed,Closed,6/5/2005 16:05:00,4/22/2007 7:11:00,6/5/2005 17:15:00,15,686,686.00
NonSequential parser gives an error,https://issues.apache.org/jira/browse/PDFBOX-2293,Fixed,Major,"I get the following error when using the sequential parse with Pdfbox 1.8.5.

expected='endstream' actual='' org.apache.pdfbox.io.PushBackInputStream@eb43bd5: java.io.IOException:  at org.apache.pdfbox.pdfparser.BaseParser.parseCOSStream(BaseParser.java:628) [pdfbox-1.8.5.jar:]
        at org.apache.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:605) [pdfbox-1.8.5.jar:]
        at org.apache.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:194) [pdfbox-1.8.5.jar:]
        at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1220) [pdfbox-1.8.5.jar:]
        at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1187) [pdfbox-1.8.5.jar:]
        at org.apache.pdfbox.util.PDFMergerUtility.mergeDocuments(PDFMergerUtility.java:236) [pdfbox-1.8.5.jar:]
        at org.apache.pdfbox.util.PDFMergerUtility.mergeDocuments(PDFMergerUtility.java:185) [pdfbox-1.8.5.jar:]
After looking at some of the fixed issues reported for similar problem(s), I have tried using PDFBox 2.0.0 built from the latest repository code and the nonsequential parser for the pdf processing. However, the file created as randomAccessFile seems to get damaged (cannot be opened in Acrobat Reader after the run) when I use PDFbox 2.0.0 for my processing.
I am unable to attach a sample file because of privacy concerns for the content. I also get an error and am not able to generate the merged output.
The code snippet is as follows-

for (String fName : fileList) {
	pd = null;
        File pdFile = new File(fName);
	fNameStr = fName.substring(0, fName.lastIndexOf('.'))
					+ ""_new.pdf"";

	InputStream is = new FileInputStream(pdFile);
        RandomAccessFile raf = new RandomAccessFile(pdFileNew, ""rws"");
			pd = PDDocument.loadNonSeq(is, raf );
        pd.getDocumentCatalog();
        pd.save(fNameStr);
        pd.close();
	if (is != null) {
	   is.close();
	}
	if(raf != null) {
	  raf.close();
	}

	ut.addSource(fNameStr);
}
FileOutputStream fos = new FileOutputStream(outFileName);
ut.setDestinationStream(fos);
ut.setIgnoreAcroFormErrors(true);
ut.mergeDocuments();
fos.close();
Thank You.",Fixed,Closed,8/27/2014 23:29:00,10/22/2014 6:48:00,8/28/2014 5:44:00,17,56,55.00
validate_cluster.sh should auto-detect DOCKER_ARCH,https://issues.apache.org/jira/browse/YUNIKORN-1308,Fixed,Major,"By default, the script validate_cluster.sh does not detect/set DOCKER_ARCH. This can result in the following problem:

Creating kind validation cluster
  Apache YuniKorn version: 1.1.0
  Helm chart directory:    ./helm-charts/yunikorn
  Kind cluster config:     ./kind.yaml
  Kubernetes image:        kindest/node:v1.22.4
  Registry name:           apache
  Plugin mode:             false
  Image Architecture:      
Creating cluster ""yk8s"" ...
 ✓ Ensuring node image (kindest/node:v1.22.4) 🖼
 ✓ Preparing nodes 📦 📦 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
 ✓ Joining worker nodes 🚜 
Set kubectl context to ""kind-yk8s""
You can now use your cluster with:

kubectl cluster-info --context kind-yk8s

Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

Pre-Loading docker images...

Pre-Loading admission--1.1.0 image failed, aborting
Removing kind cluster
Deleting cluster ""yk8s"" ...
After setting DOCKER_ARCH to amd64, the problem disappeared.",Fixed,Closed,9/6/2022 14:18:00,9/12/2022 13:54:00,9/7/2022 0:47:00,3,6,5.00
"Liberal ""babel"" parser that accepts all SQL dialects",https://issues.apache.org/jira/browse/CALCITE-2280,Fixed,Major,"Create a parser that accepts all SQL dialects.

It would accept common dialects such as Oracle, MySQL, PostgreSQL, BigQuery. If you have preferred dialects, please let us know in the comments section. (If you're willing to work on a particular dialect, even better!)

We would do this in a new module, inheriting and extending the parser in the same way that the DDL parser in the ""server"" module does.

This would be a messy and difficult project, because we would have to comply with the rules of each parser (and its set of built-in functions) rather than writing the rules as we would like them to be. That's why I would keep it out of the core parser. But it would also have large benefits.

This would be new territory Calcite: as a tool for manipulating/understanding SQL, not (necessarily) for relational algebra or execution.

Some possible uses:

analyze query lineage (what tables and columns are used in a query);
translate from one SQL dialect to another (using the JDBC adapter to generate SQL in the target dialect);
a ""deep"" compatibility mode (much more comprehensive than the current compatibility mode) where Calcite could pretend to be, say, Oracle;
SQL parser as a service: a REST call gives a SQL query, and returns a JSON or XML document with the parse tree.
If you can think of interesting uses, please discuss in the comments.

There are similarities with Uber's QueryParser tool. Maybe we can collaborate, or make use of their test cases.

We will need a lot of sample queries. If you are able to contribute sample queries for particular dialects, please discuss in the comments section. It would be good if the sample queries are based on a familiar schema (e.g. scott or foodmart) but we can be flexible about this.",Fixed,Closed,4/25/2018 22:54:00,7/9/2018 8:36:00,4/28/2018 5:29:00,25,75,72.00
Wrong sort elimination when using permuted join order,https://issues.apache.org/jira/browse/DERBY-6148,Fixed,Major,"I have a query that looks like this:

SELECT tests.id,tests.item,title FROM tests,item_usage
WHERE username=? AND user_role>?
AND item_usage.item=tests.item
ORDER BY tests.item,title

The result ordering is by item code followed by title, but the item codes are listed in the order in which they appear in the ITEMS table where they are the primary key rather than in ascending order as expected. If however I change the ORDER BY clause to sort by item_usage.item rather than tests.item, it works correctly, even though the two values are the same!

The same thing happens in another unrelated query involving item_usage, and the same workaround cures it.

The relevant tables are defined like so:

CREATE TABLE item_usage (
username VARCHAR(15) NOT NULL,
item VARCHAR(15) NOT NULL,
value SMALLINT DEFAULT 0,
CONSTRAINT item_usage_pk PRIMARY KEY (username,item),
CONSTRAINT item_usage_1 FOREIGN KEY (username)
REFERENCES users(username)
ON DELETE CASCADE,
CONSTRAINT item_usage_2 FOREIGN KEY (item)
REFERENCES items(item)
ON DELETE CASCADE,
CONSTRAINT item_usage_3 CHECK (value BETWEEN 0 AND 4)
);

CREATE TABLE tests (
id INTEGER GENERATED ALWAYS AS IDENTITY,
item VARCHAR(15) NOT NULL,
title VARCHAR(255) NOT NULL,
disp SMALLINT NOT NULL DEFAULT 0,
starttime TIMESTAMP DEFAULT NULL,
endtime TIMESTAMP DEFAULT NULL,
offsetx INTEGER NOT NULL DEFAULT 0,
offsety INTEGER NOT NULL DEFAULT 0,
rate INTEGER NOT NULL DEFAULT 0,
duration INTEGER NOT NULL DEFAULT 0,
calibrate INTEGER NOT NULL DEFAULT 0,
deadline TIMESTAMP DEFAULT NULL,
stepsize INTEGER NOT NULL DEFAULT 0,
interval INTEGER NOT NULL DEFAULT 0,
stand CHAR(1) DEFAULT NULL,
hidden CHAR(1) DEFAULT NULL,
repeated CHAR(1) DEFAULT NULL,
private CHAR(1) DEFAULT NULL,
sequential CHAR(1) DEFAULT NULL,
final CHAR(1) DEFAULT NULL,
notes CLOB DEFAULT NULL,
testxml CLOB NOT NULL,
author VARCHAR(15) NOT NULL,
time TIMESTAMP NOT NULL,
CONSTRAINT tests_pk PRIMARY KEY (id),
CONSTRAINT tests_1 UNIQUE (item, title),
CONSTRAINT tests_2 FOREIGN KEY (item)
REFERENCES items(item)
ON DELETE CASCADE,
CONSTRAINT tests_3 CHECK (disp BETWEEN 0 AND 100),
CONSTRAINT tests_4 CHECK (rate BETWEEN 0 AND 100),
CONSTRAINT tests_5 CHECK (stepsize BETWEEN 0 AND 100)
);

If I run the query manually I get this, as expected:

ID ITEM TITLE
37 60001 Test 1
42 60001 Test 2
51 60001 Test 3
17 61303 Test 2a
16 61303 Test 2b
7 7205731 Test 2a
8 7205731 Test 2b

Now, this is actually part of a web app that should turn this into a list of options in a <select> item using the following code:

while (query.next())

{ println(""<option value='"" + query.getInt(""id"") + ""'>"" + encode(query.getString(""item"") + "": "" + query.getString(""title"")) + ""</option>""); }
What I actually get is this:

<option value=""17"">61303: Test 2a</option>
<option value=""16"">61303: Test 2b</option>
<option value=""7"">7205731: Test 2a</option>
<option value=""8"">7205731: Test 2b</option>
<option value=""37"">60001: Test 1</option>
<option value=""42"">60001: Test 2</option>
<option value=""51"">60001: Test 3</option>

The results are sorted by item then by title, but the item order is the order in which they were originally inserted into the items table (where the item and item description are stored, referenced by item_usage.item) rather than by item code.

I've tried to reproduce this behaviour in a simple example, but without success. I have logged the query plans for both versions; the log output is as follows, with the INCORRECT query (using ORDER BY tests.item) followed later by the CORRECT query (using ORDER BY item_usage.item):

(moved queryplans to attachment; see attachment queryplans.txt -dagw)",Fixed,Closed,4/3/2013 10:54:00,5/28/2013 11:40:00,5/28/2013 9:28:00,29,55,0.00
Remove dependency on cloudflare CDN,https://issues.apache.org/jira/browse/TINKERPOP-2775,Done,Major,"As per ASF privacy policy [1], we should not host JS or CSS files on CDNs such as Cloudflare instead, we should opt to host the Fonts/JS/CSS locally on the Apache server.

We have multiple instances [2] where we are downloading scripts from cloudflare. This task should remove this dependency and host the files locally.

[1] https://privacy.apache.org/faq/committers.html
 [2] Instances of Fonts/JS/CSS downloaded from CDN (not exhaustive): https://github.com/apache/tinkerpop/blob/master/docs/site/home/index.html#L26 
https://github.com/apache/tinkerpop/blob/master/docs/site/home/index.html#L951 ",Done,Closed,7/27/2022 13:06:00,1/9/2023 16:26:00,1/9/2023 16:26:00,1,166,0.00
Fix website issues,https://issues.apache.org/jira/browse/UNOMI-843,Fixed,Major,"There are multiple issues on the website:

Embedded YouTube videos can't work anymore due to server Content-Server-Policy changes, we must replace them with links or use the technique described here : https://privacy.apache.org/faq/committers.html
Google Analytics tag is old, should be replaced with more recent one. Actually it might be better to replace it with Matomo, see https://privacy.apache.org/faq/committers.html (even better would be Unomi in the future )
Some links use HTTP instead of HTTPS
Fix all the issues reported by Apache's web checker: https://whimsy.apache.org/site/project/unomi
Add the monthly meeting information to the contribute page with the following information (and ideally a calendar link) : https://lists.apache.org/thread/70oo862br3d4g7j8dvnyy3o4z1p0ozfq",Fixed,Closed,7/17/2024 11:28:00,8/27/2024 8:20:00,N/A,0,41,0.00
Broken links on RAT's webpage - mailing-lists / prepare webpage for releasing 0.16.1,https://issues.apache.org/jira/browse/RAT-353,Fixed,Major,"The current webpage at
https://creadur.apache.org/rat/
links to mail-lists.html, which should be
https://creadur.apache.org/rat/mailing-lists.html

Check for other dead links and fix them appropriately.

Further analysis via
{{$ linkchecker https://creadur.apache.org/rat/ -F text/UTF-8/rat-linkchecker.txt
}} showed 108 errors on the current RAT webpage.",Fixed,Closed,1/16/2024 21:38:00,1/23/2024 21:01:00,1/20/2024 23:47:00,33,7,3.00
bin/solr script doesn't do ps properly on some systems,https://issues.apache.org/jira/browse/SOLR-17112,Fixed,Major,"On Google's colab, the following fails:

!wget https://dlcdn.apache.org/solr/solr/9.4.0/solr-9.4.0.tgz && tar -xf solr-9.4.0.tgz && cd solr-9.4.0 && echo `pwd`

!apt update && apt install bc -y && cd solr-9.4.0 && bin/solr stop -p 8983; bin/solr -c -force -Denable.packages=true

!cd solr-9.4.0 && bin/solr package add-repo data-import-handler ""https://raw.githubusercontent.com/searchscale/dataimporthandler/master/repo/""
If I add the following before the last line, it works:

!cat solr-9.4.0/bin/solr|sed -e 's:ps -f -p:ps -fww -p:g' > tmp; cp tmp solr-9.4.0/bin/solr; chmod +x solr-9.4.0/bin/solr
I think that extra ""ww"" is needed to make sure Solr works fine on all systems. FYI dep4b.",Fixed,Closed,1/6/2024 17:29:00,1/26/2024 14:32:00,1/6/2024 17:31:00,14,20,20.00
[pdmodel.font.PDFont] Error: Could not parse predefined CMAP file for 'PDFXC-Indentity0-0',https://issues.apache.org/jira/browse/PDFBOX-940,Fixed,Major,"Hi,

when i am trying to upload a pdf document the following error is thrown in the tomcat.. i am using pdfbox-1.4.0.jar..

17:29:33,465 ERROR [pdmodel.font.PDFont] Error: Could not parse predefined CMAP file for 'PDFXC-Indentity0-0'

please find the solution",Fixed,Closed,1/12/2011 12:08:00,1/1/2014 17:14:00,3/2/2011 7:52:00,30,1085,"1,036.00"
RPC Sasl QOP is broken,https://issues.apache.org/jira/browse/HADOOP-9816,Fixed,Blocker,HADOOP-9421 broke the handling of SASL wrapping for RPC QOP integrity and privacy options.,Fixed,Closed,8/1/2013 16:31:00,8/5/2013 22:18:00,8/1/2013 17:06:00,11,4,4.00
cordova-plugin-geolocation should reverse permissions request in ios8,https://issues.apache.org/jira/browse/CB-8826,Fixed,Minor,"I would like to suggest a change to how the geolocation plugin requests
permissions in iOS8. In the event that both iOS8 NSLocation usage
permissions exist, I suggest that we first request the least permissive one
(NSLocationWhenInUseUsageDescription).

This should amount to simply reversing the logic in CDVLocation.m:

if([[NSBundle mainBundle]
objectForInfoDictionaryKey:@""NSLocationWhenInUseUsageDescription""])
{
            [self.locationManager  requestWhenInUseAuthorization];
} else if([[NSBundle mainBundle] objectForInfoDictionaryKey:@
""NSLocationAlwaysUsageDescription""]) {
            [self.locationManager requestAlwaysAuthorization];
}
I have a use case where an app launches with both descriptions set, but
depending on client configuration the ""AlwaysInUse"" permission may not be necessary. As the logic is written now, the plugin will always request that one, which could look a bit extreme to the end user.",Fixed,Closed,4/8/2015 18:43:00,10/28/2015 19:41:00,7/22/2015 19:52:00,4,203,98.00
TextPosition.getHeight() returns erroneous value for some PDFs,https://issues.apache.org/jira/browse/PDFBOX-1001,Fixed,Major,"For a PDF that worked fine under 1.2.1 the height value returned is negative and the wrong value (i.e. using Math.abs() won't fix it). Other PDFs work fine.
PDF Debug shows ""Creator:Crystal Reports"" and ""Producer:PDF-XChange (XCPRO30.DLL v3.30.0064) (Windows 2k)""
And when examining the 'Stream' items, the text is not what displays.

Any suggestions on what to look for so that I can do differential analysis against other PDFs to see what they do/not have in common with this one?
(It's client data so I can't post the PDF. )

It's stopping us from moving off 1.2.1 (and later versions fix another issue we have of seeing question marks instead of the actual characters).",Fixed,Closed,4/24/2011 4:42:00,8/30/2011 17:29:00,4/26/2011 3:24:00,7,128,126.00
Getting image with black background when converting from PDF to Image!!,https://issues.apache.org/jira/browse/PDFBOX-1023,Fixed,Major,"Everytime I try to conver a PDF file with a graphic on it, to Image (PNG) I get a black background beneath the graphic, where the background is white originally, here's my code:

PDDocument document = PDDocument.load(new File(""C:\\export_settings
testReport.pdf""));
List<PDPage> pages = document.getDocumentCatalog().getAllPages();

for (int i = 0; i < pages.size(); i++)

{ PDPage singlePage = pages.get(i); BufferedImage buffImage = singlePage.convertToImage(); ImageIO.write(buffImage, ""PNG"", new File(""C:\\export_settings\\page"" + i + "".png"")); }
The image quality is good, except for this, I tried with two different methos but I got the same result, please help me, thanks!",Fixed,Closed,5/30/2011 23:50:00,6/7/2011 14:03:00,5/30/2011 23:55:00,6,8,8.00
Copyright Notice of Website outdated,https://issues.apache.org/jira/browse/DIR-240,Fixed,Major,"Although in 2009, the Copyright notice on our website is still

© 2003-2008, The Apache Software Foundation - Privacy Policy

For instance here (footer)
http://directory.apache.org/

The templates need an update (unfortunately, I do not know how to accomplish this)",Fixed,Closed,2/3/2009 18:44:00,2/4/2009 10:18:00,2/4/2009 10:18:00,1,1,0.00
Wire encryption is broken,https://issues.apache.org/jira/browse/HBASE-11149,Fixed,Major,"Upon some testing with the QOP configuration (hbase.rpc.protection), discovered that RPC doesn't work with ""integrity"" and ""privacy"" values for the configuration key. I was using 0.98.x for testing but I believe the issue is there in trunk as well (haven't checked 0.96 and 0.94).",Fixed,Closed,5/9/2014 17:37:00,5/23/2014 1:47:00,5/9/2014 18:30:00,22,14,14.00
Classpath in XML report is wrong,https://issues.apache.org/jira/browse/SUREFIRE-164,Fixed,Minor,"The XML report contains in the property java.class.path Maven's classpath, but not the class path used to execute the tests.",Fixed,Closed,10/5/2006 6:25:00,11/24/2007 0:22:00,11/21/2006 1:32:00,4,415,368.00
Added restriction to historic queries on web UI,https://issues.apache.org/jira/browse/HIVE-17701,Fixed,Major,"The HiveServer2 Web UI (HIVE-12550) shows recently completed queries.
However, a user can see the queries run by other users as well, and that is a security/privacy concern.
Only admin users should be allowed to see queries from other users (similar to behavior of display for configs, stack trace etc).",Fixed,Closed,10/5/2017 0:51:00,10/9/2017 0:04:00,10/5/2017 0:54:00,15,4,4.00
Cannot ship code hints without also shipping file-system paths,https://issues.apache.org/jira/browse/FLEX-23026,Fixed,Major,"For privacy reasons, developers need a way to ship code hints in .swc files without also shipping file-system paths. A new compiler option should be added, perhaps: -exclude-file-paths.

Currently, file paths can only be disabled by setting debug=false, but that results in no hints and no paths.",Fixed,Closed,2/8/2010 11:40:00,10/26/2010 8:10:00,1/28/2012 6:46:00,3,260,459.00
Long rendering time,https://issues.apache.org/jira/browse/PDFBOX-3791,Fixed,Major,"Attached pdf file takes too long (more then 9 secs) to render in PDFDebugger (this is a simplified version of a real life pdf that I can not publish for privacy reasons, it takes 57 seconds to render and it contains 56 images and some text).

I have tried with the options provided in https://pdfbox.apache.org/2.0/getting-started.html but performance is the same",Fixed,Closed,5/11/2017 20:22:00,5/12/2017 11:38:00,5/11/2017 21:34:00,7,1,1.00
Security: passwords logging and file permisions,https://issues.apache.org/jira/browse/DRILL-6189,Fixed,Major,"Prerequisites:
1. Log level is set to ""all"" in the conf/logback.xml:

<logger name=""org.apache.drill"" additivity=""false"">
    <level value=""all"" />
    <appender-ref ref=""FILE"" />
</logger>
2. PLAIN authentication mechanism is configured:

  security.user.auth: {
	enabled: true,
	packages += ""org.apache.drill.exec.rpc.user.security"",
	impl: ""pam"",
	pam_profiles: [ ""sudo"", ""login"" ]
  }
Steps:
1. Start the drillbits
2. Connect by sqlline:

/opt/mapr/drill/drill-1.13.0/bin/sqlline -u ""jdbc:drill:zk=node1:5181;"" -n user1 -p 1111
Expected result: Logs shouldn't contain clear-text passwords

Actual results: During the drillbit startup or establishing connections via the jdbc or odbc, the following lines appear in the drillbit.log:

properties {
    key: ""password""
    value: ""1111""
}
Same thing happens with storage configuration data, everything, including passwords is being logged to file.

Another issue:

Currently Drill config files has the permissions 0644:

-rw-r--r--. 1 mapr mapr 1081 Nov 16 14:42 core-site-example.xml
-rwxr-xr-x. 1 mapr mapr 1807 Dec 19 11:55 distrib-env.sh
-rw-r--r--. 1 mapr mapr 1424 Nov 16 14:42 distrib-env.sh.prejmx
-rw-r--r--. 1 mapr mapr 1942 Nov 16 14:42 drill-am-log.xml
-rw-r--r--. 1 mapr mapr 1279 Dec 19 11:55 drill-distrib.conf
-rw-r--r--. 1 mapr mapr  117 Nov 16 14:50 drill-distrib-mem-qs.conf
-rw-r--r--. 1 mapr mapr 6016 Nov 16 14:42 drill-env.sh
-rw-r--r--. 1 mapr mapr 1855 Nov 16 14:50 drill-on-yarn.conf
-rw-r--r--. 1 mapr mapr 6913 Nov 16 14:42 drill-on-yarn-example.conf
-rw-r--r--. 1 mapr mapr 1135 Dec 19 11:55 drill-override.conf
-rw-r--r--. 1 mapr mapr 7820 Nov 16 14:42 drill-override-example.conf
-rw-r--r--. 1 mapr mapr 3136 Nov 16 14:42 logback.xml
-rw-r--r--. 1 mapr mapr  668 Nov 16 14:51 warden.drill-bits.conf
-rw-r--r--. 1 mapr mapr 1581 Nov 16 14:42 yarn-client-log.xml
As they may contain some sensitive information, like passwords or secret keys, they cannot be viewable to everyone. So I suggest to reduce the permissions at least to 0640.",Fixed,Closed,2/27/2018 12:59:00,3/4/2018 17:14:00,2/28/2018 16:01:00,17,5,4.00
Facebook Like iframe too narrow when in topbar,https://issues.apache.org/jira/browse/MSKINS-92,Fixed,Minor,"See Apache Syncope website at http://syncope.apache.org

On the top right you can see the Facebook Like button rendered by

    <iframe src=""http://www.facebook.com/plugins/like.php?href=http://syncope.apache.org/&send=false&layout=button_count&show-faces=false&action=like&colorscheme=dark""
        scrolling=""no"" frameborder=""0""
        style=""border:none; width:80px; height:20px; margin-top: 10px;""  class=""pull-right"" ></iframe>
when changing style to

""border:none; width:100px; height:20px; margin-top: 10px;""
the right-side box is rendered correctly.",Fixed,Closed,2/18/2014 5:18:00,5/3/2015 16:36:00,12/25/2014 12:49:00,22,439,129.00
UUID replacement,https://issues.apache.org/jira/browse/CB-49,Fixed,Blocker,"reported at: https://github.com/phonegap/phonegap-iphone/issues/238
by: https://github.com/sandstrom

As you might have read iOS 5 will remove the UDID (http://techcrunch.com/2011/08/19/apple-ios-5-phasing-out-udid/).

This is an excellent alternative and it would be nice if you would implement something along these lines to keep the functionality. The idea of hashing together with the bundle id is great, because it makes it impossible to track across applications, which is what apple wanted to fix (although it can be circumvented that would only anger them, and tracking across apps isn't required for most apps anyway).

https://github.com/gekitz/UIDevice-with-UniqueIdentifier-for-iOS-5",Fixed,Closed,11/28/2011 21:02:00,3/23/2012 22:26:00,11/28/2011 21:03:00,12,116,116.00
Skip whitespaces when resolving a XRef,https://issues.apache.org/jira/browse/PDFBOX-1737,Fixed,Major,"Oleg Krechowetzki reported an issue with the non sequential parser via private mail. He provided a working solution and a test pdf which can't be attached due to privacy reasons.

The following exception occurs when parsing the pdf in question using the non sequential parser:

Caused by: java.io.IOException: Error: Expected a long type, actual='xref'
at org.apache.pdfbox.pdfparser.BaseParser.readLong(BaseParser.java:1668)
at org.apache.pdfbox.pdfparser.BaseParser.readObjectNumber(BaseParser.java:1598)
at org.apache.pdfbox.pdfparser.NonSequentialPDFParser.parseXrefObjStream(NonSequentialPDFParser.java:458)",Fixed,Closed,10/3/2013 12:51:00,10/3/2013 12:55:00,10/3/2013 12:55:00,3,0,0.00
iOS 11 Error When Taking Picture Missing NSPhotoLibraryAddUsageDescription,https://issues.apache.org/jira/browse/CB-13332,Fixed,Trivial,"I am using this plugin to take a picture, but in iOS11 I receive the following error:

*
This app has crashed because it attempted to access privacy-sensitive data without a usage description. The app's Info.plist must contain an NSPhotoLibraryAddUsageDescription key with a string value explaining to the user how the app uses this data.*

Note: I have descriptions for CAMERA_USAGE_DESCRIPTION and PHOTOLIBRARY_USAGE_DESCRIPTION. I was able to take a picture in iOS 10.",Fixed,Closed,9/26/2017 2:42:00,10/21/2017 15:29:00,10/5/2017 23:40:00,13,25,16.00
Tighten HFileLink api to enable non-snapshot uses,https://issues.apache.org/jira/browse/HBASE-12749,Fixed,Major,"In HBASE-12332 we'd like to use the FileLink's IO redirecting powers but want to be able to specify arbitrary alternate link paths and not be tied to the SnapshotFileLink file pattern (aka, table=region-hfile).

To do this we need change the constructors and some internals so that it is more generic. Along the way, we remove the FileStatus constructor arguments in favor of Path's and reduce the number of ways to create HFileLinks, and tighten up the scope privacy of many methods.",Fixed,Closed,12/23/2014 3:01:00,12/24/2014 13:09:00,12/23/2014 3:25:00,21,1,1.00
"PDA renderers for tr:page, tr:panelPage and tr:panelPageHeader need to render facets more in line with desktop counterparts",https://issues.apache.org/jira/browse/TRINIDAD-1303,Fixed,Minor,"PDA renderers for tr:page, tr:panelPage and tr:panelPageHeader support different sets of facets and render supported facets at different positions. This makes difficult to develop mobile applications.",Fixed,Closed,11/12/2008 21:58:00,12/5/2008 8:27:00,11/12/2008 22:31:00,3,23,23.00
CCITTFaxG31DDecodeInputStream - Extended codes have wrong length,https://issues.apache.org/jira/browse/PDFBOX-1233,Fixed,Major,"When dealing with large fax images there are Extended Make Up Codes.
They are added to the tree as ...

buildUpMakeUp(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT);
buildUpMakeUp(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT);

Accept, the length is 0 based not starting at 1792.

The quick hack is to create a new method so the length of the node is correct

private static void buildUpMakeUpLong(short[] codes,
NonLeafLookupTreeNode root)
{
for (int len = 0, c = codes.length; len < c; len++)

{ LookupTreeNode leaf = new MakeUpTreeNode((len + 28) * 64); addLookupTreeNode(codes[len], root, leaf); }
}

as thus ...

buildUpMakeUpLong(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT);
buildUpMakeUpLong(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT);",Fixed,Closed,2/21/2012 13:33:00,3/11/2012 13:58:00,2/22/2012 12:14:00,3,19,18.00
Make i18n/LocalizedDisplay.sql and i18n/LocalizedConnectionAttribute.sql behave equally on different platforms,https://issues.apache.org/jira/browse/DERBY-1726,Fixed,Minor,"Myrna van Lunteren commented on DERBY-244:

The one remark I have is that I still cannot get the LocalizedDisplay.sql and LocalizedConnectionAttribute.sql test from the i18n directory to behave the same under windows and Linux (with sun jdk 1.4.2.).
For windows, I had to update the masters for these tests, but running them on Linux still failed for me.
With jdk131, ibm131 and ibm142 the LocalizedDisplay.sql test hung, and LocalizedConnectionAttribute exits with a MalformedInputException.
It would be nice if we could figure out a way to add these tests to the suites...

— stack of LocalizedConnectionAttribute on Linux —
Exception in thread ""main"" sun.io.MalformedInputException
at sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java(Compiled Code))
at sun.nio.cs.StreamDecoder$ConverterSD.convertInto(StreamDecoder.java:287)
at sun.nio.cs.StreamDecoder$ConverterSD.implRead(StreamDecoder.java:337)
at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:223)
at java.io.InputStreamReader.read(InputStreamReader.java:208)
at java.io.BufferedReader.fill(BufferedReader.java:153)
at java.io.BufferedReader.readLine(BufferedReader.java:316)
at java.io.BufferedReader.readLine(BufferedReader.java:379)
at org.apache.derbyTesting.functionTests.harness.RunTest.setDirectories(RunTest.java:729)
at org.apache.derbyTesting.functionTests.harness.RunTest.main(RunTest.java:262)
----------------------------------------------------------------------------",Fixed,Closed,8/18/2006 17:26:00,11/25/2008 8:20:00,6/10/2008 17:50:00,37,830,168.00
java.awt.geom.IllegalPathStateException: missing initial moveto in path definition,https://issues.apache.org/jira/browse/PDFBOX-2728,Fixed,Major,"I get this exception :

java.awt.geom.IllegalPathStateException: missing initial moveto in path definition
        at java.awt.geom.Path2D$Float.needRoom(Path2D.java:280)
        at java.awt.geom.Path2D.closePath(Path2D.java:1769)
        at org.apache.pdfbox.rendering.PageDrawer.closePath(PageDrawer.java:693)
        at org.apache.pdfbox.contentstream.operator.graphics.ClosePath.process(ClosePath.java:35)
        at org.apache.pdfbox.contentstream.PDFStreamEngine.processOperator(PDFStreamEngine.java:788)
        at org.apache.pdfbox.contentstream.PDFStreamEngine.processStreamOperators(PDFStreamEngine.java:454)
        at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:425)
        at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:398)
        at org.apache.pdfbox.contentstream.PDFStreamEngine.processPage(PDFStreamEngine.java:164)
        at org.apache.pdfbox.rendering.PageDrawer.drawPage(PageDrawer.java:164)
        at org.apache.pdfbox.rendering.PDFRenderer.renderPage(PDFRenderer.java:213)
similar to PDFBOX-2189.

I can't include the PDF file for privacy reason but I think a similar solution applied for the other bug could fix this problem too",Fixed,Closed,3/25/2015 14:46:00,3/30/2015 16:47:00,3/26/2015 7:08:00,9,5,4.00
Regression in 2.0.19,https://issues.apache.org/jira/browse/PDFBOX-4805,Fixed,Major,"Joel Hirsh reported a regression with PDFTextStripper which was introduced with 2.0.19, see his post on users@ for details.

He can't share the pdf in questions due to privacy but did some debugging and found out that PDFBOX-4760 is the case for that regression. I accidentally committed some unrelated code which leads to bad text extraction results. As the code targets some corner cases it didn't came up as an issue when running our pre release tests. The issue is limited to the 2.0 trunk.",Fixed,Closed,3/31/2020 10:15:00,3/31/2020 10:20:00,3/31/2020 10:19:00,1,0,0.00
Invalidated signature signing pdf twice,https://issues.apache.org/jira/browse/PDFBOX-4261,Fixed,Major,"A customer sent us a pdf that has this problem: when it is signed twice by pdfbox 1.8.x the second signature invalidates the first one.

If we apply the same procedure using pdfbox 2.0.x the problem doesn't occur, but the customer required java 1.5 so we can't switch to the new version in this case.

For privacy purposes we had anonymized the original PDF file by editing 3 stream inside the pdf, without altering the original structure. So the file ""92752146_noSign_anonymous.pdf"" you can find in attachement has not the original text/image streams, but reproduces the problem as the original one.

Thank you in advance",Fixed,Closed,7/5/2018 11:11:00,7/11/2018 17:51:00,7/6/2018 15:48:00,10,6,5.00
PHP feature is not activated after Oracle JS Parser Implementation is installed,https://issues.apache.org/jira/browse/NETBEANS-2847,Fixed,Major,"While re-opening a project that had been imported from previous version and was marked ""Broken"". It re-opened and was no longer marked ""broken"", but then this exception occurred.

",Fixed,Closed,7/14/2019 12:08:00,4/20/2021 14:45:00,7/14/2019 12:17:00,13,646,646.00
Subversion demands unnecessary access to parent directories of operations,https://issues.apache.org/jira/browse/SVN-3242,Fixed,Critical,"We have updated to the latest 1.5 version of svn. Now we have a problem with
some operations. E.g.
C:\work>svn -m version cp https://someserver.com/svn/ProjectName/2008/trunk
-r56318 https://someserver.com/svn/ProjectName/2008/tags/8.11.07
svn: Server sent unexpected return value (403 Forbidden) in response to PROPFIND
request for '/svn'

C:\work>svn --version
svn, version 1.5.0 (r31699)
  compiled Jun 23 2008, 12:59:48

but the same command line works fine with svn 1.4.

Our svn-server is configured to give no access to root folder
(https://someserver.com/svn/) but gives rw access to project folders
(https://someserver.com/svn/ProjectName/), and seems svn1.5 want to do
something with root even it I work only with project folders.
Moreover this command is not only one which gives that problem, in some
circumstances (not sure which, but I guess if new files were added) it
gives the same error even for ""svn up"" command.
Original issue reported by kan",Fixed,Closed,7/17/2008 14:32:00,9/22/2010 14:40:00,10/16/2008 22:28:00,82,797,706.00
(ios) Present notification view controller by inappbrowser view controller,https://issues.apache.org/jira/browse/CB-13555,Fixed,Major,"When inappbrowser window is shown, if main uiwebview or wkwebview calls cordova Dialog plugin method to show the dialog view, the dialog should show to user on top of the inappbrowser view controller.

However, currently the dialog view is shown behind the inappbrowser view, so user cannot see it or click button on the dialog

An similar issue was reported for barcode scanner plugin at
https://github.com/phonegap/phonegap-plugin-barcodescanner/issues/570

The issue can be repeated with the below method
function confirm(){
var win = window.open( ""https://www.google.com"", ""_blank"" );
win.addEventListener( ""loadstop"", function() {
setTimeout(function() {
function onConfirm(buttonIndex)

{ console.log('You selected button ' + buttonIndex); }
navigator.notification.confirm(
'You are the winner!', // message
onConfirm, // callback to invoke with index of button pressed
'Game Over', // title
['Restart','Exit'] // buttonLabels
);
}, 1000 );
});
}",Fixed,Closed,11/8/2017 19:31:00,11/15/2017 22:19:00,11/8/2017 19:37:00,9,7,7.00
Possible wrong calculation of header length,https://issues.apache.org/jira/browse/MIME4J-265,Fixed,Major,"I've implemented a sort of mail server and I have many threads listening for incoming emails.
I'm using mime4j to parse javamail Message.

I had only one case of:
Caused by: org.apache.james.mime4j.io.MaxHeaderLengthLimitException: Maximum header length limit exceeded
at org.apache.james.mime4j.stream.DefaultFieldBuilder.append(DefaultFieldBuilder.java:63)
at org.apache.james.mime4j.stream.MimeEntity.readRawField(MimeEntity.java:212)
at org.apache.james.mime4j.stream.MimeEntity.nextField(MimeEntity.java:258)

Looking at the code of DefaultFieldBuilder, it seems that the check over line length is not done on the single line but on the overall header, I'm refering to this line:

if (this.maxlen > 0 && this.buf.length() + len >= this.maxlen) {
Why should you add ""this.buf.length"" ?
I know that there is no limit on header length, but only in its lines.

I can't attach my eml for privacy reasons but I can confirm that I have no too much long line

Thanks",Fixed,Closed,1/5/2018 10:15:00,5/2/2024 9:51:00,1/5/2018 10:17:00,12,2309,"2,309.00"
Issue with SQL Server Database with JUDDI 3.3.6 : The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000),https://issues.apache.org/jira/browse/JUDDI-999,Fixed,Major,"We were using SQLServer with JUDDI 3.0.4. It is working fine so far.

Now, we are trying to move to JUDDI version 3.3.6. We are encountering following issue on start-up.

Caused by: <openjpa-2.3.0-r422266:1540826 nonfatal general error> org.apache.openjpa.persistence.PersistenceException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002]Caused by: <openjpa-2.3.0-r422266:1540826 nonfatal general error> org.apache.openjpa.persistence.PersistenceException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002] at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:559) at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:455) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.synchronizeMappings(JDBCBrokerFactory.java:160) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.synchronizeMappings(JDBCBrokerFactory.java:164) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.newBrokerImpl(JDBCBrokerFactory.java:122) at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:209) at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:155) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:226) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:153) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:59) at org.apache.juddi.config.PersistenceManager.getEntityManager(PersistenceManager.java:48) at org.apache.juddi.config.AppConfig.getPersistentConfiguration(AppConfig.java:174) at org.apache.juddi.config.AppConfig.loadConfiguration(AppConfig.java:160) at org.apache.juddi.config.AppConfig.<init>(AppConfig.java:82) at org.apache.juddi.config.AppConfig.getInstance(AppConfig.java:272) at org.apache.juddi.config.AppConfig.getConfiguration(AppConfig.java:298) at org.apache.juddi.api.impl.AuthenticatedService.<init>(AuthenticatedService.java:75) at org.apache.juddi.api.impl.UDDIInquiryImpl.<init>(UDDIInquiryImpl.java:88) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Unknown Source) at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:142) ... 36 moreCaused by: org.apache.openjpa.lib.jdbc.ReportingSQLException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002] at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:219) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:203) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.access$700(LoggingConnectionDecorator.java:59) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingStatement.executeUpdate(LoggingConnectionDecorator.java:914) at org.apache.openjpa.lib.jdbc.DelegatingStatement.executeUpdate(DelegatingStatement.java:118) at org.apache.openjpa.jdbc.schema.SchemaTool.executeSQL(SchemaTool.java:1231) at org.apache.openjpa.jdbc.schema.SchemaTool.createTable(SchemaTool.java:976) at org.apache.openjpa.jdbc.schema.SchemaTool.add(SchemaTool.java:552) at org.apache.openjpa.jdbc.schema.SchemaTool.add(SchemaTool.java:364) at org.apache.openjpa.jdbc.schema.SchemaTool.run(SchemaTool.java:341) at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:505) ... 58 more
 

The error is ""The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000)""

Eventually the table ""j3_tmodel_instance_info"" failed to create. We are using SQLServer version 12.0.5207.0. It poses a limit on varchar fields to 8000.

We have tried modifying the column length in class ""TmodelInstanceInfo"" and redeploying the app, however then it starts giving other issue.

The type ""class org.apache.juddi.model.TmodelInstanceInfo"" has not been enhanced.
 

Could anyone please help us. We are in RED flag and our application cease to work after update to JUDDI 3.3.6

Any help be greatly appreciated. Kindly let me know if I need to provide more information to assist investigation.

Thanks a lot",Fixed,Closed,12/12/2019 12:35:00,1/11/2020 23:12:00,12/12/2019 13:56:00,39,30,30.00
NullPointerException in CmapSubtable.getCharCode,https://issues.apache.org/jira/browse/PDFBOX-5465,Fixed,Major,"Hi,

I got a NPE in the getCharCode method of CmapSubtable :

java.lang.NullPointerException: null
 at org.apache.fontbox.ttf.CmapSubtable.getCharCode(CmapSubtable.java:669) ~[fontbox-2.0.25.jar!/:2.0.25]
 at org.apache.fontbox.ttf.CmapSubtable.getCharCodes(CmapSubtable.java:686) ~[fontbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.pdmodel.font.PDType0Font.toUnicode(PDType0Font.java:528) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.PDFStreamEngine.showGlyph(PDFStreamEngine.java:811) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.PDFStreamEngine.showText(PDFStreamEngine.java:749) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.PDFStreamEngine.showTextString(PDFStreamEngine.java:608) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.operator.text.ShowText.process(ShowText.java:56) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.PDFStreamEngine.processOperator(PDFStreamEngine.java:939) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.PDFStreamEngine.processStreamOperators(PDFStreamEngine.java:514) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:492) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.contentstream.PDFStreamEngine.processPage(PDFStreamEngine.java:155) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.text.LegacyPDFStreamEngine.processPage(LegacyPDFStreamEngine.java:155) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.text.PDFTextStripper.processPage(PDFTextStripper.java:363) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.text.PDFTextStripper.processPages(PDFTextStripper.java:291) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.text.PDFTextStripper.writeText(PDFTextStripper.java:238) ~[pdfbox-2.0.25.jar!/:2.0.25]
 at org.apache.pdfbox.text.PDFTextStripper.getText(PDFTextStripper.java:202) ~[pdfbox-2.0.25.jar!/:2.0.25]
 

-> It seems, in some cases the glyphIdToCharacterCode array is not instantiated.

Sorry, but for privacy reason I can't share the PDF which cause this issue.",Fixed,Closed,6/27/2022 12:07:00,8/31/2022 3:05:00,6/27/2022 15:22:00,4,65,65.00
TestReloadableDefinitionsFactory fails when the project is in a path with spaces in its name,https://issues.apache.org/jira/browse/TILES-33,Fixed,Minor,"TestReloadableDefinitionsFactory fails if it is run through Maven under Windows 2000 when the project is in a path with spaces in its name
The same test run from Eclipse 3.2 does not show the problem.

Here is the report from Maven (asterisks are there to protect privacy)
<snip>
-------------------------------------------------------
T E S T S
-------------------------------------------------------
[surefire] Running org.apache.tiles.TestReloadableDefinitionsFactory
[surefire] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0,031 sec
[surefire]
[surefire] testReloadableDefinitionsFactory(org.apache.tiles.TestReloadableDefinitionsFactory) Time elapsed: 0,016 sec <<< FAILURE!
junit.framework.AssertionFailedError: Error running test: java.net.URISyntaxException: Illegal character in path at index 18: file:/C:/Documents and Settings/***/tiles/tiles-core/target/test-classes/org/apache/tiles/config/temp-defs.xml
at junit.framework.Assert.fail(Assert.java:47)
at org.apache.tiles.TestReloadableDefinitionsFactory.testReloadableDefinitionsFactory(TestReloadableDefinitionsFactory.java:148)
...
</snip>

The reason seem to be that sun.misc.Launcher$AppClassLoader.getResource (used in Eclipse) replaces spaces with '%20', while URLClassLoader.getResource (used by Maven) does not replace them.",Fixed,Closed,8/2/2006 13:06:00,9/7/2006 11:38:00,9/7/2006 11:38:00,1,36,0.00
LDAP injection vulnerability in LDAPAuthenticationSchemeImpl,https://issues.apache.org/jira/browse/DERBY-7147,Fixed,Major,"An LDAP injection vulnerability has been identified in LDAPAuthenticationSchemeImpl.getDNFromUID(). An exploit has not been provided, but there is a possibility that an intruder could bypass authentication checks in Derby-powered applications which rely on external LDAP servers.

For more information on LDAP injection, see https://www.synopsys.com/glossary/what-is-ldap-injection.html",Fixed,Closed,11/7/2022 13:15:00,12/12/2022 18:54:00,11/7/2022 13:56:00,59,35,35.00
Unable to decrypt PDF with String and Stream filter to identity,https://issues.apache.org/jira/browse/PDFBOX-4517,Fixed,Major,"I receive a PDF that contains the following Encryption Dictionnary:

32 0 obj
<</O (Ûýµ\fÁÒIÆ&Îé ^°Â>5N,\\qè¬#O‰2³Ã\b¼5¶ºj;šP)/EFF/StdCF/P -1852/R 5/OE (Q0Òzªè^À¸ÆÏÝéðnP}‚»Ÿ]ã.y„úŸcúÑ^)/U (ÐT7ŽHib³Ç\t|´_¶ºU¢ŸäNbíà>Ð@¼ ½ðï£Xš‘ú-Uz¯L<0ã)/EncryptMetadata false/V 5/Length 256/CF<</StdCF<</AuthEvent/EFOpen/Length 32/CFM/AESV3>>>>/StmF/Identity/Filter/Standard/StrF/Identity/Perms (¶;´—‹€]m¶Ç„ø)/UE (ºî«Õâk$ô‹f‘Î0¥®e""ªÄ]¬9ÎNï‘1öÓ)>>
endobj
and I was unable to open it with PDF Box.

Unfortunately, I can't share this PDF with you due to customer privacy and I was unable to find a tool that allow to create such a PDF.

This kind of encryption is useless I think, but it's probably intersting to support it anyway. Browsers and Adobe Reader have no problem to open it.",Fixed,Closed,4/17/2019 9:18:00,5/5/2019 17:41:00,4/17/2019 20:27:00,10,18,18.00
Error when starting Apache Unomi when offline,https://issues.apache.org/jira/browse/UNOMI-75,Fixed,Major,"Happened when I started unomi while being offline (wifi stopped)

2017-01-27 15:16:50,399 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/parameterized-types to bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT
2017-01-27 15:16:50,400 | ERROR | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Unable to start blueprint container for bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT
org.xml.sax.SAXParseException: src-resolve: Cannot resolve the name 'ptp:ParameterizedInt' to a 'simpleType definition' component.
at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)[:]
at org.apache.xerces.util.ErrorHandlerWrapper.error(Unknown Source)[:]
at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDAbstractTraverser.reportSchemaError(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDAttributeTraverser.traverseNamedAttr(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDAttributeTraverser.traverseLocal(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDAbstractTraverser.traverseAttrsAndAttrGrps(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.processComplexContent(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.traverseComplexTypeDecl(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.traverseLocal(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseNamedElement(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseGlobal(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDHandler.traverseSchemas(Unknown Source)[:]
at org.apache.xerces.impl.xs.traversers.XSDHandler.parseSchema(Unknown Source)[:]
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadSchema(Unknown Source)[:]
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:]
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:]
at org.apache.xerces.jaxp.validation.XMLSchemaFactory.newSchema(Unknown Source)[:]
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.createSchema(NamespaceHandlerRegistryImpl.java:637)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.doGetSchema(NamespaceHandlerRegistryImpl.java:458)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.getSchema(NamespaceHandlerRegistryImpl.java:443)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:343)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1]
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:]
at java.lang.Thread.run(Thread.java:745)[:1.8.0_111]
2017-01-27 15:16:50,439 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/beans to bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT
2017-01-27 15:16:50,440 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/parameterized-types to bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT
After some more research this is really a problem with Aries Blueprint and Apache CXF, here are the relevant tickets :

https://issues.apache.org/jira/browse/ARIES-1540

and

https://issues.apache.org/jira/browse/CXF-7183

We could use the first fix for the current release but we will need to somehow upgrade CXF again when we switch to Karaf 4.",Fixed,Closed,2/1/2017 11:57:00,2/1/2017 12:27:00,2/1/2017 12:27:00,2,0,0.00
base class warning in SAXException.hpp copy constructor,https://issues.apache.org/jira/browse/XERCESC-1162,Fixed,Major,"I have been getting multiple copies of this warning:

include/xercesc/sax/SAXException.hpp: In copy constructor
`xercesc_2_5::SAXException::SAXException(const xercesc_2_5::SAXException&)':
include/xercesc/sax/SAXException.hpp:195: warning: base class `class
xercesc_2_5::XMemory' should be explicitly initialized in the copy constructor

I have to fix the code in SAXException.hpp by changing this:

SAXException(const SAXException& toCopy) :

fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager))
, fMemoryManager(toCopy.fMemoryManager)
{
}

into this:

SAXException(const SAXException& toCopy) : XMemory(),

fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager))
, fMemoryManager(toCopy.fMemoryManager)
{
}

once XMemory() is declared as a base class, all warnings are gone. I've seen
this in 2.5.0 as well. Below are the compiler flags that I have set which
should help you recreate this bug:

-g3 -I. -I./include -isystem ./libs/crystalize/Linux/include -I. -
I./include -isystem ./libs/crystalize/Linux/include -D_linux_ -D_x86_ -
DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT -I./libs/Database -
I./libs/Database/libs/xerces/Linux/include isystem ./libs/sybase/sybase
12.5.1/Linux/include I./libs/xerces/Linux/include -Wall -W -pedantic -Wno
long-long Wpointer-arith -Wcast-qual -Wwrite-strings -Wconversion -Wstrict
prototypes -Wmissing-prototypes -Woverloaded-virtual -Wchar-subscripts -
Wparentheses Wpointer-arith -Winline -Wnon-virtual-dtor -Wreorder -fno-gnu
keywords -Wctor-dtor-privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar -
Wno-float-equal Wdisabled-optimization -Wswitch -Wpointer-arith -pipe -Wno
cast-qual Wno-unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage
length=0 -DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64 -
D_linux_ -D_x86_ -DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT -
I./libs/utilities/include -I./libs/AST_Common -I./libs/AST_Common/AST/enums -
I./libs/Database -I./libs/Database/libs/boost/Linux -
I./libs/Database/libs/omni/Linux/include I./libs/Database/libs/sybase/sybase
12.5.1/Linux/include -I./libs/Database/libs/xerces/Linux/include -
isystem ./libs/sybase/sybase-12.5.1/Linux/include -
I./libs/xerces/Linux/include -Wall -W -pedantic -Wno-long-long -Wpointer-arith -
Wcast-qual Wwrite-strings -Wconversion -Wstrict-prototypes -Wmissing
prototypes Woverloaded-virtual -Wchar-subscripts -Wparentheses -Wpointer
arith Winline -Wnon-virtual-dtor -Wreorder -fno-gnu-keywords -Wctor-dtor
privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar -Wno-float-equal -
Wdisabled-optimization Wswitch -Wpointer-arith -pipe -Wno-cast-qual -Wno
unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage-length=0 -
DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64

Let me know if you need more information. Thank you.

-Vrajesh",Fixed,Closed,2/25/2004 2:35:00,1/8/2005 22:24:00,7/27/2004 19:42:00,2,318,165.00
svnmerge.py migration tool(s) do not guarantee proper svn:mergeinfo range ordering,https://issues.apache.org/jira/browse/SVN-3302,Fixed,Critical,"Per http://svn.haxx.se/dev/archive-2008-10/0280.shtml,
svnmerge-migrate-history.py (and probably svnmerge-migrate-history-remotely.py,
too) do not ensure that the svn:mergeinfo properties they create have their
rangelists sorted properly, which can result in the versioning of bogus property
value that Subversion can't use.",Fixed,Closed,10/21/2008 17:12:00,3/10/2009 15:22:00,10/21/2008 17:13:00,14,140,140.00
byte[].encodeBase64() incorrectly introduces line breaks,https://issues.apache.org/jira/browse/GROOVY-2878,Fixed,Major,"Groovy's encodeBase64 inserts 0x0A chars (LF) into long strings to break lines. This is contrary to my reading of RFC4648:

{codec}
3.1. Line Feeds in Encoded Data

MIME [4] is often used as a reference for base 64 encoding. However,
MIME does not define ""base 64"" per se, but rather a ""base 64 Content-
Transfer-Encoding"" for use within MIME. As such, MIME enforces a
limit on line length of base 64-encoded data to 76 characters. MIME
inherits the encoding from Privacy Enhanced Mail (PEM) [3], stating
that it is ""virtually identical""; however, PEM uses a line length of
64 characters. The MIME and PEM limits are both due to limits within
SMTP.

Implementations MUST NOT add line feeds to base-encoded data unless
the specification referring to this document explicitly directs base
encoders to add line feeds after a specific number of characters.{codec}
This has resulted in incorrect behaviour in Grails also. However the author notes that some groovy applications may rely on this functionality currently, so this could be a breaking change for some.

It would be better to be correct IMO. There is no clue in groovy docs that this introduces line breaks in this MIME/SMTP specific way.",Fixed,Closed,6/2/2008 6:45:00,6/18/2009 8:19:00,6/2/2008 7:55:00,5,381,381.00
Cleanup suspect coding practices in the org.apache.derby.impl.tools.ij package.,https://issues.apache.org/jira/browse/DERBY-6195,Fixed,Minor,Similar to DERBY-6177.,Fixed,Closed,4/22/2013 15:36:00,4/23/2013 19:23:00,4/22/2013 19:13:00,7,1,1.00
ArrayIndexOutOfBoundsException: 9 parsing RTF,https://issues.apache.org/jira/browse/TIKA-1192,Fixed,Major,"When trying to parse an RTF file I'm getting the following exception. I am not able to attach the file for privacy reasons:

java.lang.ArrayIndexOutOfBoundsException: 9
                           TextExtractor.java:872 org.apache.tika.parser.rtf.TextExtractor.processControlWord
                           TextExtractor.java:566 org.apache.tika.parser.rtf.TextExtractor.parseControlWord
                           TextExtractor.java:492 org.apache.tika.parser.rtf.TextExtractor.parseControlToken
                           TextExtractor.java:459 org.apache.tika.parser.rtf.TextExtractor.extract
                           TextExtractor.java:448 org.apache.tika.parser.rtf.TextExtractor.extract
                                RTFParser.java:56 org.apache.tika.parser.rtf.RTFParser.parse
                                 (Unknown Source) sun.reflect.NativeMethodAccessorImpl.invoke0
                 NativeMethodAccessorImpl.java:57 sun.reflect.NativeMethodAccessorImpl.invoke
             DelegatingMethodAccessorImpl.java:43 sun.reflect.DelegatingMethodAccessorImpl.invoke
                                  Method.java:606 java.lang.reflect.Method.invoke
                                Reflector.java:93 clojure.lang.Reflector.invokeMatchingMethod
                                Reflector.java:28 clojure.lang.Reflector.invokeInstanceMethod
                               tika_parser.clj:20 rtf-parser.tika-parser/parse
               form-init2921349737948661927.clj:1 rtf-parser.tika-parser/eval4200
                               Compiler.java:6619 clojure.lang.Compiler.eval
                               Compiler.java:6582 clojure.lang.Compiler.eval
                                    core.clj:2852 clojure.core/eval
                                     main.clj:259 clojure.main/repl[fn]
                                     main.clj:259 clojure.main/repl[fn]
                                     main.clj:277 clojure.main/repl[fn]
                                     main.clj:277 clojure.main/repl
                                 RestFn.java:1096 clojure.lang.RestFn.invoke
                        interruptible_eval.clj:56 clojure.tools.nrepl.middleware.interruptible-eval/evaluate[fn]
                                     AFn.java:159 clojure.lang.AFn.applyToHelper
                                     AFn.java:151 clojure.lang.AFn.applyTo
                                     core.clj:617 clojure.core/apply
                                    core.clj:1788 clojure.core/with-bindings*
                                  RestFn.java:425 clojure.lang.RestFn.invoke
                        interruptible_eval.clj:41 clojure.tools.nrepl.middleware.interruptible-eval/evaluate
                       interruptible_eval.clj:171 clojure.tools.nrepl.middleware.interruptible-eval/interruptible-eval[fn]
                                    core.clj:2330 clojure.core/comp[fn]
                       interruptible_eval.clj:138 clojure.tools.nrepl.middleware.interruptible-eval/run-next[fn]
                                      AFn.java:24 clojure.lang.AFn.run
                     ThreadPoolExecutor.java:1145 java.util.concurrent.ThreadPoolExecutor.runWorker
                      ThreadPoolExecutor.java:615 java.util.concurrent.ThreadPoolExecutor$Worker.run
                                  Thread.java:724 java.lang.Thread.run",Fixed,Closed,11/6/2013 21:31:00,11/9/2013 11:29:00,11/8/2013 1:43:00,9,3,1.00
strange Digester parsing error,https://issues.apache.org/jira/browse/DIGESTER-27,Fixed,Major,"While testing our application we ran into a strange Digester parse issue.
It looks like the Digester sometimes forgets to parse a value in the xml. Here
the situation:

1. If we fire testscript A that doesnot comply to the schema we set on the
Digester then we get a parsing error as expected. The error is that field Z in
the xml was not valid.
2. We fire testscript B which should return an answer. The first time we fire
it the Digester doesnot map field Z (which now has a valid value) to the java
class as defined in the rule file.
3. We fire testscript B again unchanged and now field Z is mapped by the
Digester to the correct attribute in the corresponding java class.

If at point 1 we dont fire testscript A (with the invalid value for attribute Z)
but say C or any other this doesnot occur and we get the reply we expect......

It seems like that after a call which results in a SAXException due to an
invalid value in the XML according to the attached schema the next call fails
to parse the xml correctly to the java object defined in the rule file. The
third call however (which is exactly the same as the second) succeeds.

Any idea's?

Regards,
Lars Vonk",Fixed,Closed,6/28/2004 17:27:00,3/9/2007 20:39:00,6/29/2004 8:24:00,5,984,983.00
Wrong XAException return code when broker timeout is hit,https://issues.apache.org/jira/browse/ARTEMIS-591,Fixed,Major,"By creating testcases for checking behavior of transaction timeout I've hit an issue of wrong error code being returned when broker transaction timeout is hit before TM transaction timeout expires.
It uses XAER_PROTO instead of RBTIMEOUT.

This issue does not cause data inconsistency.

Scenario:

ejb sends a message to a queue
processing inside of the ejb takes long time
TM transaction timeout is set big enough to not hit the timeout
jms broker internal transaction timeout is smaller than time needed for processing ejb method
jms broker txn timeout occurs - broker local txn is rolled back
txn is removed from list of broker's local in-process transactions
TM calls XAResource.end
the call returns XAException.XAER_PROTO
That's current implementation returns XAER_PROTO in this scenario but RBTIMEOUT would be more appropriate.

From discussion with Narayana developers, RM should return the most specific error return code as possible. In this scenario it's RBTIMEOUT.

Other notes from TM dev point of view:

""[XA_RBTIMEOUT]
The work represented by this transaction branch took too long.""
per XA spec page 39.

The more complex question is, at what point can the resource manager forget about that branch (and therefore return NOTA to subsequent calls)?

The XA spec says ""After the transaction manager calls xa_end(), it should no longer consider the calling thread associated with that resource manager (although it must consider the resource manager part of the transaction branch when it prepares the branch.)""
which implies the branch is still considered live at that point, a view corroborated by:

""[XA_RB∗]
The resource manager has dissociated the transaction branch from the thread of control and has marked rollback-only the work performed on behalf of ∗xid.""
Exception being thrown

WARN  [com.arjuna.ats.jta] (Thread-0
(ActiveMQ-client-global-threads-1468293951)) ARJUNA016056:
TransactionImple.delistResource - caught exception during delist :
XAException.XAER_PROTO: javax.transaction.xa.XAException
 at
org.apache.activemq.artemis.core.protocol.core.impl.ActiveMQSessionContext.xaEnd(ActiveMQSessionContext.java:346)
 at
org.apache.activemq.artemis.core.client.impl.ClientSessionImpl.end(ClientSessionImpl.java:1115)
 at
org.apache.activemq.artemis.ra.ActiveMQRAXAResource.end(ActiveMQRAXAResource.java:112)
 at
org.apache.activemq.artemis.service.extensions.xa.ActiveMQXAResourceWrapperImpl.end(ActiveMQXAResourceWrapperImpl.java:81)
 at
com.arjuna.ats.internal.jta.transaction.arjunacore.TransactionImple.delistResource(TransactionImple.java:897)
 at
org.jboss.jca.core.connectionmanager.listener.TxConnectionListener$TransactionSynchronization.beforeCompletion(TxConnectionListener.java:1063)
 at
org.jboss.jca.core.connectionmanager.transaction.TransactionSynchronizer.invokeBefore(TransactionSynchronizer.java:438)
 at
org.jboss.jca.core.connectionmanager.transaction.TransactionSynchronizer.beforeCompletion(TransactionSynchronizer.java:376)
 at
org.jboss.as.txn.service.internal.tsr.JCAOrderedLastSynchronizationList.beforeCompletion(JCAOrderedLastSynchronizationList.java:130)
 at
com.arjuna.ats.internal.jta.resources.arjunacore.SynchronizationImple.beforeCompletion(SynchronizationImple.java:76)
 at
com.arjuna.ats.arjuna.coordinator.TwoPhaseCoordinator.beforeCompletion(TwoPhaseCoordinator.java:371)
 at
com.arjuna.ats.arjuna.coordinator.TwoPhaseCoordinator.end(TwoPhaseCoordinator.java:91)
 at com.arjuna.ats.arjuna.AtomicAction.commit(AtomicAction.java:162)
 at
com.arjuna.ats.internal.jta.transaction.arjunacore.TransactionImple.commitAndDisassociate(TransactionImple.java:1200)
 at
com.arjuna.ats.internal.jta.transaction.arjunacore.BaseTransaction.commit(BaseTransaction.java:126)
 at
com.arjuna.ats.jbossatx.BaseTransactionManagerDelegate.commit(BaseTransactionManagerDelegate.java:89)
 at
org.jboss.as.ejb3.inflow.MessageEndpointInvocationHandler.afterDelivery(MessageEndpointInvocationHandler.java:71)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at
org.jboss.as.ejb3.inflow.AbstractInvocationHandler.handle(AbstractInvocationHandler.java:60)
 at
org.jboss.as.ejb3.inflow.MessageEndpointInvocationHandler.doInvoke(MessageEndpointInvocationHandler.java:135)
 at
org.jboss.as.ejb3.inflow.AbstractInvocationHandler.invoke(AbstractInvocationHandler.java:73)
 at
org.jboss.as.test.jbossts.crashrec.jms.mdb.JMSCrashMessageDrivenBean$$$endpoint1.afterDelivery(Unknown
Source)
 at
org.apache.activemq.artemis.ra.inflow.ActiveMQMessageHandler.onMessage(ActiveMQMessageHandler.java:321)
 at
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl.callOnMessage(ClientConsumerImpl.java:932)
 at
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl.access$400(ClientConsumerImpl.java:47)
 at
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl$Runner.run(ClientConsumerImpl.java:1045)
 at
org.apache.activemq.artemis.utils.OrderedExecutorFactory$OrderedExecutor$ExecutorTask.run(OrderedExecutorFactory.java:100)
 at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)",Fixed,Closed,6/23/2016 7:03:00,11/17/2016 21:54:00,6/23/2016 7:09:00,20,147,147.00
CouchDB fails to bind to IPv6 address on Windows,https://issues.apache.org/jira/browse/COUCHDB-1032,Fixed,Critical,"I'm trying to bind to IPv6 address :: but couchDB didn't start.

I've tried with
bind_address = ::
and with the specific IPv6 address.

LOG level setted do debug ###########################
Erlang R14A (erts-5.8) [source] [smp:2:2] [rq:2] [async-threads:0]

Eshell V5.8 (abort with ^G)
1> Apache CouchDB 1.0.1 (LogLevel=debug) is starting.
Configuration Settings [""../etc/couchdb/default.ini"",
""../etc/couchdb/local.ini""]:
[admins] **LINE REMOVED**
[admins] **LINE REMOVED**
[attachments] compressible_types=""text/*, application/javascript, application/json, application/xml""
[attachments] compression_level=""8""
[couch_httpd_auth] auth_cache_size=""50""
[couch_httpd_auth] authentication_db=""_users""
[couch_httpd_auth] authentication_redirect=""/_utils/session.html""
[couch_httpd_auth] require_valid_user=""false""
[couch_httpd_auth] **LINE REMOVED**
[couch_httpd_auth] timeout=""600""
[couchdb] database_dir=""../var/lib/couchdb""
[couchdb] delayed_commits=""true""
[couchdb] max_attachment_chunk_size=""4294967296""
[couchdb] max_dbs_open=""100""
[couchdb] max_document_size=""4294967296""
[couchdb] os_process_timeout=""5000""
[couchdb] uri_file=""../var/lib/couchdb/couch.uri""
[couchdb] util_driver_dir=""../lib/couch-1.0.1/priv/lib""
[couchdb] view_index_dir=""../var/lib/couchdb""
[daemons] auth_cache=""

{couch_auth_cache, start_link, []}
""
[daemons] db_update_notifier=""

{couch_db_update_notifier_sup, start_link, []}
""
[daemons] external_manager=""

{couch_external_manager, start_link, []}
""
[daemons] httpd=""

{couch_httpd, start_link, []}
""
[daemons] query_servers=""

{couch_query_servers, start_link, []}
""
[daemons] stats_aggregator=""

{couch_stats_aggregator, start, []}
""
[daemons] stats_collector=""

{couch_stats_collector, start, []}
""
[daemons] uuids=""

{couch_uuids, start, []}
""
[daemons] view_manager=""

{couch_view, start_link, []}
""
[httpd] allow_jsonp=""false""
[httpd] authentication_handlers=""

{couch_httpd_oauth, oauth_authentication_handler}
,

{couch_httpd_auth, cookie_authentication_handler}
,

{couch_httpd_auth, default_authentication_handler}
""
[httpd] bind_address=""::""
[httpd] default_handler=""

{couch_httpd_db, handle_request}
""
[httpd] max_connections=""2048""
[httpd] port=""5984""
[httpd] secure_rewrites=""true""
[httpd] vhost_global_handlers=""_utils, _uuids, _session, _oauth, _users""
[httpd_db_handlers] _changes=""

{couch_httpd_db, handle_changes_req}
""
[httpd_db_handlers] _compact=""

{couch_httpd_db, handle_compact_req}
""
[httpd_db_handlers] _design=""

{couch_httpd_db, handle_design_req}
""
[httpd_db_handlers] _temp_view=""

{couch_httpd_view, handle_temp_view_req}
""
[httpd_db_handlers] _view_cleanup=""

{couch_httpd_db, handle_view_cleanup_req}
""
[httpd_design_handlers] _info=""

{couch_httpd_db, handle_design_info_req}
""
[httpd_design_handlers] _list=""

{couch_httpd_show, handle_view_list_req}
""
[httpd_design_handlers] _rewrite=""

{couch_httpd_rewrite, handle_rewrite_req}
""
[httpd_design_handlers] _show=""

{couch_httpd_show, handle_doc_show_req}
""
[httpd_design_handlers] _update=""

{couch_httpd_show, handle_doc_update_req}
""
[httpd_design_handlers] _view=""

{couch_httpd_view, handle_view_req}
""
[httpd_global_handlers] /=""

{couch_httpd_misc_handlers, handle_welcome_req, <<\""Welcome\"">>}
""
[httpd_global_handlers] _active_tasks=""

{couch_httpd_misc_handlers, handle_task_status_req}
""
[httpd_global_handlers] _all_dbs=""

{couch_httpd_misc_handlers, handle_all_dbs_req}
""
[httpd_global_handlers] _config=""

{couch_httpd_misc_handlers, handle_config_req}
""
[httpd_global_handlers] _log=""

{couch_httpd_misc_handlers, handle_log_req}
""
[httpd_global_handlers] _oauth=""

{couch_httpd_oauth, handle_oauth_req}
""
[httpd_global_handlers] _replicate=""

{couch_httpd_misc_handlers, handle_replicate_req}
""
[httpd_global_handlers] _restart=""

{couch_httpd_misc_handlers, handle_restart_req}
""
[httpd_global_handlers] _session=""

{couch_httpd_auth, handle_session_req}
""
[httpd_global_handlers] _stats=""

{couch_httpd_stats_handlers, handle_stats_req}
""
[httpd_global_handlers] _utils=""

{couch_httpd_misc_handlers, handle_utils_dir_req, \""../share/couchdb/www\""}
""
[httpd_global_handlers] _uuids=""

{couch_httpd_misc_handlers, handle_uuids_req}
""
[httpd_global_handlers] favicon.ico=""

{couch_httpd_misc_handlers, handle_favicon_req, \""../share/couchdb/www\""}
""
[log] file=""../var/log/couchdb/couch.log""
[log] include_sasl=""true""
[log] level=""debug""
[query_server_config] reduce_limit=""true""
[query_servers] javascript=""./couchjs.exe ../share/couchdb/server/main.js""
[replicator] max_http_pipeline_size=""10""
[replicator] max_http_sessions=""10""
[stats] rate=""1000""
[stats] samples=""[0, 60, 300, 900]""
[uuids] algorithm=""sequential""
Failure to start Mochiweb: eafnosupport
[error] [<0.106.0>] {error_report,<0.34.0>,
{<0.106.0>,crash_report,
[[{initial_call,{mochiweb_socket_server,init,['Argument__1']}},
{pid,<0.106.0>}
,
{registered_name,[]}
,
{error_info,{exit,eafnosupport,
[

{gen_server,init_it,6}
,
{proc_lib,init_p_do_apply,3}
]}},
{ancestors,[couch_secondary_services,couch_server_sup, <0.35.0>]}
,
{messages,[]}
,
{links,[<0.89.0>]}
,
{dictionary,[]}
,
{trap_exit,true}
,
{status,running}
,
{heap_size,1597}
,
{stack_size,24}
,
{reductions,352}
],
[]]}}

=CRASH REPORT==== 20-Jan-2011::09:39:55 ===
crasher:
initial call: mochiweb_socket_server:init/1
pid: <0.106.0>
registered_name: []
exception exit: eafnosupport
in function gen_server:init_it/6
ancestors: [couch_secondary_services,couch_server_sup,<0.35.0>]
messages: []
links: [<0.89.0>]
dictionary: []
trap_exit: true
status: running
heap_size: 1597
stack_size: 24
reductions: 352
neighbours:
[error] [<0.89.0>] {error_report,<0.34.0>,
{<0.89.0>,supervisor_report,
[{supervisor,{local,couch_secondary_services}},
{errorContext,start_error},
{reason,eafnosupport},
{offender,[{pid,undefined},
{name,httpd},
{mfargs,{couch_httpd,start_link,[]}},
{restart_type,permanent},
{shutdown,1000},
{child_type,worker}]}]}}

=SUPERVISOR REPORT==== 20-Jan-2011::09:39:55 ===
Supervisor: {local,couch_secondary_services}
Context: start_error
Reason: eafnosupport
Offender: [{pid,undefined},
{name,httpd},
{mfargs,{couch_httpd,start_link,[]}},
{restart_type,permanent},
{shutdown,1000},
{child_type,worker}]

[error] [<0.81.0>] {error_report,<0.34.0>,
{<0.81.0>,supervisor_report,
[{supervisor,{local,couch_server_sup}},
{errorContext,start_error}
,
{reason,shutdown}
,
{offender,
[

{pid,undefined},
{name,couch_secondary_services},
{mfargs,{couch_server_sup,start_secondary_services,[]}},
{restart_type,permanent},
{shutdown,infinity},
{child_type,supervisor}]}]}}

=SUPERVISOR REPORT==== 20-Jan-2011::09:39:55 ===
Supervisor: {local,couch_server_sup}
Context: start_error
Reason: shutdown
Offender: [{pid,undefined}
,
{name,couch_secondary_services}
,
{mfargs,{couch_server_sup,start_secondary_services,[]}},
{restart_type,permanent}
,
{shutdown,infinity}
,
{child_type,supervisor}
]

=CRASH REPORT==== 20-Jan-2011::09:39:55 ===
crasher:
initial call: application_master:init/4
pid: <0.34.0>
registered_name: []
exception exit: {bad_return,
{{couch_app,start,
[normal,
[""../etc/couchdb/default.ini"",
""../etc/couchdb/local.ini""]]},
{'EXIT',
badmatch,{error,shutdown,
[

{couch_server_sup,start_server,1},
{application_master,start_it_old,4}]}}}}
in function application_master:init/4
ancestors: [<0.33.0>]
messages: [{'EXIT',<0.35.0>,normal}]
links: [<0.33.0>,<0.6.0>]
dictionary: []
trap_exit: true
status: running
heap_size: 610
stack_size: 24
reductions: 422
neighbours:

=INFO REPORT==== 20-Jan-2011::09:39:55 ===
application: couch
exited: {bad_return,{{couch_app,start,
[normal,
[""../etc/couchdb/default.ini"",
""../etc/couchdb/local.ini""]]},
{'EXIT',badmatch,{error,shutdown,
[{couch_server_sup,start_server,1}
,
{application_master,start_it_old,4}
]}}}}
type: temporary
1>",Fixed,Closed,1/20/2011 8:57:00,2/10/2011 11:24:00,2/10/2011 10:35:00,3,21,0.00
Access Denied Exceptions fill up the logs with tracebacks that give no additional information,https://issues.apache.org/jira/browse/SLING-1727,Fixed,Major,"All errors in AbstractSlingPostOperation are logged with full tracebacks, however AccessDeniedExceptions all happen on the Save operation and so the traceback just fills the log up without providing any extra information

The traceback should happen at debug level, with a one line message at info level.

currently the traceback is

03.09.2010 11:02:27.298 ERROR [0:0:0:0:0:0:0:1%0 [1283508147295] POST /test/authztest/node1283508146/childnode.html HTTP/1.1] org.apache.sling.servlets.post.impl.operations.ModifyOperation Exception during response processing. javax.jcr.AccessDeniedException: /test/authztest/node1283508146/childnode/user2-1283508146: not allowed to add or modify item
at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:411)
at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1097)
at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)
at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109)
at $Proxy11.save(Unknown Source)
at org.apache.sling.servlets.post.AbstractSlingPostOperation.run(AbstractSlingPostOperation.java:125)
at org.apache.sling.servlets.post.impl.SlingPostServlet.doPost(SlingPostServlet.java:242)
at org.apache.sling.api.servlets.SlingAllMethodsServlet.mayService(SlingAllMethodsServlet.java:148)
at org.apache.sling.api.servlets.SlingSafeMethodsServlet.service(SlingSafeMethodsServlet.java:344)
at org.apache.sling.api.servlets.SlingSafeMethodsServlet.service(SlingSafeMethodsServlet.java:375)
at org.apache.sling.engine.impl.request.RequestData.service(RequestData.java:523)
at org.apache.sling.engine.impl.SlingMainServlet.processRequest(SlingMainServlet.java:427)
at org.apache.sling.engine.impl.filter.RequestSlingFilterChain.render(RequestSlingFilterChain.java:48)
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:64)
at org.apache.sling.engine.impl.debug.RequestProgressTrackerLogFilter.doFilter(RequestProgressTrackerLogFilter.java:59)
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
at org.sakaiproject.nakamura.batch.RequestEventsFilter.doFilter(RequestEventsFilter.java:96)
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
at org.sakaiproject.nakamura.files.pool.ContentPoolFilter.doFilter(ContentPoolFilter.java:78)
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
at org.sakaiproject.nakamura.privacy.RestPrivacyFilter.doFilter(RestPrivacyFilter.java:81)
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
at org.sakaiproject.nakamura.persistence.TransactionManagerFilter.doFilter(TransactionManagerFilter.java:95)
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
at org.sakaiproject.nakamura.cluster.ClusterTrackingFilter.doFilter(ClusterTrackingFilter.java:87)
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
at org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:313)
at org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:207)
at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389)
at org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:64)
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
at org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:111)
at org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:64)
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
at org.mortbay.jetty.Server.handle(Server.java:324)
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)
at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:880)
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747)
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)",Fixed,Closed,9/3/2010 10:46:00,9/3/2010 10:54:00,9/3/2010 10:54:00,1,0,0.00
Unable to create profile with consent info,https://issues.apache.org/jira/browse/UNOMI-238,Fixed,Critical,"Hi,
I'm not able to create a profile including its consents properties.
If I try to add the ""consents"" property to the profile, Unomi returns a 500 status code. Looking at the docs, I've seen that the only way to add consents is by posting an event on a given session.

I currently have millions of profiles to be imported and adding their consent during profile creation would be awesome.",Fixed,Closed,7/10/2019 14:28:00,7/16/2019 15:49:00,7/10/2019 14:36:00,12,6,6.00
Build failure on FreeBSD with CMake,https://issues.apache.org/jira/browse/XERCESC-2109,Fixed,Major,"{{{
19:20:26 cd /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build/src && /usr/bin/CC -DHAVE_CONFIG_H=1 -DXERCES_BUILDING_LIBRARY=1 -D_FILE_OFFSET_BITS=64 -D_THREAD_SAFE=1 -Dxerces_c_EXPORTS -I/usr/local/include -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build/src -isystem /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/stage/include -Wall -Wcast-align -Wcast-qual -Wctor-dtor-privacy -Wextra -Wformat=2 -Wimplicit-atomic-properties -Wmissing-declarations -Wno-long-long -Woverlength-strings -Woverloaded-virtual -Wredundant-decls -Wreorder -Wswitch-default -Wunused-variable -Wwrite-strings -Wno-variadic-macros -fstrict-aliasing -msse2 -O3 -DNDEBUG -fPIC -pthread -std=gnu++14 -o CMakeFiles/xerces-c.dir/xercesc/util/Base64.cpp.o -c /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp
19:20:26 /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp:149:14: error: use of undeclared identifier 'XERCES_SIZE_MAX'
19:20:26 else if (XERCES_SIZE_MAX - inputLength < 2)

{ 19:20:26 ^ 19:20:26 1 error generated. }}
}",Fixed,Closed,8/5/2017 7:10:00,8/7/2017 8:58:00,8/6/2017 17:26:00,5,2,1.00
Directory Studio doesn't use the SASL confidentiality layer after negotiating its use,https://issues.apache.org/jira/browse/DIRSTUDIO-1220,Fixed,Major,"There is an issue connecting to an OpenLDAP server configured with olcSaslSecProps: noplain,noanonymous,minssf=1

i.e. The server requires some form of transport encryption. Having a different issue with StartTLS (DIRSTUDIO-1219), I tried relying on the SASL confidentiality layer that SASL's GSSAPI mechanism can provide, to meet the requirement for encryption. I have chosen ""No encryption"" i.e. no SSL or StartTLS, in the Network Parameters, and then GSSAPI authentication method and Quality of Protection: Authentication with integrity and privacy protection in the SASL settings.

When connecting to the server, what I can see happening when looking at the network traffic with Wireshark is:

Client obtains a Kerberos service ticket for the LDAP server and passes it in the bind request for SASL GSSAPI authentication
Server replies with a bind response, continuing SASL GSSAPI authentication, result code 14 (SASL bind in progress), with a 4 byte message wrapped using GSS_Wrap. The 4 bytes are 0x06 0x01 0x00 0x00 - referring to RFC4752, the first byte indicates the server supports ""Integrity protection"" and/or ""Confidentiality protection"" but not ""No security layer"", as expected.
Client replies with a bind request, continuing SASL GSSAPI authentication, with a 4 byte message wrapped using GSS_Wrap. The 4 bytes are 0x04 0x01 0x00 0x00 - again referring to RFC4752, the first byte indicates the client has selected ""Confidentiality protection"".
Server replies with a bind response with result code 0 (success).
Client sends a search request with base DN: """", scope: base, filter: (objectClass=), for attributes: subschemaSubentry, **with no confidentiality protection*. This is the point where the client violates the protocol described in RFC4752 - after negotiating confidentiality protection, the client needs to actually use it!
Server interprets the lack of confidentiality protection as an error and immediately drops the connection (this makes sense from the server's POV as it could indicate an attempted man-in-the-middle attack)
Client immediately re-connects to the server, *doesn't bother to bind at all* and then issues more search requests on the base object, cn=Subschema, etc.
An error message appears in Directory Studio ""Error while opening connection
 - Missing schema location in RootDSE, using default schema"" - this is presumably because the connection isn't bound, and the server limits what it will disclose to un-bound clients.

Directory Studio can't browse the directory at all because it's not properly bound.
As you can see, there's possibly two issues here - definitely an issue with the SASL GSSAPI mechanism, and possibly also an issue with the reconnect logic.",Fixed,Closed,4/5/2019 9:35:00,6/20/2021 10:35:00,4/5/2019 9:52:00,4,807,807.00
/ImageMask true does not work. Patch included.,https://issues.apache.org/jira/browse/PDFBOX-1445,Fixed,Major,"I have the following pdf...

10 0 obj
<<
/Type /Page
/MediaBox [ 0 0 612.0 792.0 ]
/Parent 3 0 R
/Resources << /XObject << /Obj4 4 0 R /Obj5 5 0 R /Obj6 6 0 R /Obj7 7 0 R >> /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ] >>
/Contents [ 8 0 R 9 0 R ]
>>
endobj

Which then draws 4 images. The first one is a ""base"" and then rest are image masks

9 0 obj
<< /Filter /FlateDecode /Length 121 >>
stream
q
612.00 0 0 792.00 0.00 0.00 cm
/Obj4 Do
Q
q
0.129 g
524.16 0 0 556.80 48.00 127.68 cm
/Obj5 Do
Q
q
0.302 g
220.80 0 0 398.40 48.00 286.08 cm
/Obj6 Do
Q
q
0.204 g
524.16 0 0 469.44 48.00 185.28 cm
/Obj7 Do
Q
endstream
endobj

4 0 obj
<< /Type /XObject /Subtype /Image /Width 1275 /Height 1650 /BitsPerComponent 8
/ColorSpace /DeviceGray /Filter [ /FlateDecode /DCTDecode ] /Length 50485 >>
stream
endstream
endobj

5 0 obj
<< /Type /XObject /Subtype /Image /Width 2184 /Height 2320 /BitsPerComponent 1
/ImageMask true /Filter /CCITTFaxDecode /DecodeParms << /K -1 /Columns 2184 >>
/Length 15580 >>
stream

etc ...

The current code simply treats the imagemask as an image. Since this is just a 1 bit image it has no Alpha channel it overwrites the existing image and we simply get the last image drawn.

In

org.apache.pdfbox.util.operator.pagedrawer.Invoke.java

method

public void process(PDFOperator operator, List<COSBase> arguments) throws IOException

after

if (awtImage == null)

{ LOG.warn(""getRGBImage returned NULL""); return;//TODO PKOCH }
If you add the following code it fixes the problem. I can not provide the sample doc due to privacy reasons.

/**

Spec 8.9.6.2
If ImageMask is true then the image is one bit. Black means draw the current colour and white means use the colour on the current image (ie Mask).
Convert the map to an image with an Alpha channel so we can lay it on top
*/
if(image.getImageMask())
{
Color currentColour = drawer.getGraphicsState().getStrokingColor().getJavaColor();
final int onColour = 0xff000000 | currentColour.getRGB();
BufferedImage bia = new BufferedImage(awtImage.getWidth(),awtImage.getHeight(),BufferedImage.TYPE_INT_ARGB);
for(int y=0;y<awtImage.getHeight();y++)
Unknown macro: { for(int x=0;x<awtImage.getWidth();x++) { bia.setRGB(x, y, (awtImage.getRGB(x, y) & 0x00ffffff) == 0xffffff ? 0x00ffffff : onColour); } }
awtImage = bia;
}",Fixed,Closed,11/13/2012 13:49:00,11/18/2012 14:20:00,11/13/2012 17:27:00,9,5,5.00
build-couchdb build fails in SpiderMonkey on Debian Lenny 32-bit,https://issues.apache.org/jira/browse/COUCHDB-938,Fixed,Major,"I've followed the steps on couch.io for building without dependency hell on Debian Lenny 32-bit. However, this is a linux-vserver which runs on a 64-bit Ubuntu 10.04 host system. Maybe the configure script is making the wrong assumptions about the word size?

(07:29:17) frank [mika]:~/git/build-couchdb# file /bin/bash
/bin/bash: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.8, stripped

git clone git://github.com/couchone/build-couchdb
cd build-couchdb
git submodule init
git submodule update
rake

...

--------- ##
Platform. ##
--------- ##
hostname = frank
uname -m = x86_64
uname -r = 2.6.32-25-vserver
uname -s = Linux
uname -v = #44~ppa2-Ubuntu SMP Wed Oct 27 21:12:19 UTC 2010

...

c++ -o jsapi.o -c -I./dist/system_wrappers_js -include /root/git/build-couchdb/dependencies/js_src/config/gcc_hidden.h -DOSTYPE=\""Linux2.6\"" -DOSARCH=Linux -DEXPORT_JS_API -I/root/git/build-couchdb/dependencies/js_src -I. -I./dist/include -I./dist/include/nsprpub -I/root/git/build-couchdb/dependencies/js_src -fPIC -fno-rtti -fno-exceptions -Wall -Wpointer-arith -Woverloaded-virtual -Wsynth -Wno-ctor-dtor-privacy -Wno-non-virtual-dtor -Wcast-align -Wno-invalid-offsetof -Wno-variadic-macros -Wno-long-long -pedantic -fno-strict-aliasing -pthread -pipe -DNDEBUG -DTRIMMED -O3 -fstrict-aliasing -DMOZILLA_CLIENT -include ./js-confdefs.h -Wp,-MD,.deps/jsapi.pp /root/git/build-couchdb/dependencies/js_src/jsapi.cpp
In file included from /root/git/build-couchdb/dependencies/js_src/nanojit/nanojit.h:277,
from /root/git/build-couchdb/dependencies/js_src/jsbuiltins.h:45,
from /root/git/build-couchdb/dependencies/js_src/jsapi.cpp:59:
/root/git/build-couchdb/dependencies/js_src/nanojit/Containers.h:164: error: integer constant is too large for 'long' type
make[1]: *** [jsapi.o] Error 1
make[1]: Leaving directory `/tmp/tracemonkey_build20101107-10375-oeuo6o'
make: *** [default] Error 2
git checkout HEAD configure.in
git clean -df .
Removing configure
rake aborted!
Command failed with status (2): [make ...]

(See full trace by running task with --trace)",Fixed,Closed,11/7/2010 10:58:00,11/7/2010 11:39:00,11/7/2010 11:05:00,2,0,0.00
ValidField component creates javascript referencing handle_invalid_field() method that no longer exists,https://issues.apache.org/jira/browse/TAPESTRY-649,Fixed,Major,"Here is the html output from the <body>. I have a @FormBorder component
which includes a form that encloses the actual form inputs. The handle_invalid_field() function does not get created.

<script type=""text/javascript""
src=""/app?digest=b4909c59529064c46eb8843b65911500&path=%2Forg%
2Fapache%2Ftapestry%2Fform%2FForm.js&service=asset""></script>
<script type=""text/javascript""><!--

function validate_name(event)
{
var field = document.Form.name;

if (field.value.length == 0)

{ handle_invalid_field(event, field, ""You must enter a value for Name.""); return; }
}

// --></script>

<div class=""page"">

<div class=""maindiv"">

<div class=""topnav"">
<div class=""globalnav""><span class=""links""><a href=""#"">Contact Us</a| <a href=""#"">Sitemap</a| <a href=""#"">Search</a| <a href=""#"">Join Us</a></span></div>
<div class=""logo""><a href=""#""><img src=""images/logo_ulifeline.gif"" width=""219"" height=""52"" alt="""" border=""0"" /></a></div>
<div class=""mitmed""><img src=""images/logo_mitmed.gif"" width=""139"" height=""21"" alt="""" /></div>
<img src=""images/topwave.png"" width=""798"" height=""60"" alt="""" class=""bottomwave"" />
</div>

<div class=""c-content"">

<div class=""leftcol"">
<p class=""navbutton""><a href=""#"" class=""button"">Log Out</a></p>
<!-- <div class=""stylebuttonout""><div class=""stylebuttonin""><a href=""#"">Log Out</a></div></div-->
<div class=""leftnav"">
<table class=""navigation"" cellpadding=""0"" cellspacing=""0"">
<tr><td><a href=""/app?component=%24AdminBorder.%24DirectLink&page=admin%2FAddSpecialty&service=direct"">Ulifeline Administrator Home</a></td></tr>
<tr><td><a href=""/app?component=%24AdminBorder.%24DirectLink_0&page=admin%2FAddSpecialty&service=direct"" id=""nav_specialties"">Specialties</a></td></tr>

</table>
</div>
</div>

<div class=""contentarea"">

<h1 id=""pagetitle"">Add Specialty</h1>

<form method=""post"" action=""/app"" name=""Form"" id=""Form"">
<div><input type=""hidden"" name=""formids"" value=""Hidden,Hidden_0,name,Checkbox,Submit""/>
<input type=""hidden"" name=""component"" value=""formBorder.$Form""/>
<input type=""hidden"" name=""page"" value=""admin/AddSpecialty""/>
<input type=""hidden"" name=""service"" value=""direct""/>
<input type=""hidden"" name=""submitmode"" value=""""/>
<input type=""hidden"" name=""Hidden"" value=""0""/>
<input type=""hidden"" name=""Hidden_0"" value=""0""/>
</div>
<table class=""form"">

<tr>
<th><label for=""name"">Name</label></th>
<td><input type=""text"" name=""name"" id=""name""/></td>
</tr>
<tr>
<th>Active</th>
<td><input type=""checkbox"" name=""Checkbox"" id=""Checkbox""/></td>
</tr>

</table>

<div class=""formsubmit"">
<input type=""submit"" name=""Submit"" value=""Add Specialty"" id=""Submit"" class=""button""/>
</div>
</form>

</div>

</div>

<hr />

<div class=""footer"">
<span class=""left"">
<span class=""right"">
©ULifeline<br />
<a href=""#"">Terms of Use</a| <a href=""#"">Privacy Statement</a>
</span>
University Specific Contact Info goes here. Lorem ipsum dolor sit amet:<br />
Phone: 555-555-1234 Online: <a href=""#"">www.somwebaddress.edu</a>
</span>
</div>

</div>

</div>

<script language=""JavaScript"" type=""text/javascript""><!--
Tapestry.register_form('Form');

Tapestry.onsubmit('Form', validate_name);

Tapestry.set_focus('name');

// --></script></body>",Fixed,Closed,9/17/2005 3:36:00,9/22/2005 4:41:00,9/22/2005 4:41:00,1,5,0.00
"Canonical-ize hostnames for Hive metastore, and HS2 servers.",https://issues.apache.org/jira/browse/HIVE-17218,Fixed,Major,"Currently, the HiveMetastoreClient and HiveConnection do not canonical-ize the hostnames of the metastore/HS2 servers. In deployments where there are multiple such servers behind a VIP, this causes a number of inconveniences:

The client-side configuration (e.g. hive.metastore.uris in hive-site.xml) needs to specify the VIP's hostname, and cannot use a simplified CNAME, in the thrift URL. If the hive.metastore.kerberos.principal is specified using _HOST, one sees GSS failures as follows:
hive --hiveconf hive.metastore.kerberos.principal=hive/_HOST@GRID.MYTH.NET --hiveconf hive.metastore.uris=""thrift://simplified-hcat-cname.grid.myth.net:56789""
...
Exception in thread ""main"" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:542)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
...
This is because _HOST is filled in with the CNAME, and not the canonicalized name.

Oozie workflows that use HCat <credential> have to always use the VIP hostname, and can't use _HOST-based service principals, if the CNAME differs from the VIP name.
If the client-code simply canonical-ized the hostnames, it would enable the use of both simplified CNAMEs, and _HOST in service principals.",Fixed,Closed,7/31/2017 21:26:00,8/16/2017 17:45:00,7/31/2017 21:28:00,10,16,16.00
Unable to start karaf instance on fresh installation - elasticsearch error : java.lang.IllegalStateException: Received message from unsupported version: [5.2.2] minimal compatible version is: [5.6.0],https://issues.apache.org/jira/browse/UNOMI-196,Fixed,Major,"Services do not go live on a fresh installation. 

 
1. running ""bin/karaf"" results in karaf loading and then ""Initializing Unomi..."" which never finished.

2. Looking into log:display - it says: ""Unable to initialize bean elasticSearchPersistenceServiceImpl""

2018-08-23 12:55:58,615 | INFO | FelixStartLevel | RegionsPersistenceImpl | 49 - org.apache.karaf.region.persist - 3.0.8 | Loading region digraph persistence
2018-08-23 12:55:58,899 | INFO | FelixStartLevel | SecurityUtils | 30 - org.apache.sshd.core - 0.14.0 | BouncyCastle not registered, using the default JCE provider
2018-08-23 12:55:59,110 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Starting JMX OSGi agent
2018-08-23 12:55:59,157 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering MBean with ObjectName [osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9] for service with service.id [15]
2018-08-23 12:55:59,194 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.ServiceStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=serviceState,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,196 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.wiring.BundleWiringStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=wiringState,version=1.1,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,196 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.BundleStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=bundleState,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.PackageStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=packageState,version=1.5,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.service.cm.ConfigurationAdminMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.FrameworkMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=framework,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,371 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | EventAdmin support enabled, servlet events will be postet to topics.
2018-08-23 12:55:59,386 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | LogService support enabled, log events will be created.
2018-08-23 12:55:59,406 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | Pax Web started
2018-08-23 12:55:59,839 | INFO | pool-6-thread-1 | Server | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | jetty-8.1.19.v20160209
2018-08-23 12:55:59,935 | INFO | pool-6-thread-1 | AbstractConnector | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Started SelectChannelConnector@0.0.0.0:8181
2018-08-23 12:55:59,936 | INFO | pool-6-thread-1 | JettyServerImpl | 79 - org.ops4j.pax.web.pax-web-jetty - 3.2.9 | Pax Web available at [0.0.0.0]:[8181]
2018-08-23 12:55:59,964 | INFO | pool-6-thread-1 | JettyServerImpl | 79 - org.ops4j.pax.web.pax-web-jetty - 3.2.9 | Pax Web available at [0.0.0.0]:[9443]
2018-08-23 12:55:59,997 | INFO | pool-6-thread-1 | SslContextFactory | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Enabled Protocols [SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2] of [SSLv2Hello, SSLv3, TLSv1, TLSv1.1, TLSv1.2]
2018-08-23 12:55:59,999 | INFO | pool-6-thread-1 | AbstractConnector | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Started SslSelectChannelConnector@0.0.0.0:9443
2018-08-23 12:56:00,077 | INFO | FelixStartLevel | ContextLoaderListener | 106 - org.springframework.osgi.extender - 1.2.1 | Starting [org.springframework.osgi.extender] bundle v.[1.2.1]
2018-08-23 12:56:00,188 | INFO | FelixStartLevel | ExtenderConfiguration | 106 - org.springframework.osgi.extender - 1.2.1 | No custom extender configuration detected; using defaults...
2018-08-23 12:56:00,199 | INFO | FelixStartLevel | TimerTaskExecutor | 101 - org.apache.servicemix.bundles.spring-context - 3.2.17.RELEASE_1 | Initializing Timer
2018-08-23 12:56:00,833 | INFO | pache.cxf.osgi]) | HttpServiceFactoryImpl | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | Binding bundle: [org.apache.cxf.cxf-rt-transports-http [133]] to http service
2018-08-23 12:56:01,359 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer)]
2018-08-23 12:56:01,418 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer)]
2018-08-23 12:56:01,436 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:01,764 | WARN | FelixStartLevel | OSGiScriptEngineManager | 184 - com.hazelcast - 3.4.2 | Found ScriptEngineFactory candidate for com.sun.script.javascript.RhinoScriptEngineFactory, but cannot load class! -> java.lang.ClassNotFoundException: com.sun.script.javascript.RhinoScriptEngineFactory not found by com.hazelcast [184]
2018-08-23 12:56:01,827 | INFO | FelixStartLevel | XmlConfigLocator | 184 - com.hazelcast - 3.4.2 | Loading configuration /home/unomi/unomi-1.2.0-incubating/etc/hazelcast.xml from System property 'hazelcast.config'
2018-08-23 12:56:01,827 | INFO | FelixStartLevel | XmlConfigLocator | 184 - com.hazelcast - 3.4.2 | Using configuration file at /home/unomi/unomi-1.2.0-incubating/etc/hazelcast.xml
2018-08-23 12:56:02,118 | INFO | FelixStartLevel | DefaultAddressPicker | 184 - com.hazelcast - 3.4.2 | [LOCAL] [cellar] [3.4.2] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2018-08-23 12:56:02,128 | INFO | FelixStartLevel | DefaultAddressPicker | 184 - com.hazelcast - 3.4.2 | [LOCAL] [cellar] [3.4.2] Picked Address[127.0.0.1]:5701, using socket ServerSocket[addr=/0.0.0.0,localport=5701], bind any local is true
2018-08-23 12:56:02,313 | INFO | FelixStartLevel | OperationService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Backpressure is disabled
2018-08-23 12:56:02,316 | INFO | FelixStartLevel | BasicOperationScheduler | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Starting with 2 generic operation threads and 2 partition operation threads.
2018-08-23 12:56:02,369 | INFO | FelixStartLevel | system | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Hazelcast 3.4.2 (20150326 - f6349a4) starting at Address[127.0.0.1]:5701
2018-08-23 12:56:02,370 | INFO | FelixStartLevel | system | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Copyright (C) 2008-2014 Hazelcast.com
2018-08-23 12:56:02,371 | INFO | FelixStartLevel | Node | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Creating TcpIpJoiner
2018-08-23 12:56:02,372 | INFO | FelixStartLevel | LifecycleService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5701 is STARTING
2018-08-23 12:56:02,496 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Connecting to /127.0.0.1:5703, timeout: 0, bind-any: true
2018-08-23 12:56:02,497 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Could not connect to: /127.0.0.1:5703. Reason: SocketException[Connection refused to address /127.0.0.1:5703]
2018-08-23 12:56:02,497 | INFO | .cached.thread-2 | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5703 is added to the blacklist.
2018-08-23 12:56:02,498 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Connecting to /127.0.0.1:5702, timeout: 0, bind-any: true
2018-08-23 12:56:02,498 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Could not connect to: /127.0.0.1:5702. Reason: SocketException[Connection refused to address /127.0.0.1:5702]
2018-08-23 12:56:02,499 | INFO | .cached.thread-2 | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5702 is added to the blacklist.
2018-08-23 12:56:03,499 | INFO | FelixStartLevel | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2]

Members [1]

{ Member [127.0.0.1]:5701 this }
2018-08-23 12:56:03,520 | INFO | FelixStartLevel | LifecycleService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5701 is STARTED
2018-08-23 12:56:03,582 | INFO | FelixStartLevel | InternalPartitionService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Initializing cluster partition table first arrangement...
2018-08-23 12:56:03,681 | INFO | rint Extender: 2 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,682 | INFO | rint Extender: 3 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,682 | INFO | rint Extender: 1 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 3 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 1 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 2 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,927 | INFO | FelixStartLevel | BundleWatcher | 193 - org.apache.unomi.lifecycle-watcher - 1.2.0.incubating | Bundle watcher initialized.
2018-08-23 12:56:04,119 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/default.json, loading...
2018-08-23 12:56:04,119 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/campaign.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/campaignevent.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/event.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/goal.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/personaSession.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/profile.json, loading...
2018-08-23 12:56:04,121 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/propertyType.json, loading...
2018-08-23 12:56:04,121 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/rule.json, loading...
2018-08-23 12:56:04,123 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/scoring.json, loading...
2018-08-23 12:56:04,125 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/segment.json, loading...
2018-08-23 12:56:04,129 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/session.json, loading...
2018-08-23 12:56:04,156 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Connecting to ElasticSearch persistence backend using cluster name contextElasticSearch and index name context...
2018-08-23 12:56:06,682 | ERROR | FelixStartLevel | ServiceRecipe | 15 - org.apache.aries.blueprint.core - 1.6.2 | Error retrieving service from ServiceRecipe[name='elasticSearchPersistenceService']
org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:738)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate2(BeanRecipe.java:848)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:811)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.RefRecipe.internalCreate(RefRecipe.java:62)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:106)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:285)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalGetService(ServiceRecipe.java:252)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalCreate(ServiceRecipe.java:149)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:186)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:724)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:411)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1]
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:]
at java.lang.Thread.run(Thread.java:748)[:1.8.0_181]
Caused by: NoNodeAvailableException[None of the configured nodes are available: [{#transport#-1}

{o4uAdKg_QpCsuv2dzXKFyw}{localhost}{127.0.0.1:9300}]]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
[TRIMMED 50K CHAR LIMIT]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498)[:1.8.0_181]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:299)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:980)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:736)[15:org.apache.aries.blueprint.core:1.6.2]
... 35 more
2018-08-23 12:56:06,711 | WARN | FelixStartLevel | BeanRecipe | 15 - org.apache.aries.blueprint.core - 1.6.2 | Object to be destroyed is not an instance of UnwrapperedBeanHolder, type: null
2018-08-23 12:56:06,715 | ERROR | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Unable to start blueprint container for bundle org.apache.unomi.persistence-elasticsearch-core/1.2.0.incubating
org.osgi.service.blueprint.container.ComponentDefinitionException: org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:310)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalGetService(ServiceRecipe.java:252)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalCreate(ServiceRecipe.java:149)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:186)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:724)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:411)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1]
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:]
at java.lang.Thread.run(Thread.java:748)[:1.8.0_181]
Caused by: org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:738)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate2(BeanRecipe.java:848)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:811)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.RefRecipe.internalCreate(RefRecipe.java:62)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:106)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:285)[15:org.apache.aries.blueprint.core:1.6.2]
... 27 more
Caused by: NoNodeAvailableException[None of the configured nodes are available: [{#transport#-1}

{o4uAdKg_QpCsuv2dzXKFyw}
{localhost}

{127.0.0.1:9300}
]]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498)[:1.8.0_181]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:299)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:980)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:736)[15:org.apache.aries.blueprint.core:1.6.2]
... 35 more
2018-08-23 12:56:06,743 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.services/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.persistence.spi.PersistenceService)]
2018-08-23 12:56:06,748 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.cxs-lists-extension-services/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.persistence.spi.PersistenceService), (objectClass=org.apache.unomi.api.services.DefinitionsService)]
2018-08-23 12:56:06,758 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Dynamically adding namespace handler http://cxf.apache.org/configuration/beans to bundle org.apache.unomi.cxs-lists-extension-rest/1.2.0.incubating
2018-08-23 12:56:06,778 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.cxs-lists-extension-rest/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.services.UserListService)]
2018-08-23 12:56:06,787 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.cxs-geonames-services/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.api.services.DefinitionsService), (objectClass=org.apache.unomi.persistence.spi.PersistenceService)]
2018-08-23 12:56:06,795 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.geonames.services.GeonamesService)]
2018-08-23 12:56:06,804 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.cxs-privacy-extension-services/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.api.services.EventService), (objectClass=org.apache.unomi.api.services.DefinitionsService), (objectClass=org.apache.unomi.persistence.spi.PersistenceService), (objectClass=org.apache.unomi.api.services.ProfileService)]
2018-08-23 12:56:06,812 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.api.services.PrivacyService)]
2018-08-23 12:56:06,836 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Dynamically adding namespace handler http://cxf.apache.org/configuration/beans to bundle org.apache.unomi.rest/1.2.0.incubating
2018-08-23 12:56:06,838 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Dynamically adding namespace handler http://cxf.apache.org/configuration/parameterized-types to bundle org.apache.unomi.rest/1.2.0.incubating
2018-08-23 12:56:06,860 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.rest/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.api.services.ProfileService), (objectClass=org.apache.unomi.api.services.UserListService), (objectClass=org.apache.unomi.api.services.GoalsService), (objectClass=org.apache.unomi.api.services.QueryService), (objectClass=org.apache.unomi.api.services.ClusterService), (objectClass=org.apache.unomi.api.services.SegmentService), (objectClass=org.apache.unomi.api.services.EventService), (objectClass=org.apache.unomi.api.services.RulesService), (objectClass=org.apache.unomi.api.services.DefinitionsService)]
2018-08-23 12:56:06,868 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.wab/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.api.services.RulesService), (objectClass=org.apache.unomi.api.services.PrivacyService), (objectClass=org.apache.unomi.api.services.ConfigSharingService), (objectClass=org.apache.unomi.api.services.EventService), (objectClass=org.apache.unomi.api.services.ProfileService)]
2018-08-23 12:56:06,915 | INFO | FelixStartLevel | HttpServiceFactoryImpl | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | Binding bundle: [org.apache.unomi.wab [214]] to http service
2018-08-23 12:56:06,948 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.plugins-base/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.api.services.DefinitionsService), (objectClass=org.apache.unomi.api.services.ConfigSharingService), (objectClass=org.apache.unomi.api.services.ProfileService), (objectClass=org.apache.unomi.api.services.PrivacyService), (objectClass=org.apache.unomi.api.services.SegmentService), (objectClass=org.apache.unomi.api.services.EventService), (objectClass=org.apache.unomi.persistence.spi.PersistenceService)]
2018-08-23 12:56:07,013 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.plugins-mail/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.persistence.spi.PersistenceService)]
2018-08-23 12:56:07,034 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.cxs-lists-extension-actions/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.api.services.ProfileService), (objectClass=org.apache.unomi.api.services.EventService)]
2018-08-23 12:56:07,056 | INFO | FelixStartLevel | Activator | 223 - org.apache.camel.camel-core - 2.19.1 | Camel activator starting
2018-08-23 12:56:07,065 | INFO | FelixStartLevel | Activator | 223 - org.apache.camel.camel-core - 2.19.1 | Camel activator started
2018-08-23 12:56:08,163 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.router-core/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.persistence.spi.PersistenceService), (objectClass=org.apache.unomi.router.api.services.ProfileExportService), (objectClass=org.apache.unomi.api.services.ProfileService), (objectClass=org.apache.unomi.router.api.services.ProfileImportService), (&(configDiscriminator=IMPORT)(objectClass=org.apache.unomi.router.api.services.ImportExportConfigurationService)), (&(configDiscriminator=EXPORT)(objectClass=org.apache.unomi.router.api.services.ImportExportConfigurationService)), (objectClass=org.apache.unomi.api.services.SegmentService), (objectClass=org.apache.unomi.api.services.ConfigSharingService)]
2018-08-23 12:56:08,174 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.router-service/1.2.0.incubating is waiting for dependencies [(objectClass=org.apache.unomi.persistence.spi.PersistenceService), (objectClass=org.apache.unomi.api.services.ConfigSharingService)]
2018-08-23 12:56:08,186 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.unomi.router-rest/1.2.0.incubating is waiting for dependencies 3[(objectClass=org.apache.unomi.api.services.ConfigSharingService), (objectClass=org.apache.unomi.api.services.ProfileService), (objectClass=org.apache.unomi.router.api.services.ProfileExportService), (&(configDiscriminator=IMPORT)(objectClass=org.apache.unomi.router.api.services.ImportExportConfigurationService)), (&(configDiscriminator=EXPORT)(objectClass=org.apache.unomi.router.api.services.ImportExportConfigurationService))]

=======================================

3. I verified elasticsearch is well configured and I noticed in /var/log/elasticsearch/contextElasticSearch.log:

[2018-08-23T12:58:51,898][WARN ][o.e.x.s.t.n.SecurityNetty4ServerTransport] [hjui3pv] exception caught on transport layer [NettyTcpChannel

{localAddress=/127.0.0.1:9300, remoteAddress=/127.0.0.1:43570}
], closing connection
java.lang.IllegalStateException: Received message from unsupported version: [5.2.2] minimal compatible version is: [5.6.0]
at org.elasticsearch.transport.TcpTransport.ensureVersionCompatibility(TcpTransport.java:1462) ~[elasticsearch-6.3.2.jar:6.3.2]
at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1409) ~[elasticsearch-6.3.2.jar:6.3.2]
at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:64) ~[transport-netty4-6.3.2.jar:6.3.2]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:297) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:413) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:241) [netty-handler-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:545) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.16.Final.jar:4.1.16.Final]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]

Please advise what should be done in order to resolve this.

Thank you.",Fixed,Closed,8/23/2018 13:02:00,9/12/2018 12:12:00,8/23/2018 13:29:00,5,20,20.00
Need a 32×32 icon for Java options,https://issues.apache.org/jira/browse/NETBEANS-284,Fixed,Minor,"The ContainerRegistration for Java options, defined in options.java/src/org/netbeans/modules/options/java/resources/package-info.java, is currently using a 20×20 iconBase, whereas other options icons are 32×32. This makes the icon look smaller than the others in NetBeans > Preferences…. See the attached screenshot.",Fixed,Closed,1/16/2018 17:58:00,4/1/2018 19:22:00,4/1/2018 19:22:00,2,75,0.00
Hadoop configurations on the classpath seep into the S3 file system configs,https://issues.apache.org/jira/browse/FLINK-10383,Fixed,Major,"The S3 connectors are based on a self-contained shaded Hadoop. By design, they should only use config value from the Flink configuration.

However, because Hadoop loads implicitly configs from the classpath, existing ""core-site.xml"" files can interfere with the configuration in ways intransparent for the user. We should ensure such configs are not loaded.",Fixed,Closed,9/20/2018 17:43:00,9/25/2018 7:25:00,9/20/2018 18:30:00,4,5,5.00
Bandar Togel Terbaik,https://issues.apache.org/jira/browse/AAR-9319,-,-,"Bandar Togel Hadiah 4D 10 Juta Terbaik & Terpercaya | Daftar di BO Togel Terpercaya bet 100 perak 2021
[Bandar Togel Hadiah 4D 10 Juta Terpercaya|#] terhadap Zaman sekarang ini kami telah perlu pandai pandai di dalam melacak penghasilan sampingan, apabila selama ini kamu sekedar mengandalkan penghasilan primer saja gara-gara telah berlimpah peluang yang terbuka sementara ini dan keliru satunya ialah bersama bermain di Bandar Togel Hadiah 4D 10 Juta Terbesar Dan Terpercaya Kepada kamu sekalian. Layaknya yang kamu ketahui sendiri bahwa permainan tebak angka ini udah berlimpah pengaruhi nasib seseorang jadi jutawan sekedar bersama kapital yang kecil saja, Pasaran Togel Singapore jadi tidak benar satu pasaran yang paling tak terhitung dimainkan oleh bettor sementara ini dan cocok bagi kamu yang dambakan mulai terjun di dalam global pertogelan.

Pastinya kamu seluruh telah tau apa tersebut Togel Online sebab memang permainan BO Togel Hadiah 4d 10 juta satu ini telah terlampau populer berasal dari pernah sampai sekarang ini, nah disini kita inginkan merekomendasikan sebagian Bandar Singapore Pools yang tak terhitung dimainkan oleh para Togellers yang tersedia di Internet. Kudu diketahui bahwa sebelum kita merekomendasikan seluruh Web site Bandar Togel Hadiah 4d 10 Juta terpercaya di bawah ini udah kita coba mainkan terlebih dahulu sebelum memberitahukannya kepada kamu seluruh, bagi kamu yang tengah melacak Web Togel Terpercaya maka sanggup mempertimbangkan untuk bermain di tidak benar satu web site ini.

Daftar Bandar Togel Online Resmi Terpercaya 2021
Laman ini menghadirkan Daftar Bandar Togel Hadiah 4D 10 Juta Terpercaya tahun ini yang tersedia di kawasan Asia, khususnya wilayah Indonesia. Kumpulan Bandar Togel Hadiah 4D 10 Juta Terpercaya ini, akan dibahas satu persatu secara singkat dan memahami, supaya tiap tiap betor yang menonton akan jelas segera julukan web togel terpercaya yang sanggup digabung untuk daftar akun togel formal dan dijadikan partner bertaruh toto togel online. Yang tentu dan bukan barangkali mengecewakan kamu terhadap waktu ini adalah, seluruh Bandar yang kita rekomendasikan disini merupakan Bandar Togel Terpercaya hadiah terbesar. Pasti saja tersebut akan memicu untuk terhadap waktu bergabung, sebab bukan seluruh Bandar sekarang ini udah jadi Bandar Togel Hadiah 4D 10 Juta layaknya yang kita referensikan disini.

Sehabis mempunyai account member bandar Permainan Toto Hadiah 4D 10 Juta Terpercaya Indonesia, lakukanlah pengisian game credit user id. Mampu bersama dengan transfer lewat atm, m banking, e-banking bank lokal terbesar layaknya Bca, Bni, Bri, Mandiri, CIMB Niaga ataupun deposit togel via pulsa Telkomsel. Dapat juga mengikuti kecanggihan teknologi era now bersama kenakan pelaksanaan dompet online, e-wallet layaknya Gopay, Ovo, DANA sertar Linkaja!. Agen togel deposit ovo selalu siap online 24jam melayani segala keperluan kesibukan permainan judi togel depo termurah. Pokoknya ga bakal rugi deh main di bandar togel deposit 10 ribu karna layanannya kondusif, cepat dan nyaman.

Bandar Togel Hadiah 4D 10 Juta Terbesar Dan Terpercaya
Pada Zaman sekarang ini kita sudah harus pintar – pintar dalam mencari penghasilan sampingan, apabila selama ini anda hanya mengandalkan penghasilan utama saja karena sudah banyak peluang yang terbuka saat ini dan salah satunya ialah dengan bermain di [BO TOGEL HADIAH 4D 10 JUTA TERBESAR DAN TERPERCAYA|#] kepada anda sekalian. Seperti yang anda ketahui sendiri bahwa permainan tebak angka ini sudah banyak merubah nasib seseorang menjadi jutawan hanya dengan modal yang kecil saja, Pasaran Togel Singapore menjadi salah satu pasaran yang paling banyak dimainkan oleh bettor saat ini dan cocok bagi anda yang ingin mulai terjun dalam dunia pertogelan.

Pastinya anda semua sudah tau apa itu Togel Online karena memang permainan BO Togel Hadiah 4D 10 juta satu ini sudah sangat populer dari dulu hingga sekarang ini, nah disini kami ingin merekomendasikan beberapa Bandar Singapore Pools yang banyak dimainkan oleh para Togellers yang ada di Internet. Perlu diketahui bahwa sebelum kami merekomendasikan semua Situs Bandar Togel Hadiah 4D 10 Juta terpercaya di bawah ini sudah kami coba mainkan terlebih dahulu sebelum memberitahukannya kepada anda semua, bagi anda yang sedang mencari Website Togel Terpercaya maka bisa mempertimbangkan untuk bermain di salah satu situs ini.

Cara Daftar Akun di Situs BO Togel Terpercaya di Indonesia 24jam
Untuk bermain permainan togel di Togel Situs Bandar Togel Terpercaya. Tentunya kalian harus memiliki akun untuk melakukan berbagai jenis transaksi dalam melakukan betting di situs togel resmi toto. Karena kalau kita tidak punya akun, Tentunya kita tidak dapat melakukan betting, deposit, maupun withdraw. Disini kami sediakan langkah-langkah khusus untuk para bettor yang akan bermain di Togel 5 Bandar Togel Terpercaya sebagia berikut :

Pastikan anda berumur 18 Tahun lebih
Kunjungi situs 5 Bandar Togel Terpercaya
Klik tombol Pendaftaran / Daftar
Isi Biodata Diri dengan baik dan benar
Isi Username akun
isi password sesuai selera anda
Masukkan email yang aktif 24jam
Masukkan no handphone yang dapat dihubungi
Pilih Jenis pembayaran (Bank Lokal maupun E-wallet)
Isi atas nama rekening tsb
Masukan nomor rekening anda
Selesai.
 

Sebelum melakukan pendaftaran. Pastikan jaringan internet yang kamu gunakan cepat dan tentunya stabil. Karena untuk bermain togel online. Kamu harus mempunyai koneksi yang bagus untuk bermain maupun daftar akun togel di [Daftar Bandar Togel Terpercaya|#]. Maka dari itulah kami sarankan agar menggunkan koneksi Wi-Fi ketika bermain togel online di Situs Bandar Togel Hadiah 4d 10 Juta Terbesar.

4 Negara Pasaran Bandar Togel Resmi Dan Terbaik Di Dunia
Di dalam perjudian togel, tentunya tidak akan menarik apabila tidak memasang taruhan pada pasaran togel yang resmi dan terbaik di dunia. Sebab bila kita memasang pada pasaran togel sembarangan atau tidak legal di dunia. Pastinya kemenangan yang akan diraih akan sangat sulit atau bisa dikatakan itu adalah pasaran togel settingan. Sehingga apapun angka yang akan kita pasangkan maka kekalahan yang akan didapatkan.

Lalu seperti apakah negara-negara yang menjadi pasaran togel resmi dan terbaik di dunia menurut organisasi resmi seperti WLA (World Lottery Association) dan APLA (Asia Pacific on the World Lottery Association). Tentunya seluruh bettor togel sekarang ini ingin mengetahui atau penasaran dengan nama-nama pasaran togel terbaik tersebut bukan?.

Mari langsung saja admin dari blog seputaran permainan togel online ini akan menyebutkan 4 saja negara yang menjadi pasaran togel resmi dan terbaik di dunia menurut APLA dan WLA sebagai berikut :

1. Pasaran Toto Macau
Toto macau adalah pasaran togel terbaik dan resmi di dunia yang sekarang ini menjadi pasaran nomor 1 di indonesia. Dimana pada pasaran togel toto macau ini. Memiliki 3 mode jenis betting terbaik yang memudahkan seluruh bettor untuk melakukan taruhan. Bisa memilih mode dengan hadiah full terbesar ataupun mode diskon yang memiliki hadiah standart namun sewaktu melakukan taruhan modal yang dikeluarkan tidak terlalu banyak karena memiliki diskon atau potongan.

Namun toto macau sendiri dipilih bukan karena hal sepele itu saja. Ada berbagai kelebihan dan keuntungan lainnya yang menjadikan pasaran tersebut nomor 1 di indonesia sekarang ini. Meskipun pasaran ini baru saja hadir sejak tahun 2015 silam dan paling muda dibandingkan dengan pasaran-pasaran togel lainnya seperti Hongkong (HK) , Sydney (SDY) ataupun Singapore (SGP). Tetapi pasaran ini memiliki hal yang tidak dimiliki oleh pasaran-pasaran togel tertua ataupun lawas itu.

Biasanya pasaran togel umum atau pasaran togel dunia lainnya hanya memiliki jadwal perputaran angka result 1 kali dalam setiap harinya. Namun berbeda halnya dengan pasaran togel macau ini. Ia memiliki 4 kali perputaran angka yang live yang bisa dimainkan oleh seluruh bettor setiap harinya. Dengan hadiah yang besar dan memiliki hasil result terbanyak, tentunya hal tersebut menjadikan toto macau adalah pasaran nomor 1 di dunia sekarang ini karena tidak ada pasaran yang mengeluarkan angka sebanyak itu dalam 1 harinya.

Selain itu ia memiliki situs resmi pengeluaran angka yang berbasis live streaming atau siaran langsung setiap jadwal atau jam tutup pasaran. Sehingga seluruh bettor di tanah air kita dapat menyaksikan secara langsung perputaran angka tersebut Oleh karena pasaran ini memiliki lisensi resmi dari pihak WLA dan APLA tentunya setiap pengeluaran angka itu di jamin aman 100% dan tidak ada kecurangan apapun.

Buat seluruh bettor di luar sana yang masih kurang mengetahui jadwal perputaran angka atau jam result dari pasaran toto macau di Bandar Togel Hadiah 4D 10 Juta Terpercaya ini anda semua dapat melihatnya di bawah ini :

Jam Tutup Pasaran
12:59 WIB
15:59 WIB
18:59 WIB
21:59 WIB
Jam Tutup Pasaran
13:15 WIB
16:15 WIB
19:15 WIB
22:15 WIB
 

Setiap perputaran atau bola gelinding yang akan di tampilkan pada permainan toto macau, Hanya akan mencari 4 bola terakhir yang masih berada pada meja. Sehingga kita dapat menyimpulkan dengan mudah bahwasannya angka apa saja yang akan menjadi result setiap perputaran. Dan untuk live streaming atau situs resmi toto macau dapat di lihat langsung pada website ataupun situs Bandar Togel Hadiah 4D 10 Juta Terpercaya. Pastikan anda melakukan taruhan pada pasaran terbaik ini dan menjadi salah seorang bettor ternama di indonesia tahun 2021.

Setelah anda sudah yakin pada situs pilihan kalian, anda dapat langsung membuat akun togel online dengan melakukan pendaftaran pada Bandar Togel Hadiah 4D 10 Juta Terpercaya secara gratis. Cara daftar akun togel resmi online sangat mudah dilakukan oleh siapa saja. Yang terpenting anda bersedia memberikan beberapa data pribadi anda dan kemanan privacy nya terjamin 100%. Karna data-data tersebut sangat berguna bagi kelangsungan aktivitas judi togel uang asli di bo toto macau bet 100 diskon besar. Perangkat elektronik seperti computer atau smartphone dan koneksi internet yang stabil sangatlah dibutuhkan. Sebab itu semua merupakan alat
Utama bila ingin bertaruh lotto secara daring. Setelah menyelesaikan proses registrasi, anda memiliki akses masuk ke semua pasaran togel online terlengkap di asia dan dunia. Ayo segera bikin akun toto dengan Bandar Togel Hadiah 4D 10 Juta Terpercaya sekarang juga!

2. Pasaran Togel Hongkong
Hongkong pools adalah nama pasaran togel terbaik ke-2 yang ditampilkan pada situs blog seputar permainan togel online ini. Dimana hongkong atau yang biasa dikenal dengan nama lain yaitu HK merupakan salah satu pasaran togel terlama dan terbaik di indonesia. Hampir seluruh bettor di indonesia mengenal dengan nama pasaran togel tersebut.

Hongkong pools dulunya terkenal melalui via bandar darat togel di indonesia. Tidak seperti pasaran toto macau yang terkenal karena zaman sudah sangat modern seperti sekarang ini. Dulunya pasaran hongkong hanya dikenal oleh bettor-bettor lama maupun yang sudah tua saja. Karena dulunya pasaran togel yang aktif di indonesia hanya beberapa saja. Di antara salah satunya adalah pasaran togel hongkong sendiri.

Pasaran yang sudah lama hadir dan memiliki lisensi atau sertifikat resmi dari pihak APLA dan WLA di dunia adalah Hongkong pools. Pasaran ini sangat diminati oleh seluruh bettor togel di indonesia karena memiliki jadwal result yang sangat cocok untuk dimainkan. Dimana jam result yang dimiliki oleh hongkong pools adalah 23.00 WIB yang merupakan adalah waktu senggang atau waktu santai untuk para masyarakat pekerja di indonesia. Sehingga dapat dimainkan tanpa harus terburu-buru.

Tetapi jika anda semua malas atau enggan mencari satu-persatu website yang berada di internet sekarang ini. Karena ribet ataupun memakan waktu yang cukup lama. Tentunya pada situs blog togel online ini anda semua bisa memilih dari beberapa situs yang kami sajikan tersebut sebagai partner resmi anda semua dalam melakukan taruhan. Dikarenakan setiap website yang telah direferensikan pada situs ini dijamin aman dan terpercaya 100%.

Untuk hadiah togel pada pasaran hongkong ini dulunya memiliki hadiah togel yang standar ataupun tergolong kecil. Sebab para bettor hanya dapat melakukan taruhan togel pada bandar darat di indonesia. Berbeda dengan pasaran toto macau yang memiliki hadiah togel terbesar dan spektakuler. Tentunya seiring berjalannya waktu pasaran togel pun berkembang mengikuti zaman yang modern. Dan sekarang kita bisa melakukan taruhan pada pasaran togel tersebut dengan 3 mode betting yang memiliki hadiah togel setimpal dengan pasaran-pasaran lainnya.

3. Pasaran Togel Sydney
Sydney pools atau biasa yang dikenal dengan nama lain oleh para bettor yaitu SDY merupakan salah satu pasaran togel resmi dan terbaik yang memiliki sertifikat dan reputasi terbaik di indonesia. Tidak seperti pasaran togel lainnya atau pasaran abal-abal di luar sana yang sewaktu-waktu bisa mengubah hasil result pada situs pengeluaran angka. Tentunya pada pasaran sydney di jamin 100% aman dan tidak ada kecurangan karena memiliki akses resmi dari pihak WLA dan APLA.

Namun siapa sangka bahwasannya pasaran togel sydney ini menjadi salah satu pasaran togel terbaik di indonesia yang memiliki peminat judi terbanyak di indonesia. Pasaran yang berasal dari negara australia yang bertempatkan pada kota Sydney ini. memiliki jadwal result siang hari yaitu pukul 13.50 WIB dari situs resmi yaitu Bandar Togel Hadiah 4D 10 Juta Terpercaya.

Pasaran togel sydney sama dengan pasaran hongkong ataupun singapore yang merupakan pasaran ter-lawas atau terlama di indonesia. Sehingga hampir seluruh bettor mengenal dengan pasaran yang satu ini dan di jamin 100% aman tidak adanya kecurangan apapun saat memutarkan angka. Dan pastinya pasaran yang satu ini tersedia pada seluruh website ataupun Bandar Togel Hadiah 4D 10 Juta Terpercaya Di indonesia.

4. Pasaran Togel Singapore
Pasaran togel terbaik ke-4 adalah singapore pools atau SGP yang dikenal oleh seluruh penjudi di tanah air indonesia. Pastinya seluruh masyarakat indonesia mengenal baik dengan pasaran yang satu ini. Baik kalangan muda maupun lanjut usia mengetahui betul bagaimana dari pasaran ini. Karena pasaran ini resmi dari negara SINGAPORE untuk perputaran lottery atau angka keluarannya.

Selain ada akses resmi dari pihak APLA dan WLA tentunya ada berbagai organisasi resmi yang wajib dimiliki oleh tiap-tiap pasaran togel resmi dan terbaik di dunia. Sebab setiap perputaran togel yang di lakukan tersebut wajib memiliki pengamatan dari organisasi tersebut agar tidak terjadi kecurangan ataupun hal-hal yang tidak diinginkan. Dan dengan adanya organisasi tersebut hadiah togel online pun bisa menjadi sangat besar karena sponsor yang diberikan oleh pihak organisasi tersebut.

Namun pasaran ini sempat menjadi kontroversi karena pada tahun 2020 silam pasaran ini libur cukup panjang dari bulan 4 2020 hingga akhir tahun 2021. Hal tersebut menjadikan pasaran singapore menjadi redup ataupun ditakuti oleh sebagian bettor togel di indonesia. Sebab semasa libur panjang dari negara tersebut banyak sekali munculnya website-website palsu yang mengaku-ngaku sebagai situs resmi Singapore.

Tetapi sekarang ini Singapore atau SGP pools sudah kembali bersinar dan memberikan banyak sekali keuntungan untuk seluruh peminat judi togel di indonesia. Untuk hasil result dari pasaran ini hanya pada website Bandar Togel Hadiah 4D 10 Juta Terpercaya dan memiliki jadwal tayang yaitu 17.50 WIB untuk hari senin , rabu , kamis , sabtu dan minggu, sedangkan hari selasa dan jumat seperti biasa pasaran ini akan tutup alias libur.

Related Keywords

bandar togel terpercaya dan terlengkap
bandar togel terpercaya 2020
bandar togel online terpercaya dan berbayar
bandar togel resmi indonesia
bandar togel hadiah terbesar
bandar togel hadiah prize 12345
bandar togel resmi terbaru deposit 10 ribu
bandar togel terbaik dan terbesar 2021
bandar togel hadiah 4d 10 juta
daftar bandar togel terbesar dan terpercaya
5 bandar togel terpercaya 2021
daftar bandar togel terbesar resmi deposit 10 ribu
daftar bandar togel tertua
semua bandar togel terbaik dan terpercaya no 1
bandar lotre togel deposit via dana
bandar togel terpercaya via gopay
bandar togel terbesar 2021
bandar togel 4D hadiah terbesar
bandar togel terbaru dan terpercaya 2020
bandar togel terbesar di dunia
bandar togel resmi terpercaya 2021
bandar togel terjitu di indonesia
bandar togel terpercaya hadiah terbesar
bandar togel terpercaya dan paling jitu 2021
bandar togel terbesar di indonesia
bandar togel terbaik di indonesia
bandar togel terjitu dan terpercaya online
Bandar Togel terpercaya indonesia via dana
Bandar Togel terbesar via gopay
Bandar Togel online 24 jam terpercaya
Bandar Togel terpercaya 2020
data togel terlengkap 2021
Bandar Togel terbaik
Bandar Togel termudah via ovo
Bandar Togel terbaru
Bandar Togel terbaru dan termurah via linkaja
Bandar Togel terbesar dan terpercaya
Bandar Togel terpercaya di indonesiabandar togel terpercaya dan terlengkap
bandar togel terpercaya 2020
bandar togel online terpercaya dan berbayar
bandar togel formal indonesia
bandar togel hadiah terbesar
bandar togel hadiah prize 12345
bandar togel formal terbaru deposit 10 ribu
bandar togel paling baik dan terbesar 2021
bandar togel hadiah 4d 10 juta
daftar bandar togel terbesar dan terpercaya
5 bandar togel terpercaya 2021
daftar bandar togel terbesar formal deposit 10 ribu
daftar bandar togel tertua
seluruh bandar togel paling baik dan terpercaya no 1
bandar lotre togel deposit via dana
bandar togel terpercaya via gopay
bandar togel terbesar 2021
bandar togel 4d hadiah terbesar
bandar togel terbaru dan terpercaya 2020
bandar togel terbesar di global
bandar togel formal terpercaya 2021
bandar togel terjitu di indonesia
bandar togel terpercaya hadiah terbesar
bandar togel terpercaya dan paling jitu 2021
bandar togel terbesar di indonesia
bandar togel paling baik di indonesia
bandar togel terjitu dan terpercaya online
Bandar Togel terpercaya indonesia via dana
Bandar Togel terbesar via gopay
Bandar Togel online 24 jam terpercaya
Bandar Togel terpercaya 2020
knowledge togel terlengkap 2021
Bandar Togel paling baik
Bandar Togel termudah via ovo
Bandar Togel terbaru
Bandar Togel terbaru dan termurah via linkaja
Bandar Togel terbesar dan terpercaya
Bandar Togel terpercaya di indonesia
Bandar Togel terpercaya hari ini",Fixed,Closed,12/23/2021 9:21:00,12/23/2021 9:21:00,N/A,0,0,0.00
ERROR: Unable to generate spec: read file info,https://issues.apache.org/jira/browse/COUCHDB-2741,Fixed,Major,"Trying to build 51b98a4, I'm getting this failure during the make install target:

/usr/bin/make DESTDIR=/home/micah/debian/couchdb/couchdb-2.0/debian/couchdb install
make[2]: Entering directory '/home/micah/debian/couchdb/couchdb-2.0'
==> b64url (compile)
==> cassim (compile)
==> lager (compile)
==> couch_log (compile)
==> config (compile)
==> chttpd (compile)
==> couch (compile)
==> couch_epi (compile)
==> couch_index (compile)
==> couch_mrview (compile)
==> couch_replicator (compile)
==> couch_plugins (compile)
==> couch_event (compile)
==> couch_stats (compile)
==> ddoc_cache (compile)
==> ets_lru (compile)
==> meck (compile)
==> fabric (compile)
==> bear (compile)
==> folsom (compile)
==> global_changes (compile)
==> goldrush (compile)
==> ibrowse (compile)
==> ioq (compile)
==> jiffy (compile)
==> khash (compile)
==> mango (compile)
==> mem3 (compile)
==> mochiweb (compile)
==> oauth (compile)
==> rexi (compile)
==> snappy (compile)
==> setup (compile)
==> rel (compile)
==> couchdb-2.0 (compile)
Installing CouchDB into /home/micah/debian/couchdb/couchdb-2.0/debian/couchdb//usr/lib/couchdb...
==> rel (generate)
ERROR: Unable to generate spec: read file info /usr/lib/erlang/man/man8/cups-deviced.8.gz failed
ERROR: Unexpected error: rebar_abort
ERROR: generate failed while processing /home/micah/debian/couchdb/couchdb-2.0/rel: rebar_abort
Makefile:84: recipe for target 'install' failed
this is my install.mk:

# Licensed under the Apache License, Version 2.0 (the ""License""); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
#
# The contents of this file are auto-generated by configure
#
package_author_name = The Apache Software Foundation
install_dir = /usr/lib/couchdb

bin_dir = /usr/bin
libexec_dir = /lib/x86_64-linux-gnu/couchdb
doc_dir = /usr/share/doc/apache-couchdb/couchdb
sysconf_dir = /etc/couchdb
data_dir = /usr/share/couchdb

database_dir = /var/lib
view_index_dir = /var/lib
log_file = /var/log/couch.log

html_dir = /usr/share/doc/apache-couchdb/html
pdf_dir = /usr/share/doc/apache-couchdb/pdf
man_dir = /usr/share/man
info_dir = /share/info

with_fauxton = 1
with_docs = 1

user = micah",Fixed,Closed,7/7/2015 15:45:00,3/18/2017 4:43:00,7/17/2015 16:06:00,4,620,610.00
LICENSE and NOTICE files are not correct,https://issues.apache.org/jira/browse/FLINK-10987,Fixed,Blocker,Flink's LICENSE and NOTICE files are not correct wrt http://www.apache.org/dev/licensing-howto.html. We need to update them before we can release 1.7.0.,Fixed,Closed,11/22/2018 12:31:00,11/28/2018 17:24:00,11/28/2018 15:33:00,6,6,1.85