NAME,LINK,DESCRIPTION,ChatGPT_RQ1,Priority,Final_Result,Rationale
Website updates to satisfy Apache privacy policies,https://issues.apache.org/jira/browse/KAFKA-13868,"The ASF has updated its privacy policy and all websites must be compliant. 

The full guidelines can be found in https://privacy.apache.org/faq/committers.html 

The Kafka website has a few issues, including: 

It's missing a link to the privacy policy: https://privacy.apache.org/policies/privacy-policy-public.html 
It's using Google Analytics 
It's using Google Fonts 
It's using scripts hosted on Cloudflare CDN 
Embedded videos don't have an image placeholder 
As per the email sent to the PMC, all updates have to be done by July 22.","B: The Kafka website is missing a link to the privacy policy: https://privacy.apache.org/policies/privacy-policy-public.html.  
B: The Kafka website is using Google Analytics.  
B: The Kafka website is using Google Fonts.  
B: The Kafka website is using scripts hosted on Cloudflare CDN.  
B: Embedded videos on the Kafka website don't have an image placeholder.  
B: All updates to the Kafka website must be completed by July 22.",Critical,FALSE,Over-decomposition
ApplicationMasterProtocolPBClientImpl.allocate fails with EOFException when RPC privacy is enabled,https://issues.apache.org/jira/browse/HADOOP-14062,"When privacy is enabled for RPC (hadoop.rpc.protection = privacy), ApplicationMasterProtocolPBClientImpl.allocate sometimes (but not always) fails with an EOFException. I've reproduced this with Spark 2.0.2 built against latest branch-2.8 and with a simple distcp job on latest branch-2.8. 

Steps to reproduce using distcp: 

1. Set hadoop.rpc.protection equal to privacy 
2. Write data to HDFS. I did this with Spark as follows: 

sc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789"")).mkString(""|"")).toDF().repartition(100).write.parquet(""hdfs:///tmp/testData"") 
3. Attempt to distcp that data to another location in HDFS. For example: 

hadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy 
I observed this error in the ApplicationMaster's syslog: 

2016-12-19 19:13:50,097 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1482189777425_0004, File: hdfs://<namenode_host>:8020/tmp/hadoop-yarn/staging/<hdfs_user>/.staging/job_1482189777425_0004/job_1482189777425_0004_1.jhist 
2016-12-19 19:13:51,004 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0 
2016-12-19 19:13:51,031 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1482189777425_0004: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:22528, vCores:23> knownNMs=3 
2016-12-19 19:13:52,043 INFO [RMCommunicator Allocator] org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking ApplicationMasterProtocolPBClientImpl.allocate over null. Retrying after sleeping for 30000ms. 
java.io.EOFException: End of File Exception between local host is: ""<application_master_host>/<ip_addr>""; destination host is: ""<rm_host>"":8030; : java.io.EOFException; For more details see: http://wiki.apache.org/hadoop/EOFException 
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) 
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 
at java.lang.reflect.Constructor.newInstance(Constructor.java:422) 
at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801) 
at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765) 
at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486) 
at org.apache.hadoop.ipc.Client.call(Client.java:1428) 
at org.apache.hadoop.ipc.Client.call(Client.java:1338) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) 
at com.sun.proxy.$Proxy80.allocate(Unknown Source) 
at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:497) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335) 
at com.sun.proxy.$Proxy81.allocate(Unknown Source) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269) 
at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281) 
at java.lang.Thread.run(Thread.java:745) 
Caused by: java.io.EOFException 
at java.io.DataInputStream.readInt(DataInputStream.java:392) 
at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785) 
at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156) 
at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053) 
Marking as ""critical"" since this blocks YARN users from encrypting RPC in their Hadoop clusters.","B: When privacy is enabled for RPC (hadoop.rpc.protection = privacy), ApplicationMasterProtocolPBClientImpl.allocate sometimes fails with an EOFException.

B: The EOFException occurs intermittently when attempting to allocate resources via ApplicationMasterProtocolPBClientImpl.

B: The bug can be reproduced by setting hadoop.rpc.protection to privacy and running a distcp job.

B: Detailed steps to reproduce the bug include writing data to HDFS using Spark and then attempting to run a distcp command to copy that data.

B: The error is logged in the ApplicationMaster's syslog when the EOFException occurs during resource allocation.

B: The issue should be marked as ""critical"" because it blocks YARN users from encrypting RPC in their Hadoop clusters.",Critical,FALSE,Over-decomposition
Add privacy policy in web site,https://issues.apache.org/jira/browse/AMBARI-766,As the web site use google analytics privacy policy page must be added.,B: The website needs a Google Analytics privacy policy page added.,Major,TRUE,
Thrift Server 1 uses different QOP settings than RPC and Thrift Server 2 and can easily be misconfigured so there is no encryption when the operator expects it.,https://issues.apache.org/jira/browse/HBASE-17513,"As of HBASE-14400 the setting hbase.thrift.security.qop was unified to behave the same as the general HBase RPC protection. However, this only happened for the Thrift2 server. The Thrift server found in the thrift package (aka Thrift Server 1) still hard codes the old configs of 'auth', 'auth-int', and 'auth-conf'.

Additionally, these Quality of Protection (qop) settings are used only by the SASL transport. If a user configures the HBase Thrift Server to make use of the HTTP transport (to enable doAs proxying e.g. for Hue) then a QOP setting of 'privacy' or 'auth-conf' won't get them encryption as expected.

We should

1) update hbase-thrift/src/main/.../thrift/ThriftServerRunner to rely on SaslUtil to use the same 'authentication', 'integrity', 'privacy' configs in a backward compatible way
2) also have ThriftServerRunner warn when both hbase.thrift.security.qop and hbase.regionserver.thrift.http are set, since the latter will cause the former to be ignored. (users should be directed to hbase.thrift.ssl.enabled and related configs to ensure their transport is encrypted when using the HTTP transport.)","B: Update hbase-thrift/src/main/.../thrift/ThriftServerRunner to rely on SaslUtil to use the same 'authentication', 'integrity', 'privacy' configs in a backward compatible way.  
B: Modify ThriftServerRunner to warn when both hbase.thrift.security.qop and hbase.regionserver.thrift.http are set, directing users to hbase.thrift.ssl.enabled and related configs for encryption when using the HTTP transport.  
B: Ensure that the Thrift server found in the thrift package (Thrift Server 1) no longer hard codes the old configs of 'auth', 'auth-int', and 'auth-conf'.  
B: Investigate and clarify the behavior of QOP settings when the HBase Thrift Server is configured to use the HTTP transport, ensuring that expected encryption is achieved.",Critical,FALSE,Incorrect Interpretation of Solutions
Missing localization strings in the installer,https://issues.apache.org/jira/browse/FLEX-34392,"the following strings aren't localized 
Select AIR and Flash Player 
Select Flex Version 
Select AIR Version 
Select Flash Player Version 
anonymous usage statics will be collected in accordance with our privacy policy.","B: The string ""Select AIR and Flash Player"" isn't localized.  
B: The string ""Select Flex Version"" isn't localized.  
B: The string ""Select AIR Version"" isn't localized.  
B: The string ""Select Flash Player Version"" isn't localized.  
B: The string ""anonymous usage statics will be collected in accordance with our privacy policy."" isn't localized.",Minor,TRUE,
Use SaslUtil to set Sasl.QOP in 'Thrift',https://issues.apache.org/jira/browse/HBASE-19118,"In Configure the Thrift Gateway, it says ""set the property hbase.thrift.security.qop to one of the following three values: privacy, integrity, authentication"", which would lead to failure of starting up a thrift server. 
In fact, the value of hbase.thrift.security.qop should be auth, auth-int, auth-conf, according to the documentation of Sasl.QOP","B: In Configure the Thrift Gateway, the documentation incorrectly states that the property hbase.thrift.security.qop can be set to ""privacy, integrity, authentication"". 

B: The property hbase.thrift.security.qop should actually be set to ""auth, auth-int, auth-conf"" according to the documentation of Sasl.QOP. 

B: Starting up a thrift server fails when the incorrect values for hbase.thrift.security.qop are used.",Major,FALSE,Over-decomposition
Netbeans not allowed to access personnal files and folder under macOS,https://issues.apache.org/jira/browse/NETBEANS-5004,"With the new privacy and security features enforced macOS Catalina, Netbeans and any of its subprocesses are not allowed to access any of the personnal files and folders (e.g.: Downloads, Documents...).

Normally, a system prompt should ask the user when the application attempts to access such files, but here nothing happens. For instance, when I try to access the Document directory through the ""Open file..."" menu, this directory appears empty.

The bug seems to be related to the fact that the shell script used to launch Netbeans (/bin/sh [/Applications/NetBeans/Apache NetBeans) is sandboxed.

Giving full disk access to Netbeans in system preferences did not solve the issue.

The system console error related to this bug is attached to this ticket.","B: With the new privacy and security features enforced in macOS Catalina, Netbeans is not allowed to access personal files and folders (e.g.: Downloads, Documents...).

B: When Netbeans attempts to access the Document directory through the ""Open file..."" menu, the directory appears empty.

B: The shell script used to launch Netbeans (/bin/sh [/Applications/NetBeans/Apache NetBeans) appears to be sandboxed, which may be causing access issues.

B: Granting full disk access to Netbeans in system preferences did not resolve the access issue.",Major,FALSE,Incorrect Interpretation of Solutions
Convert all Lucene web properties to use the ASF CMS,https://issues.apache.org/jira/browse/LUCENE-2748,"The new CMS has a lot of nice features (and some kinks to still work out) and Forrest just doesn't cut it anymore, so we should move to the ASF CMS: http://apache.org/dev/cms.html","B: The new CMS has a lot of nice features that need to be documented.  
B: There are some kinks in the new CMS that need to be identified and resolved.  
B: Forrest is no longer suitable for our needs and should be deprecated.  
B: We should develop a migration plan to move to the ASF CMS.  
B: The ASF CMS documentation should be reviewed and understood before migration.",Major,FALSE,Over-decomposition
PutHDFS will pass files to success when they were not successfully written with hadoop client misconfiguration,https://issues.apache.org/jira/browse/NIFI-1062,"PutHDFS will create an empty file, but the data it is attempting to write will fail with this stack trace, and the flow file gets routed to success. 

2015-10-24 11:16:19,278 WARN [Thread-4674] org.apache.hadoop.hdfs.DFSClient DataStreamer Exception 
java.lang.IllegalArgumentException: null 
at javax.security.auth.callback.NameCallback.<init>(NameCallback.java:90) ~[na:1.8.0_45] 
at com.sun.security.sasl.digest.DigestMD5Client.processChallenge(DigestMD5Client.java:324) ~[na:1.8.0_45] 
at com.sun.security.sasl.digest.DigestMD5Client.evaluateChallenge(DigestMD5Client.java:220) ~[na:1.8.0_45] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.evaluateChallengeOrResponse(SaslParticipant.java:113) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:451) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getSaslStreams(SaslDataTransferClient.java:390) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:262) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:210) ~[na:na] 
at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:182) ~[na:na] 
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1413) ~[na:na] 
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1361) ~[na:na] 
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588) ~[na:na] 
2015-10-24 11:16:19,303 INFO [Timer-Driven Process Thread-2] o.apache.nifi.processors.hadoop.PutHDFS PutHDFS[id=3c30a474-86b4-45fc-b771-95d7a1b5054d] copied StandardFlowFileRecord[uuid=5a3deada-9739-474f-a83d-0447ad5aefd9,claim=StandardContentClaim [resourceClaim=StandardResourceClaim[id=1445694733273-1, container=default, section=1], offset=87, length=29],offset=0,name=x11.txt,size=29] to HDFS at /use/hdfs/x11.txt in 56 milliseconds at a rate of 511 bytes/sec","B: PutHDFS creates an empty file when it should write data.

B: The flow file gets routed to success even when there is an error in writing data.

B: An IllegalArgumentException occurs during the data writing process due to a null argument.

B: The stack trace indicates a failure in the SASL handshake process during data transfer.",Major,FALSE,Over-analysis
"When a cube built successfully with around 170 measures, some queries cannot be executed",https://issues.apache.org/jira/browse/KYLIN-1518,"A cube with the same data model, the same cube definition except the number of measures. If the number is around 150, the same query can be executed successfully. The error log is as follows: 

at org.apache.calcite.avatica.Helper.createException(Helper.java:41) 
at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:112) 
at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:130) 
at org.apache.kylin.rest.service.QueryService.execute(QueryService.java:361) 
at org.apache.kylin.rest.service.QueryService.queryWithSqlMassage(QueryService.java:276) 
at org.apache.kylin.rest.service.QueryService.query(QueryService.java:118) 
at org.apache.kylin.rest.service.QueryService$$FastClassByCGLIB$$4957273f.invoke(<generated>) 
at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618) 
at org.apache.kylin.rest.service.QueryService$$EnhancerByCGLIB$$ab2dbbe7.query(<generated>) 
at org.apache.kylin.rest.controller.QueryController.doQueryWithCache(QueryController.java:191) 
at org.apache.kylin.rest.controller.QueryController.query(QueryController.java:95) 
at sun.reflect.GeneratedMethodAccessor169.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:606) 
at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:213) 
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:126) 
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:96) 
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:617) 
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:578) 
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80) 
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:923) 
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:852) 
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:882) 
at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:789) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:646) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:727) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330) 
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118) 
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:105) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192) 
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160) 
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346) 
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:195) 
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:266) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) 
at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220) 
at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122) 
at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504) 
at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170) 
at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) 
at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950) 
at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) 
at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421) 
at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074) 
at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611) 
at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) 
at java.lang.Thread.run(Thread.java:745) 
Caused by: java.lang.IndexOutOfBoundsException: Index: 187, Size: 186 
at java.util.ArrayList.rangeCheck(ArrayList.java:635) 
at java.util.ArrayList.get(ArrayList.java:411) 
at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:914) 
at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:885) 
at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) 
at org.apache.calcite.rex.RexProgramBuilder.registerInput(RexProgramBuilder.java:274) 
at org.apache.calcite.rex.RexProgramBuilder.addProject(RexProgramBuilder.java:185) 
at org.apache.calcite.rex.RexProgram.create(RexProgram.java:215) 
at org.apache.calcite.rex.RexProgram.create(RexProgram.java:184) 
at org.apache.kylin.query.relnode.OLAPProjectRel.implementEnumerable(OLAPProjectRel.java:219) 
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:159) 
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:155) 
at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.createEnumerable(OLAPRel.java:155) 
at org.apache.kylin.query.relnode.OLAPToEnumerableConverter.implement(OLAPToEnumerableConverter.java:97) 
at org.apache.calcite.adapter.enumerable.EnumerableRelImplementor.implementRoot(EnumerableRelImplementor.java:99) 
at org.apache.calcite.adapter.enumerable.EnumerableInterpretable.toBindable(EnumerableInterpretable.java:92) 
at org.apache.calcite.prepare.CalcitePrepareImpl$CalcitePreparingStmt.implement(CalcitePrepareImpl.java:1050) 
at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:293) 
at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:188) 
at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:671) 
at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:572) 
at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:541) 
at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:173) 
at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:561) 
at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:477) 
at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:109) 
... 75 more","B: A cube with the same data model and cube definition except for the number of measures is causing an error when the number of measures is around 150.

B: The error log indicates a java.lang.IndexOutOfBoundsException with the message ""Index: 187, Size: 186"".

B: The error occurs in the method org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef.

B: The issue is related to the implementation of the OLAPProjectRel in org.apache.kylin.query.relnode.OLAPProjectRel.implementEnumerable.",Major,FALSE,Over-analysis
Statestore should garbage collect hung connections,https://issues.apache.org/jira/browse/IMPALA-1726,"If a node is truly hung, the statestore may apparently wait forever to receive the heartbeat response. We need to check the TCP timeouts on the connections from the statestore to the subscriber. 

Since the operating system can also interfere, we should periodically visit all heartbeat threads and see how long they've been in the heartbeat RPC for. I think we can forcibly close the socket in a GC thread if it's taken too long. The next time round should hit the TCP cnxn timeout (or be refused), and the subscriber should be marked as dead.","B: If a node is truly hung, the statestore may apparently wait forever to receive the heartbeat response.

B: Check the TCP timeouts on the connections from the statestore to the subscriber.

B: Periodically visit all heartbeat threads and see how long they've been in the heartbeat RPC for.

B: Forcibly close the socket in a GC thread if it has been in the heartbeat RPC for too long.

B: Ensure that the next time round hits the TCP connection timeout (or is refused) and mark the subscriber as dead.",Critical,FALSE,Over-decomposition
Yarn NodeManager OOM Listener Fails Compilation on Ubuntu 18.04,https://issues.apache.org/jira/browse/YARN-8498,"While building this project, I ran into a few compilation errors here. The first one was in this file: 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/impl/oom_listener_main.c 

At the very end, during the compilation of the OOM test, it fails again: 
hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:256:7: error: ¡®__WAIT_STATUS¡¯ was not declared in this scope 
__WAIT_STATUS mem_hog_status = {}; 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:257:30: error: ¡®mem_hog_status¡¯ was not declared in this scope 
__pid_t exited0 = wait(mem_hog_status); 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:275:21: error: expected ¡®;¡¯ before ¡®oom_listener_status¡¯ 
__WAIT_STATUS oom_listener_status = {}; 

hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc:276:30: error: ¡®oom_listener_status¡¯ was not declared in this scope 
__pid_t exited1 = wait(oom_listener_status);","B: Compilation error in the file hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/impl/oom_listener_main.c. 

B: Compilation error in the file hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc at line 256: error: ' __WAIT_STATUS' was not declared in this scope.

B: Compilation error in the file hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc at line 257: error: 'mem_hog_status' was not declared in this scope.

B: Compilation error in the file hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc at line 275: error: expected ';' before 'oom_listener_status'.

B: Compilation error in the file hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/oom-listener/test/oom_listener_test_main.cc at line 276: error: 'oom_listener_status' was not declared in this scope.",Blocker,FALSE,Over-analysis
Performance of DirectColorModel RGB bitmap images,https://issues.apache.org/jira/browse/XGC-71,When RGB bitmaps are used there can be performance improvements made to mitigate some of the impact.,"B: When RGB bitmaps are used, there can be performance improvements made.  
B: Mitigate some of the impact caused by using RGB bitmaps.",-,FALSE,Over-decomposition
activemq-client tests are failing with JDK17+,https://issues.apache.org/jira/browse/AMQ-9341,"activemq-client module doesn't build due to test failure: 

Caused by: java.net.BindException: Can't assign requested address 
at java.base/sun.nio.ch.Net.connect0(Native Method) 
at java.base/sun.nio.ch.Net.connect(Net.java:579) 
at java.base/sun.nio.ch.Net.connect(Net.java:568) 
at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593) 
at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) 
at java.base/java.net.Socket.connect(Socket.java:633) 
at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:304) 
at org.apache.activemq.transport.tcp.TcpTransport.connect(TcpTransport.java:525)","B: activemq-client module doesn't build due to test failure.  
B: java.net.BindException: Can't assign requested address.  
B: Failure occurs at java.base/sun.nio.ch.Net.connect0(Native Method).  
B: Failure occurs at java.base/sun.nio.ch.Net.connect(Net.java:579).  
B: Failure occurs at java.base/sun.nio.ch.Net.connect(Net.java:568).  
B: Failure occurs at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593).  
B: Failure occurs at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327).  
B: Failure occurs at java.base/java.net.Socket.connect(Socket.java:633).  
B: Failure occurs at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:304).  
B: Failure occurs at org.apache.activemq.transport.tcp.TcpTransport.connect(TcpTransport.java:525).",Major,FALSE,Over-analysis
Python DataFrame CSV load on large file is writing to console in Ipython,https://issues.apache.org/jira/browse/SPARK-14103,"I am using the spark from the master branch and when I run the following command on a large tab separated file then I get the contents of the file being written to the stderr

df = sqlContext.read.load(""temp.txt"", format=""csv"", header=""false"", inferSchema=""true"", delimiter=""\t"")
Here is a sample of output:

^M[Stage 1:> (0 + 2) / 2]16/03/23 14:01:02 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 2)
com.univocity.parsers.common.TextParsingException: Error processing input: Length of parsed input (1000001) exceeds the maximum number of characters defined in your parser settings (1000000). Identified line separator characters in the parsed content. This may be the cause of the error. The line separator in your parser settings is set to '\n'. Parsed content:
Privacy-shake"",: a haptic interface for managing privacy settings in mobile location sharing applications privacy shake a haptic interface for managing privacy settings in mobile location sharing applications 2010 2010/09/07 international conference on human computer interaction interact 43331058 19371[\n] 3D4F6CA1 Between the Profiles: Another such Bias. Technology Acceptance Studies on Social Network Services between the profiles another such bias technology acceptance studies on social network services 2015 2015/08/02 10.1007/978-3-319-21383-5_12 international conference on human-computer interaction interact 43331058 19502[\n]

.......

.........

web snippets 2008 2008/05/04 10.1007/978-3-642-01344-7_13 international conference on web information systems and technologies webist 44F29802 19489
06FA3FFA Interactive 3D User Interfaces for Neuroanatomy Exploration interactive 3d user interfaces for neuroanatomy exploration 2009 internationa]
at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:241)
at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:356)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:137)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:120)
at scala.collection.Iterator$class.foreach(Iterator.scala:742)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foreach(CSVParser.scala:120)
at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foldLeft(CSVParser.scala:120)
at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:212)
at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.aggregate(CSVParser.scala:120)
at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:69)
at org.apache.spark.scheduler.Task.run(Task.scala:82)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException
16/03/23 14:01:03 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
^M[Stage 1:> (0 + 1) / 2]


For a small sample (<10,000 lines) of the data, I am not getting any error. But as soon as I go above more than 100,000 samples, I start getting the error.

I don't think the spark platform should output the actual data to stderr ever as it decreases the readability.","B: When using Spark to read a large tab-separated file, the contents of the file are being written to stderr instead of being processed correctly.

B: An error occurs when processing a large tab-separated file, indicating that the length of the parsed input exceeds the maximum number of characters defined in the parser settings.

B: The line separator in the parser settings is set to '\n', which may be causing issues when processing long lines of data in the tab-separated file.

B: The Spark job fails with an ArrayIndexOutOfBoundsException when processing a large tab-separated file, specifically when the number of lines exceeds 100,000.

B: The Spark platform outputs the actual data to stderr during error handling, which decreases readability and should not happen.",Major,FALSE,Over-analysis
Missing menu items,https://issues.apache.org/jira/browse/ASFSITE-27,"The redesign has lost several links from the navigation: 

Privacy Page 
Incubator 
Memorials 
Conferences","B: The redesign has lost the link to the Privacy Page from the navigation.  
B: The redesign has lost the link to the Incubator from the navigation.  
B: The redesign has lost the link to the Memorials from the navigation.  
B: The redesign has lost the link to the Conferences from the navigation.",Major,TRUE,
Site-to-Site Transit URI is inconsistent,https://issues.apache.org/jira/browse/NIFI-2028,"Site-to-Site client and server create provenance event at both end, and those have Transit URI in it to record how flow files are transferred. However, the URI formats are inconsistent among RAW vs HTTP. 

Test result as follows: 

These port numbers are configurable in nifi.properties 
3080: Web API port (nifi.web.http.port) 
3081: Site-to-Site RAW Socket port (nifi.remote.input.socket.port) 
Before Fix 

PUSH - RAW 

Client - SEND: nifi://localhost:3081/flow-file-uuid 
Server - RECEIVE: nifi://localhost:3081/flow-file-uuid 
PULL - RAW 

Client - RECEIVE: nifi://localhost:3081flow-file-uuid 
Server - SEND: nifi://localhost:3081/flow-file-uuid 
PUSH - HTTP 

Client - SEND: http://127.0.0.1:3080/nifi-api/flow-file-uuid 
Server - RECEIVE: nifi://127.0.0.1:57390 
PULL - HTTP 

Client - RECEIVE: http://127.0.0.1:3080/flow-file-uuid 
Server - SEND: nifi://127.0.0.1:57673 
Issues 

PULL - RAW, Client - RECEIVE: lacks '/' in between port and flow-file uuid 
RAW uses server's host and port on both end (by transit url prefix), HTTP should follow this rule 
HTTP transit uri looks like REST endpoint but it is not a real endpoint. It should be an actual endpoint URI 
RAW uses hostname, while HTTP uses IP address 
After Fix 

PUSH - RAW 

Client - SEND: nifi://localhost:3081/flow-file-uuid 
Server - RECEIVE: nifi://localhost:3081/flow-file-uuid 
PULL - RAW 

Client - RECEIVE: nifi://localhost:3081/flow-file-uuid 
Server - SEND: nifi://localhost:3081/flow-file-uuid 
PUSH - HTTP 

Client - SEND: http://localhost:3080/nifi-api/data-transfer/input-ports/$port-id/transactions/$tx-id/flow-files 
Server - RECEIVE: http://localhost:3080/nifi-api/data-transfer/input-ports/$port-id/transactions/$tx-id/flow-files 
PULL - HTTP 

Client - RECEIVE: http://localhost:3080/nifi-api/data-transfer/output-ports/$port-id/transactions/$tx-id/flow-files 
Server - SEND: http://localhost:3080/nifi-api/data-transfer/output-ports/$port-id/transactions/$tx-id/flow-files","B: PULL - RAW, Client - RECEIVE: lacks '/' in between port and flow-file uuid.  
B: RAW uses server's host and port on both ends (by transit url prefix); HTTP should follow this rule.  
B: HTTP transit URI looks like a REST endpoint but it is not a real endpoint; it should be an actual endpoint URI.  
B: RAW uses hostname, while HTTP uses IP address.  
B: PUSH - HTTP transit URI should be consistent with the format used in RAW (use server's host and port).  
B: PULL - HTTP transit URI should be consistent with the format used in RAW (use server's host and port).",Major,FALSE,Over-decomposition
compat with pandas 0.20.0,https://issues.apache.org/jira/browse/ARROW-879,pandas changes the location of an import for ``DatetimeTZDtype``: http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#reorganization-of-the-library-privacy-changes,"B: pandas changes the location of an import for ``DatetimeTZDtype``.  
B: The documentation link provided needs to be updated to reflect the changes made in pandas.  
B: The reorganization of the library's privacy changes should be reviewed for potential impacts on existing code.",Blocker,FALSE,Over-decomposition
Unexpected subelement Name,https://issues.apache.org/jira/browse/AXIS2-1747,"When the generated stub processes response, it expects the nodes come in a specific order. If the order is different, exception is thrown. 

The code is in parse() methods of different result processing factory. 

For example, the code: 
/// process the ""first"" element ""IsPrivacyChanged"" 
if (reader.isStartElement() 
... 
""IsPrivacyChanged"").equals(reader.getName())) { 
read the value 
}else 
{ 
throw exception 
} 

///now it processes 2nd element. 
if (reader.isStartElement() 
... 
""Name"").equals(reader.getName())) { 
read the value 
}else 
{ 
throw exception 
} 

If ""Name"" came in before ""IsPrivacyChanged"", exception is thrown. 

A loop should be used, and any elements can appear in any order.","B: When the generated stub processes response, it expects the nodes to come in a specific order. If the order is different, an exception is thrown.

B: The exception is thrown in the parse() methods of different result processing factories when the nodes are not in the expected order.

B: The code processing the ""first"" element ""IsPrivacyChanged"" throws an exception if it does not find the element in the expected order.

B: The code processing the ""second"" element ""Name"" throws an exception if it does not find the element in the expected order.

B: A loop should be implemented to allow any elements to appear in any order without throwing an exception.",Major,FALSE,Over-analysis
query fails when calcite parse sql,https://issues.apache.org/jira/browse/KYLIN-1539,"There are two cases of query failure when calcite parses sql. However, both are solved by the same solution, just restarting kylin server.","B: There is a query failure case when calcite parses SQL.  

B: There is a second query failure case when calcite parses SQL.  

B: Both query failures can be resolved by restarting the Kylin server.",Major,FALSE,Incorrect Interpretation of Solutions
System views design leads to bad user expirience.,https://issues.apache.org/jira/browse/IGNITE-12921,"Current implementation of system views has broken system behavior which is related with querying system views. 

Before 2.8 system views were available via SQL queries (if indexing is enabled). It did not depend on any configuration. 

After implementation of IGNITE-12145 system views available only if SqlViewExporterSpi is passed to IgniteConfiguration.setSystemViewExporterSpi(). Now, if an user configures some SystemViewExporterSpi then provided user configuration will rewrite default configuration and SqlViewExporterSpi won't be initialized. As result it is impossible to query system views and any query to the views fails with exception. This behavior is not obvious for the user. See tests below. 

The second problem is kind of design problem. System view is internal part of the system and should be available regardless of any exporter configuration (at least via SQL) such as it was implemented before 2.8 release. 

My suggestion is the following: we should remove SqlViewExporterSpi and configure all views on indexing module initialization. SqlViewExporterSPI also doesn't make sense because: 

it operates by some internal API (SchemaManager, GridKernalContext, IgniteH2Indexing). 
it doesn't allow to end user to add any new system view. 
Only thing that could be useful is a filtering. But it could be done with SQL. 

Reproducer of broken behavior: 

package org.apache.ignite.internal.processors.cache.metric; 

import org.apache.ignite.cache.query.SqlFieldsQuery; 
import org.apache.ignite.cluster.ClusterState; 
import org.apache.ignite.configuration.DataRegionConfiguration; 
import org.apache.ignite.configuration.DataStorageConfiguration; 
import org.apache.ignite.configuration.IgniteConfiguration; 
import org.apache.ignite.internal.IgniteEx; 
import org.apache.ignite.spi.systemview.jmx.JmxSystemViewExporterSpi; 
import org.apache.ignite.testframework.junits.common.GridCommonAbstractTest; 
import org.junit.Test; 

import java.util.HashSet; 
import java.util.List; 
import java.util.Set; 

import static java.util.Arrays.asList; 
import static org.apache.ignite.internal.processors.cache.index.AbstractSchemaSelfTest.queryProcessor; 

public class SystemViewTest extends GridCommonAbstractTest { 

private static boolean useDefaultSpi; 

/** {@inheritDoc} */ 
@Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception { 
IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName); 

cfg.setConsistentId(igniteInstanceName); 

cfg.setDataStorageConfiguration(new DataStorageConfiguration() 
.setDataRegionConfigurations( 
new DataRegionConfiguration().setName(""in-memory"").setMaxSize(100L * 1024 * 1024)) 
.setDefaultDataRegionConfiguration( 
new DataRegionConfiguration() 
.setPersistenceEnabled(true))); 

if (!useDefaultSpi) { 
// Configure user provided system view exporter SPI. 
cfg.setSystemViewExporterSpi(new JmxSystemViewExporterSpi()); 
} 

return cfg; 
} 

/** 
* Will executed succefully. 
*/ 
@Test 
public void testSystemViewWithDefaultSpi() throws Exception { 
useDefaultSpi = true; 

doTestSystemView(); 
} 

/** 
* Will fail with <code>Table ""VIEWS"" not found</code>. 
*/ 
@Test 
public void testSystemViewWithCustomSpi() throws Exception { 
useDefaultSpi = false; 

doTestSystemView(); 
} 

private void doTestSystemView() throws Exception { 
try (IgniteEx ignite = startGrid()) { 
ignite.cluster().state(ClusterState.ACTIVE); 

Set<String> cacheNames = new HashSet<>(asList(""cache-1"", ""cache-2"")); 

for (String name : cacheNames) 
ignite.getOrCreateCache(name); 

SqlFieldsQuery qry = new SqlFieldsQuery(""SELECT * FROM SYS.VIEWS""); 

List<List<?>> res = queryProcessor(ignite).querySqlFields(qry, true).getAll(); 

res.forEach(item -> log.info(""VIEW FOUND: "" + item)); 
} 
} 

}","B: Current implementation of system views has broken system behavior related to querying system views.

B: System views were available via SQL queries before version 2.8 without any configuration dependency.

B: After the implementation of IGNITE-12145, system views are only available if SqlViewExporterSpi is passed to IgniteConfiguration.setSystemViewExporterSpi().

B: If a user configures a SystemViewExporterSpi, it rewrites the default configuration and prevents SqlViewExporterSpi from being initialized.

B: As a result of the above issue, it is impossible to query system views, and any query to these views fails with an exception.

B: The current behavior regarding system views is not obvious to the user.

B: System views should be available regardless of any exporter configuration, at least via SQL, similar to the behavior before the 2.8 release.

B: There is a design problem where SqlViewExporterSpi should be removed, and all views should be configured on indexing module initialization.

B: SqlViewExporterSpi operates by some internal API (SchemaManager, GridKernalContext, IgniteH2Indexing) and does not allow the end user to add any new system view.

B: Filtering could be done with SQL instead of using SqlViewExporterSpi.

B: The provided code snippet demonstrates the reproducer of the broken behavior in querying system views.",Critical,FALSE,Over-decomposition
TestFailpoints::test_failpoints crash in ARM build,https://issues.apache.org/jira/browse/IMPALA-11542,"Saw the crash in https://jenkins.impala.io/job/ubuntu-16.04-from-scratch-ARM/13 

In the ERROR log: 

Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30000,server=y,suspend=n 
impalad: /home/ubuntu/native-toolchain/source/llvm/llvm-5.0.1-asserts.src-p3/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldELF.cpp:400: void llvm::RuntimeDyldELF::resolveAArch64Relocation(const llvm::SectionEntry&, uint64_t, uint64_t, uint32_t, int64_t): Assertion `isInt<33>(Result) && ""overflow check failed for relocation""' failed. 
Minidump in thread [20013]exec-finstance (finst:1e4a0f56622f2a15:51c2970900000005) running query 1e4a0f56622f2a15:51c2970900000000, fragment instance 1e4a0f56622f2a15:51c2970900000005 
Wrote minidump to /home/ubuntu/Impala/logs/ee_tests/minidumps/impalad/de4830d8-009d-47f4-f14bb68a-f0d8cd4c.dmp 
In the INFO log: 

I0830 06:54:49.173234 11329 impala-beeswax-server.cc:516] query: Query { 
01: query (string) = ""SELECT STRAIGHT_JOIN *\n FROM alltypes t1\n JOIN /*+broadcast*/ alltypesagg t2 ON t1.id = t2.id\n WHERE t2.int_col < 1000"", 
03: configuration (list) = list<string>[10] { 
[0] = ""CLIENT_IDENTIFIE[...](273)"", 
[1] = ""TEST_REPLAN=1"", 
[2] = ""DISABLE_CODEGEN=False"", 
[3] = ""BATCH_SIZE=0"", 
[4] = ""NUM_NODES=0"", 
[5] = ""DISABLE_CODEGEN_ROWS_THRESHOLD=0"", 
[6] = ""MT_DOP=4"", 
[7] = ""ABORT_ON_ERROR=1"", 
[8] = ""DEBUG_ACTION=4:GETNEXT:MEM_LIMIT_EXCEEDED|COORD_BEFORE_EXEC_RPC:JITTER@100@0.3"", 
[9] = ""EXEC_SINGLE_NODE_ROWS_THRESHOLD=0"", 
}, 
04: hadoop_user (string) = ""ubuntu"", 
} 
... 
74: client_identifier (string) = ""failure/test_failpoints.py::TestFailpoints::()::test_failpoints[protocol:beeswax|table_format:seq/snap/block|exec_option:{'test_replan':1;'batch_size':0;'num_nodes':0;'disable_codegen_rows_threshold':0;'disable_codegen':False;'abort_on_error':1;'exec_sing"", 
... 
I0830 06:54:49.173739 11329 Frontend.java:1877] 1e4a0f56622f2a15:51c2970900000000] Analyzing query: SELECT STRAIGHT_JOIN * 
FROM alltypes t1 
JOIN /*+broadcast*/ alltypesagg t2 ON t1.id = t2.id 
WHERE t2.int_col < 1000 db: functional_seq_snap 
The client_identifier shows it's TestFailpoints::test_failpoints.","B: Crash occurs in the Impala server while executing a specific query, as indicated by the assertion failure in the ERROR log.

B: The ERROR log contains an assertion failure related to relocation overflow in the `RuntimeDyldELF` component of LLVM.

B: A minidump file is generated and written to the specified path due to the crash.

B: The INFO log shows the details of the executed query that caused the crash, including the query string and its configuration.

B: The client_identifier in the INFO log indicates that the failure is related to the `TestFailpoints::test_failpoints` test case.",Critical,FALSE,Over-decomposition
Value::Record containing enums fail to validate when using namespaces in Schema,https://issues.apache.org/jira/browse/AVRO-3674,"Consider the following schema: 

{ 
""type"": ""record"", 
""name"": ""NamespacedMessage"", 
""namespace"": ""com.domain"", 
""fields"": [ 
{ 
""type"": ""record"", 
""name"": ""field_a"", 
""fields"": [ 
{ 
""name"": ""enum_a"", 
""type"": { 
""type"": ""enum"", 
""name"": ""EnumType"", 
""symbols"": [ 
""SYMBOL_1"", 
""SYMBOL_2"" 
], 
""default"": ""SYMBOL_1"" 
} 
}, 
{ 
""name"": ""enum_b"", 
""type"": ""EnumType"" 
} 
] 
} 
] 
} 
I might represent this in Rust using the following structs: 

#[derive(Serialize)] 
enum EnumType { 
#[serde(rename = ""SYMBOL_1"")] 
Symbol1, 
#[serde(rename = ""SYMBOL_2"")] 
Symbol2, 
} 

#[derive(Serialize)] 
struct FieldA { 
enum_a: EnumType, 
enum_b: EnumType, 
} 

#[derive(Serialize)] 
struct NamespacedMessage { 
field_a: FieldA, 
} 

let msg = NamespacedMessage { 
field_a: FieldA { 
enum_a: EnumType::Symbol2, 
enum_b: EnumType::Symbol1, 
}, 
}; 
and then serialize this into a `Value` using the following logic: 

let mut ser = Serializer::default(); 
let test_value: Value = msg.serialize(&mut ser).unwrap(); 
After serializing into `test_value` I would expect that `test_value.validate(&schema)` yields True. However this is not the case. 

I can work around it by removing the `namespace` definition from my schema which allows the validation to proceed. 

I believe the cause of schema validation failure is [this lookup failing](https://github.com/apache/avro/blob/release-1.11.1-rc1/lang/rust/avro/src/types.rs#L370) when schemas are utilized as the `Value` will not have a namespace associated with it. 

I believe this could be fixed by altering `validate_internal` to accept an optional namespace that is derived from the provided schema to `validate`. However, I'm not sure if this is an appropriate fix.","B: The schema validation fails when a Rust `Value` is created from the `NamespacedMessage` struct due to the absence of a namespace in the serialized `Value`.

B: Removing the `namespace` definition from the schema allows the validation to succeed.

B: The failure in schema validation is caused by a lookup failing in the Avro library when the `Value` does not have an associated namespace.

B: A potential fix for the schema validation failure could involve modifying the `validate_internal` function to accept an optional namespace derived from the provided schema.",Major,FALSE,Incorrect Interpretation of Solutions
user context not passed down in fabric_view_all_docs,https://issues.apache.org/jira/browse/COUCHDB-3232,"We omitted to pass user_ctx down in fabric_view_all_docs. Since auth has happened beforehand this hasn't been an obvious issue, but it matters for the _users db as that reacts differently based on the user. couchdb intentionally hides design documents in that database from non-admins and intentionally hides the user docs of other users. 

passing the user ctx down fixes both issues.","B: We omitted to pass user_ctx down in fabric_view_all_docs. 

B: The omission of user_ctx has not been an obvious issue due to prior authentication.

B: The _users db reacts differently based on the user, which can lead to unintended access issues.

B: CouchDB intentionally hides design documents in the _users database from non-admins.

B: CouchDB intentionally hides the user documents of other users. 

B: Passing the user_ctx down fixes the issues related to document visibility in the _users db.",Major,FALSE,Over-decomposition
HBase server doesn't preserve SASL sequence number on the network,https://issues.apache.org/jira/browse/HBASE-22492,"When auth-conf is enabled on RPC, the server encrypt response in setReponse() using saslServer. The generated cryptogram included a sequence number manage by saslServer. But then, when the response is sent over the network, the sequence number order is not preserved. 

The client receives reply in the wrong order, leading to a log message from DigestMD5Base: 

sasl:1481 - DIGEST41:Unmatched MACs 

Then the message is discarded, leading the client to a timeout. 

I propose a fix here: https://github.com/sbarnoud/hbase-release/commit/ce9894ffe0e4039deecd1ed51fa135f64b311d41 

It seems that any HBase 1.x is affected. 

This part of code has been fully rewritten in HBase 2.x, and i haven't do the analysis on HBase 2.x which may be affected. 



Here, an extract of client log that i added to help me to understand: 

 

2019-05-28 12:53:48,644 DEBUG [Default-IPC-NioEventLoopGroup-1-32] NettyRpcDuplexHandler:80 - callId: 5846 /192.163.201.65:58870 -> dtltstap004.fr.world.socgen/192.163.201.72:16020 

2019-05-28 12:53:48,651 INFO [Default-IPC-NioEventLoopGroup-1-18] NioEventLoop:101 - SG: Channel ready to read 1315913615 unsafe 1493023957 /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 

2019-05-28 12:53:48,651 INFO [Default-IPC-NioEventLoopGroup-1-18] SaslUnwrapHandler:78 - SG: after unwrap:46 -> 29 for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 seqNum 150 

2019-05-28 12:53:48,652 DEBUG [Default-IPC-NioEventLoopGroup-1-18] NettyRpcDuplexHandler:192 - callId: 5801 received totalSize:25 Message:20 scannerSize:(null)/192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 

2019-05-28 12:53:48,652 INFO [Default-IPC-NioEventLoopGroup-1-18] sasl:1481 - DIGEST41:Unmatched MACs 

2019-05-28 12:53:48,652 WARN [Default-IPC-NioEventLoopGroup-1-18] SaslUnwrapHandler:70 - Sasl error (probably invalid MAC) detected for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 saslClient @4ac31121 ctx @14fb001d msg @140313192718406 len 118 data:1c^G?^P?3??h?k??????""??x?$^_??^D;^]7^Es??Em?c?w^R^BL?????????x??omG?z?I???45}???dE?^\^S>D?^????/4f?^^?? ?^E????d?????????D?kM^@^A^@^@^@? readerIndex 118 writerIndex 118 seqNum 152 
We can see that the client unwraps the Sasl message with sequence number 152 before sequence number 151 and fails with the unmatched MAC. 



I opened a case to Oracle because we should had an error (and not the message ignored). That's because the JDK doesn't controls integrity in the right way. 

https://github.com/openjdk/jdk/blob/master/src/java.security.sasl/share/classes/com/sun/security/sasl/digest/DigestMD5Base.java 

The actual JDK controls the HMac before the sequence number and hides the real error (bad sequence number) because SASL is stateful. The JDK should check FIRST the sequence number and THEN the HMac. 

When (and if) the JDK will be patched, and accordingly to https://www.ietf.org/rfc/rfc2831.txt , we will get an exception in that case instead of having the message ignored.","B: When auth-conf is enabled on RPC, the server encrypts the response in setResponse() using saslServer, but the sequence number order is not preserved when the response is sent over the network.

B: The client receives replies in the wrong order, leading to a log message from DigestMD5Base: sasl:1481 - DIGEST41:Unmatched MACs.

B: The unmatched MAC message leads to the response being discarded, causing the client to experience a timeout.

B: Any HBase 1.x version is affected by the issue with sequence number preservation in encrypted responses.

B: The code related to sequence number management in HBase 1.x has been fully rewritten in HBase 2.x, but the impact on HBase 2.x has not been analyzed.

B: The current JDK does not control integrity correctly, allowing messages to be ignored instead of throwing an error when a sequence number is invalid.

B: The JDK should check the sequence number before the HMac, and once patched, it is expected to throw an exception for invalid sequence numbers according to RFC 2831.",Major,FALSE,Over-decomposition
"Website ""site"" section links all inaccessible",https://issues.apache.org/jira/browse/KARAF-743,"The links pointed to on the ""site"" page: http://karaf.apache.org/site.html are all broken; I'm not sure why as the site/trunk repository does indeed have pages for all the links on the site page. This also affects the Apache ""Privacy Policy"" link at the very bottom of all the Karaf website pages.","B: The links pointed to on the ""site"" page: http://karaf.apache.org/site.html are all broken.  

B: The Apache ""Privacy Policy"" link at the very bottom of all the Karaf website pages is broken.  

B: The site/trunk repository has pages for all the links on the site page, but they are not working.",Minor,FALSE,Over-decomposition
Datanode#checkSecureConfig should allow SASL and privileged HTTP,https://issues.apache.org/jira/browse/HDFS-13081,"Datanode#checkSecureConfig currently check the following to determine if secure datanode is enabled. 

The server has bound to privileged ports for RPC and HTTP via SecureDataNodeStarter. 
The configuration enables SASL on DataTransferProtocol and HTTPS (no plain HTTP) for the HTTP server. 
Authentication of Datanode RPC server can be done either via SASL handshake or JSVC/privilege RPC port. 
This guarantees authentication of the datanode RPC server before a client transmits a secret, such as a block access token. 

Authentication of the HTTP server can also be done either via HTTPS/SSL or JSVC/privilege HTTP port. This guarantees authentication of datandoe HTTP server before a client transmits a secret, such as a delegation token. 

This ticket is open to allow privileged HTTP as an alternative to HTTPS to work with SASL based RPC protection. 

cc: cnauroth , daryn, jnpandey for additional feedback.","B: Datanode#checkSecureConfig needs to verify if the server has bound to privileged ports for RPC and HTTP via SecureDataNodeStarter.

B: Datanode#checkSecureConfig should ensure that the configuration enables SASL on DataTransferProtocol and HTTPS, and does not allow plain HTTP for the HTTP server.

B: Datanode#checkSecureConfig must check if authentication of the Datanode RPC server can be performed via SASL handshake or JSVC/privilege RPC port.

B: Datanode#checkSecureConfig needs to confirm that authentication of the HTTP server can be done via HTTPS/SSL or JSVC/privilege HTTP port.

B: This ticket is open to implement support for privileged HTTP as an alternative to HTTPS to work with SASL based RPC protection.",Major,TRUE,
RTF parser misses text content,https://issues.apache.org/jira/browse/TIKA-1713,"We have a lot of Outlook msg files that have RTF body content. Tika is not finding any text within these messages. It appears to be a mixture of RTF and HTML. 

I've extracted an example RTF body (see attachment) for use with the following test case: 

ByteArrayOutputStream bytes = new ByteArrayOutputStream() 
rtfParser.parse( 
this.class.getResourceAsStream(""/problems/no-text.rtf""), 
new EmbeddedContentHandler(new BodyContentHandler(bytes)), 
new Metadata(), new ParseContext() 
); 
assertTrue(""Document is missing required text"", bytes.toByteArray().length > 0)","B: Tika is not finding any text within Outlook msg files that have RTF body content.

B: The RTF body content appears to be a mixture of RTF and HTML.

B: Create a test case to verify that Tika can extract text from RTF files using the provided example RTF body.

B: Ensure that the output from the RTF parser is not an empty byte array when parsing the example RTF file.",Major,FALSE,Over-decomposition
NiFi Registry fails to push flow to GIT using SSH,https://issues.apache.org/jira/browse/NIFI-11927,"I have been successfully running NiFi Registry version 1.20.0 (and others before it) for about a year now. Now I needed to switch server that Registry runs on, since the old one was outdated, and decided to set up NiFi Registry version 1.22.0. 

The setup worked great and NiFi seems to have been running properly. However I recently noticed that it wasn't pushing anything to GIT. The changes are made, new versions are committed, they show up both in Registry GUI as well as the NiFi instance, but when I try to fetch some version it will not allow me giving me the following error: 



I started investigating the issue and noticed that the GIT repository that NiFi Registry was using was several versions ahead of origin, no version got pushed to the GIT repo. 

I investigated the logs and noticed following messages show up when I make new version in NiFi and commit it to registry: 

2023-08-10 07:52:03,317 WARN [NiFi Registry Web Server-18] javax.persistence.spi javax.persistence.spi::No valid providers found. 
2023-08-10 07:52:05,120 INFO [GitFlowMetaData Push thread] o.a.s.c.u.s.e.EdDSASecurityProviderRegistrar getOrCreateProvider(EdDSA) created instance of net.i2p.crypto.eddsa.EdDSASecurityProvider 
2023-08-10 07:52:05,326 INFO [GitFlowMetaData Push thread] o.a.s.c.i.DefaultIoServiceFactoryFactory No detected/configured IoServiceFactoryFactory; using Nio2ServiceFactoryFactory 
2023-08-10 07:52:05,351 ERROR [GitFlowMetaData Push thread] o.a.n.r.p.flow.git.GitFlowMetaData Failed to push commits to origin due to org.eclipse.jgit.api.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly 
org.eclipse.jgit.api.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly 
at org.eclipse.jgit.api.PushCommand.call(PushCommand.java:147) 
at org.apache.nifi.registry.provider.flow.git.GitFlowMetaData.lambda$startPushThread$1(GitFlowMetaData.java:299) 
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) 
at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) 
at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) 
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) 
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) 
at java.base/java.lang.Thread.run(Thread.java:833) 
Caused by: org.eclipse.jgit.errors.TransportException: git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git: remote hung up unexpectedly 
at org.eclipse.jgit.transport.TransportGitSsh$SshPushConnection.<init>(TransportGitSsh.java:382) 
at org.eclipse.jgit.transport.TransportGitSsh.openPush(TransportGitSsh.java:159) 
at org.eclipse.jgit.transport.PushProcess.execute(PushProcess.java:127) 
at org.eclipse.jgit.transport.Transport.push(Transport.java:1384) 
at org.eclipse.jgit.api.PushCommand.call(PushCommand.java:137) 
... 7 common frames omitted 
Caused by: java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()' 
at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:189) 
at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:142) 
at org.eclipse.jgit.transport.sshd.SshdSession.connect(SshdSession.java:99) 
at org.eclipse.jgit.transport.sshd.SshdSessionFactory.getSession(SshdSessionFactory.java:235) 
at org.eclipse.jgit.transport.sshd.SshdSessionFactory.getSession(SshdSessionFactory.java:1) 
at org.eclipse.jgit.transport.SshTransport.getSession(SshTransport.java:107) 
at org.eclipse.jgit.transport.TransportGitSsh$SshPushConnection.<init>(TransportGitSsh.java:358) 
... 11 common frames omitted 
I think the line Caused by: java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()' is what's causing the issue with pushing to GIT. This does not show up on the old Registry instance when I push to that. 



I checked the setup and compared it to the previous version running on old server. They have exactly the same configuration, save for new one running on Java 17 not 11 (although I tried running it on 11 too). 



The SSH keys are valid and recognized by our Gitlab instance, I can push manually as the nifi user, there are literally no differences between the configuration on one server compared to another. I even checked the configuration inside .git folder in the flow_storage directory, it was the same as on the other server. The ~/.ssh folder for nifi user has similar data between two servers, both have keys and known hosts set up, with right permissions, etc. 

I decided to try running version 1.20.0 of registry on new server, and it seems to have worked immediately, without any issue, it manages to push to GIT on its own, with no changes to config. I tested version 1.23 as well and it had the same issue, I haven't tested 1.21.0 though. 



For more information, here is the structure I have: 

©À©¤©¤ nifi-registry-1.20.0 
©¦ ©À©¤©¤ bin 
©¦ ©À©¤©¤ conf 
©¦ ©À©¤©¤ docs 
©¦ ©À©¤©¤ ext 
©¦ ©À©¤©¤ lib 
©¦ ©À©¤©¤ logs 
©¦ ©À©¤©¤ run 
©¦ ©¸©¤©¤ work 
©À©¤©¤ nifi-registry-1.23.0 
©¦ ©À©¤©¤ bin 
©¦ ©À©¤©¤ conf 
©¦ ©À©¤©¤ docs 
©¦ ©À©¤©¤ ext 
©¦ ©À©¤©¤ lib 
©¦ ©À©¤©¤ logs 
©¦ ©À©¤©¤ run 
©¦ ©¸©¤©¤ work 
©¸©¤©¤ nifi-registry-files 
©À©¤©¤ authorization-files 
©¦ ©À©¤©¤ authorizations.xml 
©¦ ©À©¤©¤ authorizers.xml 
©¦ ©À©¤©¤ login-identity-providers.xml 
©¦ ©¸©¤©¤ users.xml 
©À©¤©¤ certificate-files 
©¦ ©À©¤©¤ keystore.jks 
©¦ ©¸©¤©¤ truststore.jks 
©À©¤©¤ configuration-files 
©¦ ©À©¤©¤ providers.xml 
©¦ ©¸©¤©¤ registry-aliases.xml 
©À©¤©¤ database-drivers 
©¦ ©¸©¤©¤ mariadb-java-client-2.7.4.jar 
©À©¤©¤ extension-bundles 
©¸©¤©¤ flow-storage 
©¸©¤©¤ Buckets... 


Contents of the providers.xml: 

<providers> 
<flowPersistenceProvider> 
<class&amp;amp;gt;org.apache.nifi.registry.provider.flow.git.GitFlowPersistenceProvider</class&amp;amp;gt; 
<property name=""Flow Storage Directory"">/disk1/nifi-registry/nifi-registry-files/flow-storage</property> 
<property name=""Remote To Push"">origin</property> 
<property name=""Remote Access User""></property> 
<property name=""Remote Access Password""></property> 
<property name=""Remote Clone Repository""></property> 
</flowPersistenceProvider> 
<extensionBundlePersistenceProvider> 
<class&amp;amp;gt;org.apache.nifi.registry.provider.extension.FileSystemBundlePersistenceProvider</class&amp;amp;gt; 
<property name=""Extension Bundle Storage Directory"">/disk1/nifi-registry/nifi-registry-files/extension-bundles</property> 
</extensionBundlePersistenceProvider> 
</providers> 


Contents of the .git/config file in the flow_storage directory: 

[core] 
repositoryformatversion = 0 
filemode = true 
bare = false 
logallrefupdates = true 
[http] 
sslVerify = false 
proxy = http://<PROXY_URL>:3128 
[https] 
sslVerify = false 
proxy = http://<PROXY_URL>:3128 
[user] 
name = nifi_user 
email = OMITTED_FOR_PRIVACY 
[remote ""origin""] 
url = git@<OMITTED_FOR_PRIVACY>nifi/nifi-flows.git 
fetch = +refs/heads/*:refs/remotes/origin/* 
[branch ""master""] 
remote = origin 
merge = refs/heads/master","B: The NiFi Registry version 1.22.0 is not pushing changes to the GIT repository despite showing commits in the Registry GUI.

B: The logs show a warning message ""javax.persistence.spi::No valid providers found"" when committing new versions to the registry.

B: The logs indicate an error ""Failed to push commits to origin due to org.eclipse.jgit.api.errors.TransportException: remote hung up unexpectedly"" when attempting to push to GIT.

B: The error ""Caused by: java.lang.NoSuchMethodError: 'java.lang.Object org.apache.sshd.client.future.ConnectFuture.verify()'"" appears when pushing to GIT from NiFi Registry version 1.22.0.

B: The configuration for NiFi Registry version 1.22.0 is identical to that of the previous version running on the old server, except for the Java version.

B: The SSH keys are valid and recognized by the GitLab instance, allowing manual pushes as the nifi user.

B: NiFi Registry version 1.20.0 works correctly on the new server, able to push changes to GIT without any issues.

B: The user has not tested NiFi Registry version 1.21.0 on the new server, which may also have similar issues as version 1.22.0. 

B: The contents of the providers.xml file for the flowPersistenceProvider are set correctly, including the ""Flow Storage Directory"" and ""Remote To Push"" properties.

B: The contents of the .git/config file in the flow_storage directory are configured correctly for the remote repository, including the correct URL and fetch settings.",Major,FALSE,Over-decomposition
BasicHttpCacheStorage leaking variant keys in root response's variantMap,https://issues.apache.org/jira/browse/HTTPCLIENT-2284,"BasicHttpCacheStorage has a memory leak in the root response's variantMap. When a variant cached entry is evicted due to CacheMap being too large, the root cache entry does not remove the variant key in its variantMap. This is a memory leak that can grow the variantMap indefinitely, or until the root entry get's evicted itself. 

Simplified testcase: 

A request is being sent from a client that contains a header ""x-my-variant"" with a hash of the current timestamp. 
The server responds 200, with a cacheable response. The response Vary's on ""x-my-variant"" 
These requests repeat, causing: 
The ""root"" CacheEntry to be kept in CacheMap 
The oldest variant CacheEntry to be evicted due to the CacheMap size limit 
However the ""root"" CacheEntry never removes the evicted variant keys from the variantMap","B: BasicHttpCacheStorage has a memory leak in the root response's variantMap when a variant cached entry is evicted due to CacheMap being too large.

B: The root cache entry does not remove the variant key in its variantMap after a variant CacheEntry is evicted.

B: The memory leak in BasicHttpCacheStorage can grow the variantMap indefinitely if the root entry is not evicted.

B: A request is being sent from a client that contains a header ""x-my-variant"" with a hash of the current timestamp.

B: The server responds 200 with a cacheable response that varies on ""x-my-variant"".

B: The oldest variant CacheEntry is evicted due to the CacheMap size limit while the root CacheEntry is kept in CacheMap.",Minor,FALSE,Over-decomposition
Update the ubuntu version in the build instruction,https://issues.apache.org/jira/browse/HADOOP-17635,"In BUILDING.txt 

Installing required packages for clean install of Ubuntu 14.04 LTS Desktop: 
Ubuntu 14 is already EoL, should be updated to 18 or 20.","B: Installing required packages for clean install of Ubuntu 14.04 LTS Desktop.  
B: Ubuntu 14 is already EoL, should be updated to 18 or 20.",Major,FALSE,Over-decomposition
[Threat Modeling] Drillbit may be spoofed by an attacker and this may lead to data being written to the attacker's target instead of Drillbit,https://issues.apache.org/jira/browse/DRILL-5582,"Consider the scenario: 
Alice has a drillbit (my.drillbit.co) with plain and kerberos authentication enabled containing important data. Bob, the attacker, attempts to spoof the connection and redirect it to his own drillbit (fake.drillbit.co) with no authentication setup. 

When Alice is under attack and attempts to connect to her secure drillbit, she is actually authenticating against Bob's drillbit. At this point, the connection should have failed due to unmatched configuration. However, the current implementation will return SUCCESS as long as the (spoofing) drillbit has no authentication requirement set. 

Currently, the drillbit <- to -> drill client connection accepts the lowest authentication configuration set on the server. This leaves unsuspecting user vulnerable to spoofing.","B: Alice's drillbit (my.drillbit.co) should not authenticate successfully if the connecting drillbit (fake.drillbit.co) has no authentication setup.

B: The current implementation allows a connection to succeed if the spoofing drillbit has the lowest authentication configuration, which should be addressed.

B: The drillbit connection should fail when the authentication configurations do not match between the client and server.

B: The system should implement a check to ensure that any drillbit connection attempt requires at least one matching authentication method. 

B: Users should receive a clear error message when attempting to connect to a drillbit with mismatched authentication settings.",Minor,FALSE,Over-decomposition
org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl uses internal class com.sun.org.apache.xerces.internal.impl.dv.util.Base64,https://issues.apache.org/jira/browse/NETBEANS-5427,"Netbeans 12.3 fails to deploy on Tomcat when running under JDK 16 

SEVERE [org.openide.util.RequestProcessor]: Error in RequestProcessor org.openide.nodes.AsynchChildrenSEVERE [org.openide.util.RequestProcessor]: Error in RequestProcessor org.openide.nodes.AsynchChildrenjava.lang.IllegalAccessError: class org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl (in unnamed module @0x145afd71) cannot access class com.sun.org.apache.xerces.internal.impl.dv.util.Base64 (in module java.xml) because module java.xml does not export com.sun.org.apache.xerces.internal.impl.dv.util to unnamed module @0x145afd71 at org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl.run(TomcatManagerImpl.java:533) at org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl.list(TomcatManagerImpl.java:372) at org.netbeans.modules.tomcat5.deploy.TomcatManager.modules(TomcatManager.java:718) at org.netbeans.modules.tomcat5.deploy.TomcatManager.getRunningModules(TomcatManager.java:539) at org.netbeans.modules.tomcat5.ui.nodes.TomcatWebModuleChildrenFactory.createKeys(TomcatWebModuleChildrenFactory.java:107) at org.openide.nodes.AsynchChildren.run(AsynchChildren.java:190) at org.openide.util.RequestProcessor$Task.run(RequestProcessor.java:1418) at org.netbeans.modules.openide.util.GlobalLookup.execute(GlobalLookup.java:45) at org.openide.util.lookup.Lookups.executeWith(Lookups.java:278)[catch] at org.openide.util.RequestProcessor$Processor.run(RequestProcessor.java:2033)","B: Netbeans 12.3 fails to deploy on Tomcat when running under JDK 16.

B: org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl cannot access class com.sun.org.apache.xerces.internal.impl.dv.util.Base64 due to module java.xml not exporting the necessary package.

B: An IllegalAccessError occurs in org.netbeans.modules.tomcat5.deploy.TomcatManagerImpl when attempting to run the deployment.

B: The error in RequestProcessor is related to the inability to access the running modules in Tomcat.",Major,FALSE,Over-analysis
FileExistsException: Destination .. already exists when DiskFileItem.write was given an existing file,https://issues.apache.org/jira/browse/FILEUPLOAD-293,"Since 1.4, where FILEUPLOAD-248 was shipped, passing an existing file to DiskFileItem.write will cause an FileExistsException with the message ""Destination FILE already exist"", this prevents us from upgrading to 1.4 from 1.3.3. 



2019-02-20 01:12:56,504 http-nio-2990-exec-3 ERROR [|5ccb9b99-a96f-42ba-ad01-ac278516e1a4|] [IssueAttachmentsResource.privacy-safe] Error saving attachment 
org.apache.commons.io.FileExistsException: Destination '/buildeng/bamboo-agent-home/xml-data/build-dir/CLOUDRELEASE-AGILEWD15421-FT18/jira-test-runner-jira/target/cargo/configurations/tomcat9x/temp/attachment-3404789743778163937.tmp' already exists 
{{ at org.apache.commons.io.FileUtils.moveFile(FileUtils.java:3001)}} 
{{ at org.apache.commons.fileupload.disk.DiskFileItem.write(DiskFileItem.java:405)}} 
{{ at com.atlassian.plugins.rest.common.multipart.fileupload.CommonsFileUploadFilePart.write(CommonsFileUploadFilePart.java:49)}} 
{{ at com.atlassian.jira.rest.v2.issue.IssueAttachmentsResource.getFileFromFilePart(IssueAttachmentsResource.java:175)}} 
... 



Apache Felix also ran into the same bug: 
https://issues.apache.org/jira/browse/FELIX-6037","B: Since version 1.4, passing an existing file to DiskFileItem.write causes a FileExistsException with the message ""Destination FILE already exist"".

B: The FileExistsException prevents upgrading from version 1.3.3 to 1.4.

B: An error occurs when saving an attachment, specifically indicating that the destination file already exists.

B: The error log shows that the exception originates from the moveFile method in FileUtils.java.

B: The bug has also been reported in Apache Felix, linked to issue FELIX-6037.",Major,FALSE,Over-decomposition
Fix serialization of structs containing Fixed fields,https://issues.apache.org/jira/browse/AVRO-3631,"Consider the following minimal Avro Schema: 

{ 
""type"": ""record"", 
""name"": ""TestStructFixedField"", 
""fields"": [ 
{ 
""name"": ""field"", 
""type"": { 
""name"": ""field"", 
""type"": ""fixed"", 
""size"": 6 
} 
} 
] 
} 
In Rust, I might represent this schema with the following struct: 

#[derive(Debug, Serialize, Deserialize)] 
struct TestStructFixedField { 
field: [u8; 6] 
} 
I would then expect to be able to use `apache_avro::to_avro_datum()` to convert an instance of `TestStructFixedField` into an `Vec<u8>` using an instance of `Schema` initialized from the schema listed above. 

However, this fails because the `Value` produced by `apache_avro::to_value()` represents `field` as an `Value::Array<Value::Int>` rather than a `Value::Fixed<6, Vec<u8>` which does not pass schema validation. 

I believe that there are two options to fix this: 
1. Allow Value::Array<Vec<Value::Int>> to pass validation if the array has the expected length, and none of the contents of the array are out-of-range for u8. If we go down this route, the implementation of `to_avro_datum()` will have to take care of converting Value::Int to u8 when converting into bytes. 
2. Update `apache_avro::to_value()` such that fixed length arrays are converted into `Value::Fixed<N, Vec<u8>>` rather than `Value::Array`.","B: The `apache_avro::to_value()` function produces a `Value::Array<Value::Int>` for fixed fields, which should instead produce `Value::Fixed<6, Vec<u8>>`.  

B: The schema validation fails because the `Value` produced does not match the expected type for fixed fields.  

B: Implement a solution to allow `Value::Array<Vec<Value::Int>>` to pass validation if the array has the expected length and contains valid u8 values.  

B: Modify the `to_avro_datum()` implementation to convert `Value::Int` to `u8` when converting into bytes.  

B: Update `apache_avro::to_value()` to ensure fixed-length arrays are converted into `Value::Fixed<N, Vec<u8>>` instead of `Value::Array`.",Major,FALSE,Over-decomposition
Enabling ranger plugin config should modify dependent configs,https://issues.apache.org/jira/browse/AMBARI-9626,"Changes required for enabling Ranger plugin 
HDFS 

Property Value File 
dfs.permissions.enabled true hdfs-site.xml 
HIVE 

Property Value File 
hive.security.authorization.enabled true hive-site.xml 
hive.security.authorization.manager com.xasecure.authorization.hive.authorizer.XaSecureHiveAuthorizerFactory hiveserver2-site.xml 
hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator hiveserver2-site.xml 
hive.conf.restricted.list Must contain all elements of hive.security.authorization.enabled, hive.security.authorization.manager,hive.security.authenticator.manager hive-site.xml 
HBASE 

Property Value File 
hbase.security.authorization true hbase-site.xml 
hbase.coprocessor.master.classes Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor and add if not present hbase-site.xml 
hbase.coprocessor.region.classes Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor hbase-site.xml 
hbase.rpc.protection privacy hbase-site.xml 
KNOX 
Replace instances of AclsAuthz with XASecurePDPKnox in topology.xml 

STORM 

Property Value File 
nimbus.authorizer com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer storm.yaml 
Note that nimbus.authorizer should be added only when the cluster is already Kerberized; having this property in a non-Kerberized cluster causes Storm to fail. 

Changes required for disabling Ranger plugin 
HDFS 

Property Value File 
HIVE 

Property Value File 
hive.security.authorization.manager org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory hiveserver2-site.xml 
hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator hiveserver2-site.xml 
HBASE 

Property Value File 
hbase.coprocessor.master.classes Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor hbase-site.xml 
hbase.coprocessor.region.classes Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor hbase-site.xml 
hbase.rpc.protection authentication hbase-site.xml 
KNOX 
Replace instance of XASecurePDPKnox with AclsAuthz in topology.xml 

STORM 

Property Value File 
nimbus.authorizer backtype.storm.security.auth.authorizer.SimpleACLAuthorizer com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer storm.yaml 
Note that nimbus.authorizer should be added only when the cluster is already Kerberized; having this property in a non-Kerberized cluster causes Storm to fail.","B: Changes required for enabling Ranger plugin in HDFS: Set dfs.permissions.enabled to true in hdfs-site.xml.  
B: Changes required for enabling Ranger plugin in HIVE: Set hive.security.authorization.enabled to true in hive-site.xml.  
B: Changes required for enabling Ranger plugin in HIVE: Set hive.security.authorization.manager to com.xasecure.authorization.hive.authorizer.XaSecureHiveAuthorizerFactory in hiveserver2-site.xml.  
B: Changes required for enabling Ranger plugin in HIVE: Set hive.security.authenticator.manager to org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator in hiveserver2-site.xml.  
B: Changes required for enabling Ranger plugin in HIVE: Ensure hive.conf.restricted.list contains hive.security.authorization.enabled, hive.security.authorization.manager, and hive.security.authenticator.manager in hive-site.xml.  
B: Changes required for enabling Ranger plugin in HBASE: Set hbase.security.authorization to true in hbase-site.xml.  
B: Changes required for enabling Ranger plugin in HBASE: Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor in hbase-site.xml for hbase.coprocessor.master.classes.  
B: Changes required for enabling Ranger plugin in HBASE: Replace org.apache.hadoop.hbase.security.access.AccessController with com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor in hbase-site.xml for hbase.coprocessor.region.classes.  
B: Changes required for enabling Ranger plugin in HBASE: Set hbase.rpc.protection to privacy in hbase-site.xml.  
B: Changes required for enabling Ranger plugin in KNOX: Replace instances of AclsAuthz with XASecurePDPKnox in topology.xml.  
B: Changes required for enabling Ranger plugin in STORM: Set nimbus.authorizer to com.xasecure.authorization.storm.authorizer.XaSecureStormAuthorizer in storm.yaml, only if the cluster is already Kerberized.  

B: Changes required for disabling Ranger plugin in HIVE: Set hive.security.authorization.manager to org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory in hiveserver2-site.xml.  
B: Changes required for disabling Ranger plugin in HBASE: Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor from hbase-site.xml for hbase.coprocessor.master.classes.  
B: Changes required for disabling Ranger plugin in HBASE: Remove com.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor from hbase-site.xml for hbase.coprocessor.region.classes.  
B: Changes required for disabling Ranger plugin in HBASE: Set hbase.rpc.protection to authentication in hbase-site.xml.  
B: Changes required for disabling Ranger plugin in KNOX: Replace instance of XASecurePDPKnox with AclsAuthz in topology.xml.  
B: Changes required for disabling Ranger plugin in STORM: Set nimbus.authorizer to backtype.storm.security.auth.authorizer.SimpleACLAuthorizer in storm.yaml, only if the cluster is already Kerberized.",Major,FALSE,Over-decomposition
KnoxSSO broken on recent Chrome browsers (version > 80),https://issues.apache.org/jira/browse/KNOX-2387,"Google chrome changed the default behavior of SameSite parameter in Set-Cookie header from None to Lax. This causes partial breakage of Knox SSO. 

Details about Chrome browser feature - https://www.chromestatus.com/feature/5088147346030592 

How it affects - https://support.okta.com/help/s/article/FAQ-How-Chrome-80-Update-for-SameSite-by-default-Potentially-Impacts-Your-Okta-Environment","B: Google Chrome changed the default behavior of the SameSite parameter in the Set-Cookie header from None to Lax.

B: This change causes partial breakage of Knox SSO.",Major,FALSE,Over-decomposition
"Infinite loop on server disconnect",https://issues.apache.org/jira/browse/HTTPCORE-528,"I am seeing HTTP NIO client get into an infinite loop after the server disconnections the connection. See the log output attached; note some content removed for privacy. 



Note that the HttpAsyncClient has returned the response and the application has completely processed it. The client maintains the connection, as expected, since the response included `Keep-Alive: timeout=5`. Five seconds after everything is complete, the server closes the TCP connection. The client reacts accordingly: the selector wakes up, does a read of -1 bytes, closes the session and sets a 1 second timeout to close the connection in. 



The infinite loop occurs because the selector.select() call constantly returns in AbstractIOReactor.execute() 

readyCount == 1, so events are processed 

processEvent() notes the key is readable and calls: 

session.resetLastRead() 

readable(key); 

Because resetLastRead() is constantly updated, the 1 second timeout is never reached and AbstractIOReactor.timeoutCheck() can never call sessionTimedOut() or close the connection. 



Note the entire time this is happening, netstat shows the connection is in CLOSE_WAIT state. The infinite loop continues until the OS keepalive timeout is reached and the connection is cleaned by the OS. 

I am not sure if this epoll / selector behavior is expected or not. However, I have replicated this issue in multiple environments. It seems like the async client should handle this by detecting the condition and closing the connection. 



Other notes from this infinite loop state: 

SSLIOSession.updateEventMask() never closes the session either since the state remains `CLOSING` 

AbstractIODispatch.inputReady() does not read any data from the connection since ssliosession.isAppInputReady() evaluates to false. 

SelectionKeyImpl.interestOps remains 1 (`OP_READ`)","B: HTTP NIO client enters an infinite loop after the server closes the TCP connection, preventing the connection from being closed properly.

B: The selector.select() call in AbstractIOReactor.execute() continuously returns readyCount == 1, causing events to be processed indefinitely.

B: The resetLastRead() method is constantly updated, preventing the 1 second timeout from being reached in the client.

B: The sessionTimedOut() method in AbstractIOReactor.timeoutCheck() is never called, resulting in the connection not being closed.

B: The netstat output shows the connection is in a CLOSE_WAIT state while the infinite loop is ongoing.

B: The SSLIOSession.updateEventMask() method does not close the session because the state remains CLOSING.

B: The AbstractIODispatch.inputReady() method does not read any data from the connection since ssliosession.isAppInputReady() evaluates to false.

B: The SelectionKeyImpl.interestOps remains set to 1 (OP_READ), indicating that the read interest is still active.",Major,FALSE,Over-decomposition
Improve the way job history files are managed,https://issues.apache.org/jira/browse/MAPREDUCE-323,"Today all the jobhistory files are dumped in one job-history folder. This can cause problems when there is a need to search the history folder (job-recovery etc). It would be nice if we group all the jobs under a user folder. So all the jobs for user amar will go in history-folder/amar/. Jobs can be categorized using various features like jobid, date, jobname etc but using username will make the search much more efficient and also will not result into namespace explosion.","B: Create a user-specific folder structure for job-history files, organizing them under a main history folder.  

B: Implement functionality to categorize job-history files by username, ensuring that all jobs for a user are stored in their respective folder.  

B: Ensure that the new folder structure does not lead to namespace explosion while maintaining efficient search capabilities.  

B: Develop a search feature that allows for efficient retrieval of job-history files based on the user folder structure.  

B: Assess the impact of categorizing job-history files by username on job recovery processes.",Critical,FALSE,Over-decomposition
Updated yajl-ruby ~> 1.3.1,https://issues.apache.org/jira/browse/SAMZA-1582,"Copying the reminder from Apache Security team:

Sign in
gstein,

We found a potential security vulnerability in a repository which you have been granted security alert access.

!https://ci4.googleusercontent.com/proxy/LhcZ7iaFoInSfQy1r_J1HOWPMTtxYnupwChIFTSA_wTfxDY3HgcGigfxusXNNAZ63YSBgrW9Ng_0lnJNyuw-HZe8OlzcTvPaKX4OXHI=s0-d-e1-ft#https://avatars3.githubusercontent.com/u/47359?s=56&v=4 width=28,height=28! apache/samza
Known high severity security vulnerability detected in {{yajl-ruby < 1.3.1}}defined in Gemfile.lock.
Gemfile.lock update suggested: yajl-ruby ~> 1.3.1.
Always verify the validity and compatibility of suggestions with your codebase.

Review vulnerable dependency
Only users who have been assigned access to security alerts will receive these notifications.
Unsubscribe ¡¤ Email preferences ¡¤ Terms ¡¤ Privacy ¡¤ Sign into GitHub
GitHub, Inc. 
88 Colin P Kelly Jr St. 
San Francisco, CA 94107

{quote}","B: Known high severity security vulnerability detected in {{yajl-ruby < 1.3.1}} defined in Gemfile.lock.  
B: Gemfile.lock update suggested: yajl-ruby ~> 1.3.1.  
B: Always verify the validity and compatibility of suggestions with your codebase.",Major,FALSE,Over-decomposition
Fix HBASE-22492 on branch-2 (SASL GapToken),https://issues.apache.org/jira/browse/HBASE-25586,"The issue is also exist on branch-2. 

17:27:41.556 [pool-1-thread-8] WARN org.apache.hadoop.hbase.client.ScannerCallable - Ignore, probably already closed. Current scan: {""startRow"":""19999999"",""stopRow"":"""",""batch"":20,""cacheBlocks"":true,""totalColumns"":0,""maxResultSize"":""2097152"",""families"":{},""caching"":2147483647,""maxVersions"":1,""timeRange"":[""0"",""9223372036854775807""]} on table: cluster_test 
javax.security.sasl.SaslException: Call to XXX/172.27.162.2:22101 failed on local exception: javax.security.sasl.SaslException: Gap token 
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) 
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 
at java.lang.reflect.Constructor.newInstance(Constructor.java:423) 
at org.apache.hadoop.hbase.ipc.IPCUtil.wrapException(IPCUtil.java:224) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:383) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414) 
at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410) 
at org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:117) 
at org.apache.hadoop.hbase.ipc.Call.setException(Call.java:132) 
at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.cleanupCalls(NettyRpcDuplexHandler.java:203) 
at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.exceptionCaught(NettyRpcDuplexHandler.java:220) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273) 
at org.apache.hbase.thirdparty.io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:143) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:381) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) 
at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) 
at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) 
at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) 
at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) 
at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) 
at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.lang.Thread.run(Thread.java:748) 
Caused by: javax.security.sasl.SaslException: Gap token 
at com.sun.security.sasl.gsskerb.GssKrb5Base.checkMessageProp(GssKrb5Base.java:142) 
at com.sun.security.sasl.gsskerb.GssKrb5Base.unwrap(GssKrb5Base.java:81) 
at org.apache.hadoop.hbase.security.SaslUnwrapHandler.channelRead0(SaslUnwrapHandler.java:52) 
at org.apache.hadoop.hbase.security.SaslUnwrapHandler.channelRead0(SaslUnwrapHandler.java:33) 
at org.apache.hbase.thirdparty.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) 
at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
... 20 common frames omitted 
The is can be reproduced with the attached code. hbase.rpc.protection must be set to integrity or privacy.","B: The issue exists on branch-2.

B: A warning is logged indicating that a scan is being ignored because it is probably already closed, with the following details: {""startRow"":""19999999"",""stopRow"":"""",""batch"":20,""cacheBlocks"":true,""totalColumns"":0,""maxResultSize"":""2097152"",""families"":{},""caching"":2147483647,""maxVersions"":1,""timeRange"":[""0"",""9223372036854775807""]} on table: cluster_test.

B: A javax.security.sasl.SaslException occurs with the message ""Call to XXX/172.27.162.2:22101 failed on local exception: javax.security.sasl.SaslException: Gap token"".

B: The error stack trace shows multiple calls leading to the exception in IPCUtil.wrapException and AbstractRpcClient.onCallFinished.

B: The bug can be reproduced with the attached code, and it requires that hbase.rpc.protection must be set to integrity or privacy.",Major,FALSE,Over-decomposition
CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest is broken on Fedora 25.,https://issues.apache.org/jira/browse/MESOS-7049,"Test output: 

[==========] Running 1 test from 1 test case. 
[----------] Global test environment set-up. 
[----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest 
[ RUN ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
../../src/tests/containerizer/cgroups_tests.cpp:1020: Failure 
(statistics).failure(): Failed to parse perf sample: Failed to parse perf sample line '6186960975,,cycles,mesos_test,2000511515,100.00,3.093,GHz': Unexpected number of fields 
../../src/tests/containerizer/cgroups_tests.cpp:193: Failure 
(cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy 
[ FAILED ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest (2123 ms) 
[----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest (2123 ms total) 

[----------] Global test environment tear-down 
../../src/tests/environment.cpp:836: Failure 
Failed 
Tests completed with child processes remaining: 
-+- 20455 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
\--- 20500 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
[==========] 1 test from 1 test case ran. (2141 ms total) 
[ PASSED ] 0 tests. 
[ FAILED ] 1 test, listed below: 
[ FAILED ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest 
Software versions: 

[jpeach@jpeach src]$ uname -a 
Linux jpeach.apple.com 4.9.6-200.fc25.x86_64 #1 SMP Thu Jan 26 10:17:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 
[jpeach@jpeach src]$ perf -v 
perf version 4.9.6.200.fc25.x86_64.g51a0 
[jpeach@jpeach src]$ cat /etc/os-release 
NAME=Fedora 
VERSION=""25 (Workstation Edition)"" 
ID=fedora 
VERSION_ID=25 
PRETTY_NAME=""Fedora 25 (Workstation Edition)"" 
ANSI_COLOR=""0;34"" 
CPE_NAME=""cpe:/o:fedoraproject:fedora:25"" 
HOME_URL=""https://fedoraproject.org/"" 
BUG_REPORT_URL=""https://bugzilla.redhat.com/"" 
REDHAT_BUGZILLA_PRODUCT=""Fedora"" 
REDHAT_BUGZILLA_PRODUCT_VERSION=25 
REDHAT_SUPPORT_PRODUCT=""Fedora"" 
REDHAT_SUPPORT_PRODUCT_VERSION=25 
PRIVACY_POLICY_URL=https://fedoraproject.org/wiki/Legal:PrivacyPolicy 
VARIANT=""Workstation Edition"" 
VARIANT_ID=workstation 
The test then fails to clean up, leaving stale processes and cgroups.","B: ../../src/tests/containerizer/cgroups_tests.cpp:1020: Failure (statistics).failure(): Failed to parse perf sample: Failed to parse perf sample line '6186960975,,cycles,mesos_test,2000511515,100.00,3.093,GHz': Unexpected number of fields.  
B: ../../src/tests/containerizer/cgroups_tests.cpp:193: Failure (cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy.  
B: Tests completed with child processes remaining: -+- 20455 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest \--- 20500 /home/jpeach/upstream/mesos/build/src/.libs/mesos-tests --verbose --gtest_filter=CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_PERF_PerfTest.  
B: The test then fails to clean up, leaving stale processes and cgroups.",Major,FALSE,Over-decomposition
NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/ColumnVector,https://issues.apache.org/jira/browse/RANGER-4178,"Observed below error when enabled audit type as ORC format. 

NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/ColumnVector 

https://issues.apache.org/jira/browse/RANGER-1837 

https://issues.apache.org/jira/browse/RANGER-3235 

cc: rmani","B: Observed NoClassDefFoundError when enabled audit type as ORC format.  

B: The error indicates that the class org/apache/hadoop/hive/ql/exec/vector/ColumnVector is not found. 

B: Investigate if the necessary dependencies for the ORC format are included in the project. 

B: Check if the correct version of Hadoop is being used that contains the required class.",Critical,FALSE,Over-decomposition
Race between FeatureService and ConfigAdmin for resolving mvn: URLs?,https://issues.apache.org/jira/browse/KARAF-910,"I have an intermittent problem where my custom features.xml cannot be resolved. I use a tweaked etc/org.ops4j.pax.url.mvn.cfg file so the features.xml file is never present in $HOME/.m2/repo but is instead is resolved to local repo relative to the app: 

org.ops4j.pax.url.mvn.defaultRepositories=file:${karaf.base}/system@snapshots,\ 
file:${karaf.home}/${karaf.default.repository}@snapshots,\ 
file:${karaf.base}/../../../.env/.m2/repo@snapshots 

Sometimes when I start Karaf, I get this error (actual URL edited for privacy) 

karaf@tsf> 2011-09-30 09:23:09,760 WARN [FeaturesServiceImpl.java:924] Unable to add features repository mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features at startup - o.a.k.f.i.FeaturesServiceImpl 
java.lang.RuntimeException: URL [mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features] could not be resolved. 
at org.ops4j.pax.url.mvn.internal.Connection.getInputStream(Connection.java:195) [na:na] 
at org.ops4j.pax.url.mvn.internal.AetherBridgeConnection.getInputStream(AetherBridgeConnection.java:68) [na:na] 
at org.apache.karaf.features.internal.FeatureValidationUtil.validate(FeatureValidationUtil.java:49) [na:na] 
at org.apache.karaf.features.internal.FeaturesServiceImpl.validateRepository(FeaturesServiceImpl.java:199) [na:na] 
at org.apache.karaf.features.internal.FeaturesServiceImpl.internalAddRepository(FeaturesServiceImpl.java:210) [na:na] 
at org.apache.karaf.features.internal.FeaturesServiceImpl.start(FeaturesServiceImpl.java:922) [na:na] 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [na:1.6.0_26] 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) [na:1.6.0_26] 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) [na:1.6.0_26] 
at java.lang.reflect.Method.invoke(Method.java:597) [na:1.6.0_26] 
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:226) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:824) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:636) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:724) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:640) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:331) [org.apache.aries.blueprint:0.3.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:227) [org.apache.aries.blueprint:0.3.1] 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [na:1.6.0_26] 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_26] 
at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_26] 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) [na:1.6.0_26] 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206) [na:1.6.0_26] 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_26] 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_26] 
at java.lang.Thread.run(Thread.java:662) [na:1.6.0_26] 

If I put a breakpoint in org.ops4j.pax.url.mvn.internal.Connection.getInputStream(), I can see that when it fails m_configuration.getDefaultRepositories() contains one repo ($HOME/.m2/repo) and when it succeeds m_configuration.getDefaultRepositories() contains the three repos I've specified in etc/org.ops4j.pax.url.mvn.cfg. 

I interpret that to mean that sometimes the features resolution happens before Felix reads the files in etc/ and sometimes the features load afterward. Mostly I'm using the same startlevels as Karaf _ my startup.properties file is identical to the following except for a few additions I made. 

http://svn.apache.org/viewvc/karaf/trunk/assemblies/apache-karaf/src/main/filtered-resources/etc/startup.properties?revision=1176017&view=markup","B: I have an intermittent problem where my custom features.xml cannot be resolved.

B: I use a tweaked etc/org.ops4j.pax.url.mvn.cfg file so the features.xml file is never present in $HOME/.m2/repo but is instead resolved to local repo relative to the app.

B: Sometimes when I start Karaf, I get a warning that says ""Unable to add features repository mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features at startup"".

B: The warning message indicates that a java.lang.RuntimeException occurs because the URL [mvn:<my-group-id>/<my-artifact-id>/<my-version>/xml/features] could not be resolved.

B: When the issue occurs, m_configuration.getDefaultRepositories() contains one repo ($HOME/.m2/repo).

B: When the issue does not occur, m_configuration.getDefaultRepositories() contains the three repos specified in etc/org.ops4j.pax.url.mvn.cfg.

B: I interpret that sometimes the features resolution happens before Felix reads the files in etc/ and sometimes it happens afterward.

B: My startup.properties file is identical to the default except for a few additions I made.",Blocker,FALSE,Over-decomposition
SimpleRpcServer is broken,https://issues.apache.org/jira/browse/HBASE-27097,"Concerns about SimpleRpcServer are not new, and not new to 2.5. @chenxu noticed a problem on HBASE-23917 back in 2020. After some simple evaluations it seems quite broken. 

When I run an async version of ITLCC against a 2.5.0 cluster configured with hbase.rpc.server.impl=SimpleRpcServer, the client almost immediately stalls because there are too many in flight requests. The logic to pause with too many in flight requests is my own. That's not important. Looking at the server logs it is apparent that SimpleRpcServer is quite broken. Handlers suffer frequent protobuf parse errors and do not properly return responses to the client. This is what stalls my test client. Rather quickly all available request slots are full of requests that will have to time out on the client side. 

Exceptions have three patterns but they all have in common SimpleServerRpcConnection#process. It seems likely the root cause is mismatched expectations or bugs in connection buffer handling in SimpleRpcServer/SimpleServerRpcConnection versus downstream classes that process and parse the buffers. It also seems likely that changes were made to downstream classes like ServerRpcConnection expecting NettyRpcServer's particulars without updating SimpleServerRpcConnection and/or SimpleRpcServer. That said, this is just a superficial analysis. 

1) ""Protocol message end-group tag did not match expected tag"" 

2022-06-07T16:44:04,625 WARN [Reader=5,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1 
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag. 
at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.invalidEndTag(InvalidProtocolBufferException.java:129) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.checkLastTagWas(CodedInputStream.java:4034) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4275) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9981) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9910) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14190) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15304) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14651) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:420) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:317) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.mergeFrom(ProtobufUtil.java:2638) ~[hbase-client-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?] 
2) ""Protocol message tag had invalid wire type."" 

2022-06-07T16:44:04,705 WARN [Reader=6,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1 
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException$InvalidWireTypeException: Protocol message tag had invalid wire type. 
at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.invalidWireType(InvalidProtocolBufferException.java:134) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:527) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.GeneratedMessageV3.parseUnknownField(GeneratedMessageV3.java:320) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10700) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10620) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11481) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11475) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9981) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto.<init>(ClientProtos.java:9910) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest.<init>(ClientProtos.java:14190) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15304) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14651) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:420) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:317) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.mergeFrom(ProtobufUtil.java:2638) ~[hbase-client-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?] 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?] 
at java.lang.Thread.run(Thread.java:829) ~[?:?] 
3) ""While parsing a protocol message, the input ended unexpectedly in the middle of a field."" 

2022-06-07T16:44:04,885 WARN [Reader=9,bindAddress=buildbox.localdomain,port=8120] ipc.RpcServer: /127.0.1.1:8120 is unable to read call parameter from client 127.0.0.1 
org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field. This could mean either that the input has been truncated or that an embedded message misreported its own length. 
at org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:107) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readRawLittleEndian64(CodedInputStream.java:4478) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readFixed64(CodedInputStream.java:4167) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:511) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hbase.thirdparty.com.google.protobuf.GeneratedMessageV3.parseUnknownField(GeneratedMessageV3.java:320) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10700) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue.<init>(ClientProtos.java:10620) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11481) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$QualifierValue$1.parsePartialFrom(ClientProtos.java:11475) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10520) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue.<init>(ClientProtos.java:10464) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12251) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$ColumnValue$1.parsePartialFrom(ClientProtos.java:12245) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at 
org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14097) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$1.parsePartialFrom(ClientProtos.java:14091) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hbase.thirdparty.com.google.protobuf.CodedInputStream$ByteInputDecoder.readMessage(CodedInputStream.java:4274) ~[hbase-shaded-protobuf-4.1.0.jar:4.1.0] 
at 
org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$1.parsePartialFrom(ClientProtos.java:15298) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest$Builder.mergeFrom(ClientProtos.java:14860) ~[hbase-protocol-shaded-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
org.apache.hadoop.hbase.ipc.ServerRpcConnection.processRequest(ServerRpcConnection.java:644) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.ServerRpcConnection.processOneRpc(ServerRpcConnection.java:444) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.process(SimpleServerRpcConnection.java:285) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection.readAndProcess(SimpleServerRpcConnection.java:251) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener.doRead(SimpleRpcServer.java:318) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.doRunLoop(SimpleRpcServer.java:180) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader.run(SimpleRpcServer.java:153) ~[hbase-server-2.5.1-SNAPSHOT.jar:2.5.1-SNAPSHOT] 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?] 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?] 
at java.lang.Thread.run(Thread.java:829) ~[?:?]

3. I verified elasticsearch is well configured and I noticed in /var/log/elasticsearch/contextElasticSearch.log:

[2018-08-23T12:58:51,898][WARN ][o.e.x.s.t.n.SecurityNetty4ServerTransport] [hjui3pv] exception caught on transport layer [NettyTcpChannel

{localAddress=/127.0.0.1:9300, remoteAddress=/127.0.0.1:43570}
], closing connection
java.lang.IllegalStateException: Received message from unsupported version: [5.2.2] minimal compatible version is: [5.6.0]
at org.elasticsearch.transport.TcpTransport.ensureVersionCompatibility(TcpTransport.java:1462) ~[elasticsearch-6.3.2.jar:6.3.2]
at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1409) ~[elasticsearch-6.3.2.jar:6.3.2]
at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:64) ~[transport-netty4-6.3.2.jar:6.3.2]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:297) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:413) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [netty-codec-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:241) [netty-handler-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:545) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [netty-transport-4.1.16.Final.jar:4.1.16.Final]
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.16.Final.jar:4.1.16.Final]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]

Please advise what should be done in order to resolve this.

Thank you.","B: When running an async version of ITLCC against a 2.5.0 cluster configured with hbase.rpc.server.impl=SimpleRpcServer, the client stalls due to too many in-flight requests.

B: Handlers in SimpleRpcServer suffer from frequent protobuf parse errors and do not return responses to the client properly.

B: Exceptions in SimpleRpcServer show three patterns related to SimpleServerRpcConnection#process, indicating potential mismatched expectations or bugs in connection buffer handling.

B: The first exception pattern is ""Protocol message end-group tag did not match expected tag,"" which occurs when reading call parameters from the client.

B: The second exception pattern is ""Protocol message tag had invalid wire type,"" which occurs while reading call parameters from the client.

B: The third exception pattern is ""While parsing a protocol message, the input ended unexpectedly in the middle of a field,"" indicating possible truncation or misreported length of an embedded message.

B: There is a version compatibility issue with Elasticsearch, where a message from an unsupported version is received, and the minimal compatible version is not met.",Blocker,FALSE,Over-decomposition
add -Wall to compile log4cxx will get many warning,https://issues.apache.org/jira/browse/LOGCXX-14,"When I build my project using log4cxx with -Wall ,it will get many warning. I think that why don't you use -Wall in log4cxx to check source code. it's very horrible to see these warning !","B: When I build my project using log4cxx with -Wall, it generates many warnings.  

B: Investigate the source code of log4cxx to identify and address the warnings generated when using -Wall.  

B: Propose the inclusion of -Wall in the log4cxx build process to ensure code quality and minimize warnings.",Minor,FALSE,Over-decomposition
WSDL2Java fails with imported schemas in WSDL with file not found.,https://issues.apache.org/jira/browse/AXIS2-796,"When using WSDL2Java as ant task schemas that are imported as relative and contained in the same directory as the WSDL cause the task to fail. It finds them early on and then looks in a different directory later on according to the output. This same WSDL and xsd's will work fine in the Eclipse AxisCodegen plugin. 

I believe this is possibly due to the basedir not being set correctly. The Eclipse tool specifically sets the basedir early on in the configuration whereas WSDL2Java does not. 

Stack trace produced. 

wsdl2java: 
[delete] Deleting directory D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\target\generated-sources\java 
[java] Retrieving schema at 'CustomerHeaderData.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/'. 
[java] Retrieving schema at 'CustomerMessages.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/'. 
[java] Retrieving schema at 'CustomerData.xsd', relative to 'file:/D:/Documents%20and%20Settings/uchitjj/workspace/PaintContractorWS/src/main/wsdl/CustomerMessages.xsd'. 
[java] org.apache.axis2.wsdl.codegen.CodeGenerationException: Error parsing WSDL 
[java] at org.apache.axis2.wsdl.codegen.CodeGenerationEngine.<init>(CodeGenerationEngine.java:94) 
[java] at org.apache.axis2.wsdl.WSDL2Code.main(WSDL2Code.java:32) 
[java] at org.apache.axis2.wsdl.WSDL2Java.main(WSDL2Java.java:21) 
[java] Caused by: org.apache.axis2.AxisFault: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified); nested exception is: 
[java] java.lang.RuntimeException: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.populateService(WSDL2AxisServiceBuilder.java:243) 
[java] at org.apache.axis2.wsdl.codegen.CodeGenerationEngine.<init>(CodeGenerationEngine.java:87) 
[java] ... 2 more 
[java] Caused by: java.lang.RuntimeException: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1916) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1929) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.handleImport(SchemaBuilder.java:1714) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.handleXmlSchemaElement(SchemaBuilder.java:126) 
[java] at org.apache.ws.commons.schema.XmlSchemaCollection.read(XmlSchemaCollection.java:250) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.getXMLSchema(WSDL2AxisServiceBuilder.java:959) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.copyExtensibleElements(WSDL2AxisServiceBuilder.java:1067) 
[java] at org.apache.axis2.description.WSDL2AxisServiceBuilder.populateService(WSDL2AxisServiceBuilder.java:221) 
[java] ... 3 more 
[java] Caused by: org.apache.ws.commons.schema.XmlSchemaException: D:\Documents and Settings\uchitjj\workspace\PaintContractorWS\src\CustomerMessages.xsd (The system cannot find the file specified) 
[java] at org.apache.ws.commons.schema.XmlSchemaCollection.read(XmlSchemaCollection.java:221) 
[java] at org.apache.ws.commons.schema.SchemaBuilder.resolveXmlSchema(SchemaBuilder.java:1911) 
[java] ... 10 more 
[java] Java Result: 1 
[java] Exception in thread ""main""","B: When using WSDL2Java as an ant task, schemas that are imported as relative and contained in the same directory as the WSDL cause the task to fail. 

B: The WSDL2Java task finds the schemas early on but then looks in a different directory later on according to the output.

B: The WSDL and XSD files work fine in the Eclipse AxisCodegen plugin.

B: It is believed that the basedir may not be set correctly in WSDL2Java.

B: The Eclipse tool sets the basedir early on in the configuration, whereas WSDL2Java does not.

B: The stack trace produced indicates an error parsing WSDL.

B: The stack trace shows a ""CodeGenerationException"" caused by a ""XmlSchemaException"" stating that the file ""CustomerMessages.xsd"" cannot be found.

B: The error occurs when WSDL2Java attempts to retrieve the schema at ""CustomerMessages.xsd"" relative to the specified file path.",Major,FALSE,Over-analysis
Include documentation for previous version on the website,https://issues.apache.org/jira/browse/HELIX-270,"The documentation on the website is for 0.6.2 but as far as I can see, this version is still under heavy development. 

I can't find a link to the documentation for the 0.6.1 release. I can build the website locally, though I don't get the latest doc fixes for 0.6.1, and that's much less convenient anyway. 

Could you provide separate links to the documentation of the currently released version, previous versions (in the future) and dev version? 

Same for the javadoc: the online one is 0.6.2-snapshot I think (you should probably display a version number on the javadoc headers). 

Thanks!","B: The documentation on the website is for version 0.6.2, which is still under heavy development. 

B: There is no link to the documentation for the 0.6.1 release. 

B: The locally built website does not include the latest documentation fixes for version 0.6.1.

B: There should be separate links provided for the documentation of the currently released version, previous versions, and the development version.

B: The online javadoc is showing as 0.6.2-snapshot, and it should display a version number on the javadoc headers.",Major,FALSE,Over-decomposition
axis is vulnerable to XXE,https://issues.apache.org/jira/browse/AXIS-471,"See note from ""Gregory Steuck"" <greg@nest.cx>. He posted an advisory @ 
http://groups.google.com/groups?selm=apn8hv%2421a9%241%40FreeBSD.csie.NCTU.edu.tw&oe=UTF-8&output=gplain 
as well. 

============================================================================ 
Hi Davanum, 

I sent a similar message to Sam Ruby yesterday but never got a reply. 
You seem to be actively committing changes to Axis, so you may be in a 
better position to address the problem. 

Unfortunately Axis (at least 1.0) is vulnerable to XXE attack as 
described in my post below. 

Thanks 
Greg 
----BEGIN PGP SIGNED MESSAGE---- 
Hash: SHA1 

Gregory Steuck security advisory #1, 2002 

Overview: 
XXE (Xml eXternal Entity) attack is an attack on an application that parses 
XML input from untrusted sources using incorrectly configured XML parser. 
The application may be coerced to open arbitrary files and/or TCP connections. 

Legal Notice: 
This Advisory is Copyright (c) 2002 Gregory Steuck. 
You may distribute it unmodified. 
You may not modify it and distribute it or distribute parts 
of it without the author's written permission. 

Disclaimer: 
The information in this advisory is believed to be true though 
it may be false. 
The opinions expressed in this advisory and program are my own and 
not of any company. The usual standard disclaimer applies, 
especially the fact that Gregory Steuck is not liable for any damages 
caused by direct or indirect use of the information or functionality 
provided by this advisory or program. Gregory Steuck bears no 
responsibility for content or misuse of this advisory or program or 
any derivatives thereof. 
Anything in this document may change without notice. 

Details: 
External entity references allow embedding data outside the main file into 
an XML document. In the DTD, one declares the external reference with the 
following syntax: 
<!ENTITY name SYSTEM ""URI""> 

XML processor behavior as specified is 
http://www.w3.org/TR/REC-xml#include-if-valid: 

""When an XML processor recognizes a reference to a parsed entity, in 
order to validate the document, the processor must include its 
replacement text. If the entity is external, and the processor is not 
attempting to validate the XML document, the processor may, but need 
not, include the entity's replacement text..."" 

Now assume that the XML processor parses data originating from a source under 
attacker control. Most of the time the processor will not be validating, 
but it MAY include the replacement text thus initiating an unexpected 
file open operation, or HTTP transfer, or whatever system ids the XML 
processor knows how to access. 

Suspect systems: 
The buzz on the street is ""web services"". They accept XML encoded 
data over the network, sometimes from untrusted clients. So, the 
prime targets are SOAP and XMLRPC implementations. Yet, there are 
many more XML based protocols and vulnerability does not necessary 
lie with the servers. Pick any ""XML based network protocol"" and 
try to apply the attack methodology. 

Suggested fix: 
Most XML parsers allow their user to explicitly specify external 
entity handler. In case of untrusted XML input it is best to prohibit 
all external general entities. 

Successful exploitation may yield: 

DoS on the parsing system by making it open, e.g. 
file:///dev/random | file:///dev/urandom | file://c:/con/con 
TCP scans using HTTP external entities (including behind firewalls 
since application servers often have world view different 
from that of the attacker) 
Unauthorized access to data stored as XML files on the parsing 
system file system (of course the attacker still needs a way to 
get these data back) 
DoS on other systems (if parsing system is allowed to establish 
TCP connections to other systems) 
NTLM authentication material theft by initiating UNC file access to 
systems under attacker control (far fetched?) 
Doomsday scenario: A widely deployed and highly connected application 
vulnerable to this attack may be used for DDoS. 
Products review: 
Several SOAP and XMLRPC implementation were found vulnerable. I will 
be contacting their respective authors directly. It will be up to 
those authors to publish the patches and/or advisories. 

The following implementations were found NOT vulnerable and the reasons 
contributing to their resistance were researched. 

Java: 
Apache XML-RPC server is NOT vulnerable in the default configuration 
due to its use of MinML parser which doesn't support external entities. 
Yet should be vulnerable if used with a full blown parser like Xerces 
or Crimson. To make it invulnerable in all configurations it needs to 
explicitly setup an EntityResolver that aborts having found external 
entities. 

Marqu¨¦e XML-RPC also uses MinML and thus is NOT vulnerable. 

XMLRPC-J uses freeDOM that only supports Minimal XML which 
lacks entity references (http://www.docuverse.com/smldev/minxml.jsp) 

WebLogic 6.1sp3 SOAP implementation was NOT found vulnerable. It 
appears to be using a parser that ignores entities altogether. Ignorance 
is bliss... 

Python: 
Python 2.2 SimpleXMLRPCServer does NOT seem to be vulnerable. It can use 
multiple different parsers: 

xmllib.XMLParser is the default one shipped with Python. It 
doesn't implement processing of doctype definition and thus doesn't 
understand external entities defined in there 
ExpatParser is used when expat python-expat is installed, 
it understands the references but seems to replace them with 
empty strings unconditionally. This negates the attack. 
SGMLOP parser, judging by comments in its source doesn't recognize 
external entities 
FastParser was not available for inspection 
Acknowledgments: 
Even though the issue was discovered and researched independently I 
cannot claim to be the first one to realize the risks associated with 
XML external entities. E.g. RFC 2518 discusses the issue in section 
17.7 Implications of XML External Entities. 

----BEGIN PGP SIGNATURE---- 
Version: GnuPG v1.0.6 (OpenBSD) 
Comment: Processed by Mailcrypt 3.5.6 and Gnu Privacy Guard <http://www.gnupg.org/> 

iEYEARECAAYFAj2/FZkACgkQCxVCvY31obB6vQCbBlV+v0jDRQQ7GcNxYRtajtAf 
FxUAnRCDfjLy2692iGF3Ewmxzo/VXYmz 
=t4QF 
----END PGP SIGNATURE---- 
============================================================================","B: Axis (at least 1.0) is vulnerable to XXE (XML eXternal Entity) attacks due to incorrectly configured XML parsers.

B: The application may be coerced to open arbitrary files and/or TCP connections when it parses XML input from untrusted sources.

B: Most XML parsers allow users to specify an external entity handler, and it is best to prohibit all external general entities when dealing with untrusted XML input.

B: Successful exploitation of XXE vulnerabilities can lead to DoS on the parsing system, unauthorized access to data, and potential DDoS scenarios.

B: Several SOAP and XMLRPC implementations have been found vulnerable to XXE attacks, and the respective authors need to publish patches and advisories.

B: The Apache XML-RPC server is not vulnerable in its default configuration due to its use of the MinML parser, which does not support external entities.

B: In order to make the Apache XML-RPC server invulnerable in all configurations, it needs to explicitly set up an EntityResolver that aborts upon finding external entities. 

B: Python 2.2's SimpleXMLRPCServer appears not to be vulnerable due to its default parser, xmllib.XMLParser, which does not process doctype definitions and, thus, does not recognize external entities. 

B: Other parsers like ExpatParser and SGMLOP in Python also mitigate the XXE vulnerability through their handling of external entities.",-,FALSE,Over-decomposition
PDF parsing to XHTML results in tika attempting to write invalid HTML characters.,https://issues.apache.org/jira/browse/TIKA-2955,"Hi, I am trying to parse: 314.pdf 

what is happening when I try to convert it to XHTML is my XML parser fails because: 

14:35:12.876 [main] ERROR com.funnelback.common.filter.TikaFilterProvider - Unable to filter stream with document type '.pdf' 
org.xml.sax.SAXException: net.sf.saxon.trans.XPathException: Illegal HTML character: decimal 147 
at net.sf.saxon.event.ReceivingContentHandler.endElement(ReceivingContentHandler.java:538) ~[Saxon-HE-9.9.0-2.jar:?] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.SecureContentHandler.endElement(SecureContentHandler.java:256) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.SafeContentHandler.endElement(SafeContentHandler.java:274) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.sax.XHTMLContentHandler.endDocument(XHTMLContentHandler.java:229) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.pdf.AbstractPDF2XHTML.endDocument(AbstractPDF2XHTML.java:556) ~[tika-parsers-1.19.1.jar:1.19.1] 
at org.apache.pdfbox.text.PDFTextStripper.writeText(PDFTextStripper.java:267) ~[pdfbox-2.0.12.jar:2.0.12] 
at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:117) ~[tika-parsers-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:172) ~[tika-parsers-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.19.1.jar:1.19.1] 
at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:143) ~[tika-core-1.19.1.jar:1.19.1] 
at 
[removed section of trace] 
Caused by: net.sf.saxon.trans.XPathException: Illegal HTML character: decimal 147 
at net.sf.saxon.serialize.HTMLEmitter.writeEscape(HTMLEmitter.java:379) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.serialize.XMLEmitter.characters(XMLEmitter.java:662) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.serialize.HTMLEmitter.characters(HTMLEmitter.java:441) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.serialize.HTMLIndenter.characters(HTMLIndenter.java:216) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ProxyReceiver.characters(ProxyReceiver.java:193) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.SequenceNormalizer.characters(SequenceNormalizer.java:183) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ReceivingContentHandler.flush(ReceivingContentHandler.java:646) ~[Saxon-HE-9.9.0-2.jar:?] 
at net.sf.saxon.event.ReceivingContentHandler.endElement(ReceivingContentHandler.java:526) ~[Saxon-HE-9.9.0-2.jar:?] 
... 43 more 
It looks like tika is asking the XML library to handle chracter 147 ie 0x93 which is not allowed in HTML. 

This saxon XML library is not happy with that, I think the default java one doesn't complain when given the invalid character though, however tika is probably wrong to write out that character when writing XHTML.","B: The XML parser fails when attempting to convert a PDF document (314.pdf) to XHTML due to an illegal HTML character (decimal 147).

B: The error message indicates that the SAXException is caused by the illegal HTML character being processed by the Saxon XML library.

B: The TikaFilterProvider reports an error stating it is unable to filter the stream with document type '.pdf'.

B: The issue seems to arise when Tika attempts to write out the illegal character during the XHTML conversion process.

B: The default Java XML library does not complain about the illegal character, unlike the Saxon XML library which raises an exception. 

B: The Tika library is incorrectly writing out an illegal character when generating XHTML from the PDF.",Major,FALSE,Over-analysis
ConcurrentModificationException thrown from inside camel splitter,https://issues.apache.org/jira/browse/CAMEL-6771,"We use camel 2.11.1 running on the oracle 1.7 jvm for linux. 

I have a route that looks like this. It reads in files and puts them on a seda queue with 8 concurrent consumers. 

The SpatialInterpolationPojo reads each file is read and split into two messages X and Y. 
The MyAggregator uses X and Y together and outputs a combined message A.B 
The MySplitterPojo splits A.B into two messages A and B 
from(""file://somefile"") 
.to(""seda:filteraccept?concurrentConsumers=8""); 

from(""seda:filteraccept?concurrentConsumers=8"") 
.split() 
.method(new SpatialInterpolationPojo(), ""split"") 
.to(""direct:wind-aggregator""); 

from(""direct:wind-aggregator"") 
.aggregate(packageCorrelationId(), new MyAggregator()) 
.completionPredicate(header(FIELD_AGGREGATION_COMPLETE).isNotNull()) 
.split() 
.method(new MySplitterPojo()) 
.to(""seda:output""); 
The MySplitterPojo simply returns List<Message> containing two messages that come from data in the input message body. We copy the body headers to the result messages. 

It is thread safe, it has no state, ie there are no object fields that are modified. 

The method is like this it is edited for clarity/privacy: 

public class MySplitterPojo { 

public List<Message> splitMessage( 
@Headers Map<String, Object> headers, 
@Body CombinedObject body) { 

DefaultMessage a = new DefaultMessage(); 
a.setBody(body.getA()); 
a.setHeaders(new HashMap<String, Object>(headers)); 

DefaultMessage b = new DefaultMessage(); 
b.setBody(body.getB()); 
b.setHeaders(new HashMap<String, Object>(headers)); 

ArrayList<Message> result = new ArrayList<Message>(2); 
result.add(a); 
result.add(b); 

return result; 
} 
} 
When we run this route we very occasionally get the exception below. You can see that it is entirely within camel, it appears to be trying to copy the map stored under the exchange property Exchange.AGGREGATION_STRATEGY which is a camel internal property key. 

By inspection of the message I can see that Exchange has just come out of the WindVectorAggregator. 

This seems like it must be a camel bug to me. Any ideas? 

15 Sep 2013 23:06:47,140[Camel (camel-1) thread #21 - seda://filteraccept] WARN AggregateProcessor Error processing aggregated exchange. Exchange[Message: { Trondheim, NO=WindVector [u=-5.92894983291626, v=7.060009002685547], ... }]. Caused by: [java.util.ConcurrentModificationException - null] 
java.util.ConcurrentModificationException 
at java.util.HashMap$HashIterator.nextEntry(Unknown Source) 
at java.util.HashMap$EntryIterator.next(Unknown Source) 
at java.util.HashMap$EntryIterator.next(Unknown Source) 
at java.util.HashMap.putAllForCreate(Unknown Source) 
at java.util.HashMap.<init>(Unknown Source) 
at org.apache.camel.processor.MulticastProcessor.setAggregationStrategyOnExchange(MulticastProcessor.java:1011) 
at org.apache.camel.processor.Splitter.process(Splitter.java:95) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:86) 
at org.apache.camel.processor.aggregate.AggregateProcessor$1.run(AggregateProcessor.java:495) 
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) 
at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source) 
at java.util.concurrent.FutureTask.run(Unknown Source) 
at org.apache.camel.util.concurrent.SynchronousExecutorService.execute(SynchronousExecutorService.java:62) 
at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) 
at org.apache.camel.processor.aggregate.AggregateProcessor.onSubmitCompletion(AggregateProcessor.java:487) 
at org.apache.camel.processor.aggregate.AggregateProcessor.onCompletion(AggregateProcessor.java:471) 
at org.apache.camel.processor.aggregate.AggregateProcessor.doAggregation(AggregateProcessor.java:325) 
at org.apache.camel.processor.aggregate.AggregateProcessor.process(AggregateProcessor.java:229) 
at org.apache.camel.util.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:61) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122) 
at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:60) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:571) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:504) 
at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:213) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RecipientList.sendToRecipientList(RecipientList.java:151) 
at org.apache.camel.component.bean.MethodInfo$1.doProceed(MethodInfo.java:285) 
at org.apache.camel.component.bean.MethodInfo$1.proceed(MethodInfo.java:251) 
at org.apache.camel.component.bean.BeanProcessor.process(BeanProcessor.java:161) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:117) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:80) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:122) 
at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:60) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:122) 
at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:298) 
at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:571) 
at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:504) 
at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:213) 
at org.apache.camel.processor.Splitter.process(Splitter.java:98) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.BacklogTracerInterceptor.process(BacklogTracerInterceptor.java:84) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:91) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:391) 
at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:273) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.interceptor.DefaultChannel.process(DefaultChannel.java:335) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:117) 
at org.apache.camel.processor.Pipeline.process(Pipeline.java:80) 
at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:46) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.processor.UnitOfWorkProcessor.processAsync(UnitOfWorkProcessor.java:150) 
at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:117) 
at org.apache.camel.processor.RouteInflightRepositoryProcessor.processNext(RouteInflightRepositoryProcessor.java:48) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:99) 
at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90) 
at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:72) 
at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:73) 
at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:294) 
at org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:203) 
at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:150) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) 
at java.lang.Thread.run(Unknown Source)","B: The application is using Camel 2.11.1 on the Oracle 1.7 JVM for Linux. 

B: The route reads in files and puts them on a SEDA queue with 8 concurrent consumers.

B: The `SpatialInterpolationPojo` reads each file and splits it into two messages, X and Y.

B: The `MyAggregator` uses messages X and Y together and outputs a combined message A.B.

B: The `MySplitterPojo` splits message A.B into two messages A and B.

B: The method `splitMessage` in the `MySplitterPojo` class is implemented to return a list of two messages containing the split results.

B: The `MySplitterPojo` class is thread-safe and does not maintain any state.

B: The route occasionally throws a `ConcurrentModificationException` during the processing of aggregated exchanges.

B: The exception indicates that an attempt was made to copy a map stored under the exchange property `Exchange.AGGREGATION_STRATEGY`.

B: The exception stack trace shows that the error originates from the `MulticastProcessor.setAggregationStrategyOnExchange` method.",Major,FALSE,Over-analysis
proxy-user not working for Spark on k8s in cluster deploy mode,https://issues.apache.org/jira/browse/SPARK-39399,"As part of https://issues.apache.org/jira/browse/SPARK-25355 Proxy user support was added for Spark on K8s. But the PR only added proxy user argument on the spark-submit command. The actual functionality of authentication using the proxy user is not working in case of cluster deploy mode. 

We get AccessControlException when trying to access the kerberized HDFS through a proxy user. 

Spark-Submit: 
$SPARK_HOME/bin/spark-submit \ 
--master <K8S_APISERVER> \ 
--deploy-mode cluster \ 
--name with_proxy_user_di \ 
--proxy-user <username> \ 
--class org.apache.spark.examples.SparkPi \ 
--conf spark.kubernetes.container.image=<SPARK3.2_with_hadoop3.1_image> \ 
--conf spark.kubernetes.driver.limit.cores=1 \ 
--conf spark.executor.instances=1 \ 
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \ 
--conf spark.kubernetes.namespace=<namespace_name> \ 
--conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \ 
--conf spark.eventLog.enabled=true \ 
--conf spark.eventLog.dir=hdfs://<hdfs_cluster>/scaas/shs_logs \ 

--conf spark.kubernetes.file.upload.path=hdfs://<hdfs_cluster>/tmp \ 

--conf spark.kubernetes.container.image.pullPolicy=Always \ 
$SPARK_HOME/examples/jars/spark-examples_2.12-3.2.0-1.jar 
Driver Logs: 

++ id -u 
+ myuid=185 
++ id -g 
+ mygid=0 
+ set +e 
++ getent passwd 185 
+ uidentry= 
+ set -e 
+ '[' -z '' ']' 
+ '[' -w /etc/passwd ']' 
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false' 
+ SPARK_CLASSPATH=':/opt/spark/jars/*' 
+ env 
+ grep SPARK_JAVA_OPT_ 
+ sort -t_ -k4 -n 
+ sed 's/[^=]*=\(.*\)/\1/g' 
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS 
+ '[' -n '' ']' 
+ '[' -z ']' 
+ '[' -z ']' 
+ '[' -n '' ']' 
+ '[' -z x ']' 
+ SPARK_CLASSPATH='/opt/hadoop/conf::/opt/spark/jars/*' 
+ '[' -z x ']' 
+ SPARK_CLASSPATH='/opt/spark/conf:/opt/hadoop/conf::/opt/spark/jars/*' 
+ case ""$1"" in 
+ shift 1 
+ CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"") 
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=<addr> --deploy-mode client --proxy-user proxy_user --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.SparkPi spark-internal 
WARNING: An illegal reflective access operation has occurred 
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0-1.jar) to constructor java.nio.DirectByteBuffer(long,int) 
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform 
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations 
WARNING: All illegal access operations will be denied in a future release 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of successful kerberos logins and latency (milliseconds)""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of failed kerberos logins and latency (milliseconds)""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""GetGroups""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since startup""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since last successful login""}, valueName=""Time"") 
22/04/26 08:54:38 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics 
22/04/26 08:54:38 DEBUG SecurityUtil: Setting hadoop.security.token.service.use_ip to true 
22/04/26 08:54:38 DEBUG Shell: Failed to detect a valid hadoop home directory 
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. 
at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:469) 
at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:440) 
at org.apache.hadoop.util.Shell.<clinit>(Shell.java:517) 
at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78) 
at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665) 
at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:102) 
at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:86) 
at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315) 
at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:303) 
at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1827) 
at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:709) 
at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:659) 
at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:570) 
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161) 
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) 
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90) 
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043) 
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052) 
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 
22/04/26 08:54:38 DEBUG Shell: setsid exited with exit code 0 
22/04/26 08:54:38 DEBUG Groups: Creating new Groups object 
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: backing jks path initialized to file:/etc/security/bind.jceks 
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: initialized local file as '/etc/security/bind.jceks'. 
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: the local file does not exist. 
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Usersearch baseDN: dc=<dc> 
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Groupsearch baseDN: dc=<dc> 
22/04/26 08:54:38 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.LdapGroupsMapping; cacheTimeout=300000; warningDeltaMs=5000 
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login 
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login commit 
22/04/26 08:54:38 DEBUG UserGroupInformation: using local user:UnixPrincipal: 185 
22/04/26 08:54:38 DEBUG UserGroupInformation: Using user: ""UnixPrincipal: 185"" with name 185 
22/04/26 08:54:38 DEBUG UserGroupInformation: User entry: ""185"" 
22/04/26 08:54:38 DEBUG UserGroupInformation: Reading credentials from location set in HADOOP_TOKEN_FILE_LOCATION: /mnt/secrets/hadoop-credentials/..2022_04_26_08_54_34.1262645511/hadoop-tokens 
22/04/26 08:54:39 DEBUG UserGroupInformation: Loaded 3 tokens 
22/04/26 08:54:39 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE) 
22/04/26 08:54:39 DEBUG UserGroupInformation: PrivilegedAction as:proxy_user (auth:PROXY) via 185 (auth:SIMPLE) from:org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163) 
22/04/26 08:54:39 DEBUG FileSystem: Loading filesystems 
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar 
22/04/26 08:54:39 DEBUG FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar 
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar 
22/04/26 08:54:39 DEBUG FileSystem: Looking for FS supporting hdfs 
22/04/26 08:54:39 DEBUG FileSystem: looking for configuration option fs.hdfs.impl 
22/04/26 08:54:39 DEBUG FileSystem: Looking in service filesystems for implementation class 
22/04/26 08:54:39 DEBUG FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket 
22/04/26 08:54:39 DEBUG DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0 
22/04/26 08:54:39 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>/tmp/spark-upload-bf713a0c-166b-43fc-a5e6-24957e75b224/spark-examples_2.12-3.0.1.jar 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false 
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket 
22/04/26 08:54:39 DEBUG RetryUtils: multipleLinearRandomRetry = null 
22/04/26 08:54:39 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@4a325eb9 
22/04/26 08:54:39 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@2577d6c8 
22/04/26 08:54:40 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library... 
22/04/26 08:54:40 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path: [/usr/java/packages/lib, /usr/lib64, /lib64, /lib, /usr/lib] 
22/04/26 08:54:40 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib 
22/04/26 08:54:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
22/04/26 08:54:40 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded. 
22/04/26 08:54:40 DEBUG DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.transfer.protection = authentication,privacy, configured class dfs.data.transfer.saslproperties.resolver.class = class org.apache.hadoop.security.SaslPropertiesResolver 
22/04/26 08:54:40 DEBUG Client: The ping interval is 60000 ms. 
22/04/26 08:54:40 DEBUG Client: Connecting to <server>/<ip>:8020 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796) 
22/04/26 08:54:40 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE22/04/26 08:54:40 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector.class) 
22/04/26 08:54:40 DEBUG SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one 
22/04/26 08:54:40 DEBUG SaslRpcClient: client isn't using kerberos 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720) 
22/04/26 08:54:40 WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
22/04/26 08:54:40 DEBUG Client: closing ipc connection to <server>/<ip>:8020: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:757) 
at java.base/java.security.AccessController.doPrivileged(Native Method) 
at java.base/javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) 
at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720) 
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:813) 
at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410) 
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558) 
at org.apache.hadoop.ipc.Client.call(Client.java:1389) 
at org.apache.hadoop.ipc.Client.call(Client.java:1353) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) 
at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source) 
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) 
at java.base/java.lang.reflect.Method.invoke(Unknown Source) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) 
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) 
at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source) 
at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654) 
at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579) 
at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1576) 
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) 
at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1591) 
at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65) 
at org.apache.hadoop.fs.Globber.doGlob(Globber.java:270) 
at org.apache.hadoop.fs.Globber.glob(Globber.java:149) 
at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2067) 
at org.apache.spark.util.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:318) 
at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:273) 
at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:271) 
at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293) 
at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) 
at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) 
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38) 
at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293) 
at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290) 
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108) 
at org.apache.spark.util.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:271) 
at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:364) 
at scala.Option.map(Option.scala:230) 
at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:364) 
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898) 
at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:165) 
at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163) 
at java.base/java.security.AccessController.doPrivileged(Native Method) 
at java.base/javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) 
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163) 
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) 
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90) 
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043) 
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052) 
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 
Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] 
at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173) 
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390) 
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:614) 
at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:410) 
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800) 
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:796) 
at java.base/java.security.AccessController.doPrivileged(Native Method) 
at java.base/javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) 
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796) 
... 53 more 


The reason for no delegation token found is that the proxy user UGI doesn't have any credentials/tokens ( tokenSize:: 0 ) 

22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:<hdfs>, Ident: (token for proxyUser: HDFS_DELEGATION_TOKEN owner=proxyUser, renewer=proxyUser, realUser=superuser/test@test.com, issueDate=1651165129518, maxDate=1651769929518, sequenceNumber=180516, masterKeyId=601) 
22/04/28 16:59:37 DEBUG Token: Cannot find class for token kind HIVE_DELEGATION_TOKEN 
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 08 73 68 72 70 72 61 73 61 04 68 69 76 65 1e 6c 69 76 79 2f 6c 69 76 79 2d 69 6e 74 40 43 4f 52 50 44 45 56 2e 56 49 53 41 2e 43 4f 4d 8a 01 80 71 1c 71 b5 8a 01 80 b9 35 79 b5 8e 15 cd 8e 03 6e 
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: kms-dt, Service: <ip>:9292, Ident: (kms-dt owner=proxyUser, renewer=proxyUser, realUser=superuser, issueDate=1651165129566, maxDate=1651769929566, sequenceNumber=181197, masterKeyId=1152) 
22/04/28 16:59:37 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE) 
22/04/28 16:59:37 DEBUG UserGroupInformation: createProxyUser: from:org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) 
22/04/28 16:59:37 DEBUG UserGroupInformation: proxy user created, ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) subject::Subject: 
Principal: proxyUser 
Principal: 185 (auth:SIMPLE) 
tokenSize:: 0 
22/04/28 16:59:38 DEBUG AbstractNNFailoverProxyProvider: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) tokensize:: 0 
22/04/28 16:59:38 DEBUG HAUtilClient: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) tokenSize::0 
22/04/28 16:59:38 DEBUG AbstractDelegationTokenSelector: kindName:: HDFS_DELEGATION_TOKEN service:: ha-hdfs:<hdfs> tokens size:: 0 
22/04/28 16:59:38 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>:8020/tmp/spark-upload-10582dde-f07c-4bf7-a611-5afbdd12ff6c/spark-examples_2.12-3.0.1.jar 


Please refer to the last 4 comments on https://issues.apache.org/jira/browse/SPARK-25355.","B: The proxy user support for Spark on K8s is not functioning properly in cluster deploy mode, leading to AccessControlException when accessing kerberized HDFS.

B: The spark-submit command is missing the necessary authentication functionality to use the proxy user for accessing kerberized HDFS.

B: There is an issue where the proxy user UGI does not have any credentials or tokens, resulting in a token size of 0.

B: A FileNotFoundException occurs due to HADOOP_HOME and hadoop.home.dir being unset, causing issues with Hadoop functionalities.

B: The system encounters an AccessControlException indicating that the client cannot authenticate via TOKEN or KERBEROS while trying to connect to the HDFS server.",Major,FALSE,Over-analysis
Update contribution guide w/ suggestions for GitHub merge button,https://issues.apache.org/jira/browse/BEAM-3264,"Today our guide for committers has instructions for rebasing, adjusting commits, etc. 

I think with the use of gitbox, we should encourage using the GitHub button, keeping a merge commit when possibly, but editing the default message since it references useless information like the transient branch that was merged.","B: Today our guide for committers has instructions for rebasing, adjusting commits, etc.  

B: We should encourage using the GitHub button for merging with gitbox.  

B: We should keep a merge commit when possible.  

B: The default merge commit message should be edited to remove references to useless information like the transient branch that was merged.",P2,FALSE,Over-decomposition
Remove HDP 3.0 stack from Ambari,https://issues.apache.org/jira/browse/AMBARI-22873,Remove HDP-3.0 stack definition from Ambari. Management Packs will replace stacks in Ambari 3.0.,"B: Remove HDP-3.0 stack definition from Ambari.  
B: Ensure Management Packs are implemented to replace stacks in Ambari 3.0.",Critical,FALSE,Over-decomposition
The OpenJPA web site must post the privacy policy,https://issues.apache.org/jira/browse/OPENJPA-844,"During the last few board meetings, the usage of Google Analytics to 
track the usage of our web sites was discussed. While this is not a 
problem per se, Google requires in its Terms and Conditions that all 
sites using it must post a privacy policy (to be exact, paragraph 7 of 
the Analytics Terms and Conditions at 
http://www.google.com/analytics/tos.html states that ""You must post a 
privacy policy and that policy must provide notice of your use of a 
cookie that collects anonymous traffic data."") 

The legal-discuss group, together with the Jackrabbit PMC and a number 
of individuals has pursued this issue and drafted up a privacy policy 
for Jackrabbit, which is available at 
http://jackrabbit.apache.org/privacy-policy.html 

The board would like to thank you for this and appreciates the effort 
and diligence that went into it. 

If your PMC is collecting information through Google Analytics (ATM 
there are at least 18 PMCs using it; you know who you are), we expect 
you to set up a privacy policy along the lines of the Jackrabbit PMC and 
add a note to your next board report after you have done so. 

Thank you for your cooperation. 

For the Apache board 
Henning","B: Google requires that all sites using Google Analytics must post a privacy policy that includes notice of cookie usage for collecting anonymous traffic data. 

B: The legal-discuss group and the Jackrabbit PMC have drafted a privacy policy for Jackrabbit that can be referenced.

B: PMCs that are currently collecting information through Google Analytics need to establish a privacy policy similar to that of the Jackrabbit PMC.

B: PMCs must add a note to their next board report indicating that they have set up a privacy policy.",Major,FALSE,Over-decomposition
typing issue in defaultDeniedProperties (org.apache.unomi.privacy.cfg),https://issues.apache.org/jira/browse/UNOMI-55,"Tested CXS 1.1.0 #685: 

open /etc/org.apache.unomi.privacy.cfg 
the property ""linedInId"" has a typing issue - the ""k"" is missing 
-> please add (linkedInId)","B: The property ""linedInId"" in /etc/org.apache.unomi.privacy.cfg has a typing issue - the ""k"" is missing.  
B: Please change ""linedInId"" to ""linkedInId"" in /etc/org.apache.unomi.privacy.cfg.",Major,FALSE,Over-decomposition
Email privacy issues in comment notification,https://issues.apache.org/jira/browse/ROL-650,"Notifications of new comments are sent with the commenter's email address as the FROM address. 

Having the poster's address as the FROM address seems at first glance to be a ""good thing"", however it means that any returns such as mailbox full, out of office messages etc. go back to that ""untrusted"" address. Also, if the address is a known email spammer, the comment will be posted but the weblog owner may not get the notification. The first issue is a bit of a privacy issue, since a site may decide not to publish contributors' email addresses, only to have the sent out through this route to unknow 3rd parties.","B: Notifications of new comments are sent with the commenter's email address as the FROM address.

B: Returns such as mailbox full or out of office messages go back to the commenter's email address.

B: If the commenter's email address is a known spammer, the weblog owner may not receive the notification.

B: The use of the commenter's email address in notifications raises privacy concerns for sites that do not want to publish contributors' email addresses.",Minor,FALSE,Over-decomposition
Trademarks / privacy policy footer displays broken,https://issues.apache.org/jira/browse/SUREFIRE-1844,"The footer which is at the end of Surefire's documentation pages, such as this one, have a broken display (at least in Firefox 81 and Google Chrome 85). The horizontal alignment is incorrect, causing the sentence to start outside the visible area and its end to overlap with the ""Privacy policy"" link, as can be seen in the screenshot below:","B: The footer at the end of Surefire's documentation pages has a broken display in Firefox 81. The horizontal alignment is incorrect, causing the sentence to start outside the visible area. 

B: The footer at the end of Surefire's documentation pages has a broken display in Google Chrome 85. The horizontal alignment is incorrect, causing the sentence to overlap with the ""Privacy policy"" link.",Trivial,TRUE,
iOS 6 - deal with new Privacy functionality in Contacts (ABAddressBook:: ABAddressBookCreateWithOptions),https://issues.apache.org/jira/browse/CB-902,"Currently crashes if the user does not have AddressBook permission on iOS 6. 

The user will get a popup dialog similar to the Geolocation permissions dialog. When creating an address book, we should handle the condition where the app does not have permission, and the address book returned is NULL.","B: Currently crashes if the user does not have AddressBook permission on iOS 6.  

B: The app should handle the condition where the address book returned is NULL when the user does not have permission.  

B: The user should receive a popup dialog similar to the Geolocation permissions dialog when attempting to access the address book without permission.",Critical,FALSE,Over-decomposition
Content is getting public to web search engine no privacy,https://issues.apache.org/jira/browse/OFBIZ-4360,"all content hosted on ofbiz trees is getting public throuth a general through this link 
myhost:8080/ecommerce/control/ViewSimpleContent?dataResourceId=10170","B: All content hosted on ofbiz trees is getting public through a general link.  
B: The link myhost:8080/ecommerce/control/ViewSimpleContent?dataResourceId=10170 is exposing sensitive content.",Major,FALSE,Over-decomposition
Fix HBase RPC protection documentation,https://issues.apache.org/jira/browse/HBASE-14400,"HBase configuration 'hbase.rpc.protection' can be set to 'authentication', 'integrity' or 'privacy'. 
""authentication means authentication only and no integrity or privacy; integrity implies 
authentication and integrity are enabled; and privacy implies all of 
authentication, integrity and privacy are enabled."" 

However hbase ref guide incorrectly suggests in some places to set the value to 'auth-conf' instead of 'privacy'. Setting value to 'auth-conf' doesn't provide rpc encryption which is what user wants. 

This jira will fix: 

documentation: change 'auth-conf' references to 'privacy' 
SaslUtil to support both set of values (privacy/integrity/authentication and auth-conf/auth-int/auth) to be backward compatible with what was being suggested till now. 
change 'hbase.thrift.security.qop' to be consistent with other similar configurations by using same set of values (privacy/integrity/authentication).","B: Change 'auth-conf' references in the HBase reference documentation to 'privacy'.  
B: Modify SaslUtil to support both sets of values: 'privacy', 'integrity', 'authentication' and 'auth-conf', 'auth-int', 'auth' for backward compatibility.  
B: Update 'hbase.thrift.security.qop' to use the same set of values ('privacy', 'integrity', 'authentication') for consistency with other similar configurations.",Critical,FALSE,Incorrect Interpretation of Solutions
"Obfuscate commentors email addresses when they select ""Notify me by email of new comments""",https://issues.apache.org/jira/browse/ROL-1455,"Currently, if a commentor leaves a comment and selects the option to be notified of new comments, they and all other users who selected to be notified receive the email notification with all of their email addresses rendered in the ""To"" line. Need to obfuscate their email adresses for privacy purposes. 

","B: Currently, if a commentor leaves a comment and selects the option to be notified of new comments, they receive the email notification with all of their email addresses rendered in the ""To"" line. 

B: Currently, if a commentor leaves a comment and selects the option to be notified of new comments, all other users who selected to be notified receive the email notification with all of their email addresses rendered in the ""To"" line. 

B: Need to obfuscate the email addresses of users who receive notifications for privacy purposes.",Major,FALSE,Over-decomposition
Impala website is missing mandatory elements,https://issues.apache.org/jira/browse/IMPALA-11899,"The Apache project website checker lists the following problems about impala.apache.org: 

missing link to the Apache Privacy Policy 
missing copyright notice 
see https://whimsy.apache.org/site/project/impala (the link may require committer or PMC privileges) 
These entries should be simple to add.","B: missing link to the Apache Privacy Policy  
B: missing copyright notice  
B: see https://whimsy.apache.org/site/project/impala (the link may require committer or PMC privileges)",Major,FALSE,Lacking key information
CookieSpecBase.domainMatch() leaks cookies to 3rd party domains,https://issues.apache.org/jira/browse/HTTPCLIENT-467,"The change committed for #32833
<http://issues.apache.org/bugzilla/show_bug.cgi?id=32833> is buggy; it doesn't
match browser behavior and in fact leaks cookies to third party domains.

To see, try the following:

CookieSpecBase cspec = new CookieSpecBase();
Cookie cookie = new Cookie("".hotmail.com"",""foo"",""bar"",""/"",Integer.MAX_VALUE,false);
cspec.match(""iwanttostealcookiesfromhotmail.com"",80,""/"",false,cookie);

It will return true. Testing in Firefox1.0.4 and IE6 show no such similar
leakage for similar cases. (Indeed, it'd be a headline-making privacy bug if
they were to do this.)

Those browsers do, in my limited testing, behave as desired by the filer of
#32833: a cookie of domain value '.mydomain.com' will be returned to exact host
'mydomain.com' (. However, the fix that was suggested was overbroad.

I suggest instead for CookieSpecBase.domainMatch():

public boolean domainMatch(final String host, final String domain)

{ // BUGGY: matches a '.service.com' cookie to hosts like 'enemyofservice.com' // return host.endsWith(domain) // || (domain.startsWith(""."") && host.endsWith(domain.substring(1))); // BETTER: RFC2109, plus matches a '.service.com' cookie to exact host 'service.com' return host.equals(domain) || (domain.startsWith(""."") && (host.endsWith(domain) || host.equals(domain.substring(1)))); }","B: The change committed for #32833 is buggy; it doesn't match browser behavior and leaks cookies to third party domains.

B: The method CookieSpecBase.domainMatch() contains a bug that matches a '.service.com' cookie to hosts like 'enemyofservice.com'.

B: The current implementation of CookieSpecBase.domainMatch() returns true for a specific cookie match that should not occur.

B: The suggested fix for CookieSpecBase.domainMatch() is overbroad and does not adhere to the expected behavior of browsers.

B: The expected behavior for CookieSpecBase.domainMatch() is that a cookie of domain value '.mydomain.com' should only be returned to the exact host 'mydomain.com'.",Major,FALSE,Incorrect Interpretation of Solutions
NonSequential parser gives an error,https://issues.apache.org/jira/browse/PDFBOX-2293,"I get the following error when using the sequential parse with Pdfbox 1.8.5. 

expected='endstream' actual='' org.apache.pdfbox.io.PushBackInputStream@eb43bd5: java.io.IOException: at org.apache.pdfbox.pdfparser.BaseParser.parseCOSStream(BaseParser.java:628) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:605) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:194) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1220) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1187) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.util.PDFMergerUtility.mergeDocuments(PDFMergerUtility.java:236) [pdfbox-1.8.5.jar:] 
at org.apache.pdfbox.util.PDFMergerUtility.mergeDocuments(PDFMergerUtility.java:185) [pdfbox-1.8.5.jar:] 
After looking at some of the fixed issues reported for similar problem(s), I have tried using PDFBox 2.0.0 built from the latest repository code and the nonsequential parser for the pdf processing. However, the file created as randomAccessFile seems to get damaged (cannot be opened in Acrobat Reader after the run) when I use PDFbox 2.0.0 for my processing. 
I am unable to attach a sample file because of privacy concerns for the content. I also get an error and am not able to generate the merged output. 
The code snippet is as follows- 

for (String fName : fileList) { 
pd = null; 
File pdFile = new File(fName); 
fNameStr = fName.substring(0, fName.lastIndexOf('.')) 
+ ""_new.pdf""; 

InputStream is = new FileInputStream(pdFile); 
RandomAccessFile raf = new RandomAccessFile(pdFileNew, ""rws""); 
pd = PDDocument.loadNonSeq(is, raf ); 
pd.getDocumentCatalog(); 
pd.save(fNameStr); 
pd.close(); 
if (is != null) { 
is.close(); 
} 
if(raf != null) { 
raf.close(); 
} 

ut.addSource(fNameStr); 
} 
FileOutputStream fos = new FileOutputStream(outFileName); 
ut.setDestinationStream(fos); 
ut.setIgnoreAcroFormErrors(true); 
ut.mergeDocuments(); 
fos.close(); 
Thank You.","B: I get the following error when using the sequential parse with Pdfbox 1.8.5: expected='endstream' actual='' org.apache.pdfbox.io.PushBackInputStream@eb43bd5: java.io.IOException at org.apache.pdfbox.pdfparser.BaseParser.parseCOSStream(BaseParser.java:628).

B: After trying to use PDFBox 2.0.0 built from the latest repository code, the file created as randomAccessFile seems to get damaged and cannot be opened in Acrobat Reader.

B: I am unable to attach a sample file because of privacy concerns for the content.

B: I get an error and am not able to generate the merged output.

B: The code snippet provided does not seem to work as intended for merging PDF documents. 

B: The issue arises specifically when using the `loadNonSeq` method with the `PDDocument` class while processing files.",Major,FALSE,Over-analysis
validate_cluster.sh should auto-detect DOCKER_ARCH,https://issues.apache.org/jira/browse/YUNIKORN-1308,"By default, the script validate_cluster.sh does not detect/set DOCKER_ARCH. This can result in the following problem:

Creating kind validation cluster
Apache YuniKorn version: 1.1.0
Helm chart directory: ./helm-charts/yunikorn
Kind cluster config: ./kind.yaml
Kubernetes image: kindest/node:v1.22.4
Registry name: apache
Plugin mode: false
Image Architecture: 
Creating cluster ""yk8s"" ...
_ Ensuring node image (kindest/node:v1.22.4) __
_ Preparing nodes __ __ __ 
_ Writing configuration __ 
_ Starting control-plane ___ 
_ Installing CNI __ 
_ Installing StorageClass __ 
_ Joining worker nodes __ 
Set kubectl context to ""kind-yk8s""
You can now use your cluster with:

kubectl cluster-info --context kind-yk8s

Not sure what to do next? __ Check out https://kind.sigs.k8s.io/docs/user/quick-start/

Pre-Loading docker images...

Pre-Loading admission--1.1.0 image failed, aborting
Removing kind cluster
Deleting cluster ""yk8s"" ...
After setting DOCKER_ARCH to amd64, the problem disappeared.","B: By default, the script validate_cluster.sh does not detect/set DOCKER_ARCH.  
B: Pre-loading admission--1.1.0 image fails if DOCKER_ARCH is not set correctly.  
B: After setting DOCKER_ARCH to amd64, the problem of pre-loading admission--1.1.0 image disappears.",Major,TRUE,
"Liberal ""babel"" parser that accepts all SQL dialects",https://issues.apache.org/jira/browse/CALCITE-2280,"Create a parser that accepts all SQL dialects.

It would accept common dialects such as Oracle, MySQL, PostgreSQL, BigQuery. If you have preferred dialects, please let us know in the comments section. (If you're willing to work on a particular dialect, even better!)

We would do this in a new module, inheriting and extending the parser in the same way that the DDL parser in the ""server"" module does.

This would be a messy and difficult project, because we would have to comply with the rules of each parser (and its set of built-in functions) rather than writing the rules as we would like them to be. That's why I would keep it out of the core parser. But it would also have large benefits.

This would be new territory Calcite: as a tool for manipulating/understanding SQL, not (necessarily) for relational algebra or execution.

Some possible uses:

analyze query lineage (what tables and columns are used in a query);
translate from one SQL dialect to another (using the JDBC adapter to generate SQL in the target dialect);
a ""deep"" compatibility mode (much more comprehensive than the current compatibility mode) where Calcite could pretend to be, say, Oracle;
SQL parser as a service: a REST call gives a SQL query, and returns a JSON or XML document with the parse tree.
If you can think of interesting uses, please discuss in the comments.

There are similarities with Uber's QueryParser tool. Maybe we can collaborate, or make use of their test cases.

We will need a lot of sample queries. If you are able to contribute sample queries for particular dialects, please discuss in the comments section. It would be good if the sample queries are based on a familiar schema (e.g. scott or foodmart) but we can be flexible about this.","B: Create a parser that accepts the Oracle SQL dialect.

B: Create a parser that accepts the MySQL SQL dialect.

B: Create a parser that accepts the PostgreSQL SQL dialect.

B: Create a parser that accepts the BigQuery SQL dialect.

B: Develop a new module that inherits and extends the parser in the same way that the DDL parser in the ""server"" module does.

B: Analyze query lineage to determine what tables and columns are used in a query.

B: Implement a feature to translate SQL queries from one dialect to another using the JDBC adapter.

B: Develop a ""deep"" compatibility mode that allows Calcite to mimic the behavior of a specific SQL dialect, such as Oracle.

B: Create a SQL parser as a service that accepts a SQL query and returns a JSON or XML document with the parse tree.

B: Gather sample queries for different SQL dialects based on familiar schemas, such as scott or foodmart.",Major,FALSE,Over-decomposition
Wrong sort elimination when using permuted join order,https://issues.apache.org/jira/browse/DERBY-6148,"I have a query that looks like this: 

SELECT tests.id,tests.item,title FROM tests,item_usage 
WHERE username=? AND user_role>? 
AND item_usage.item=tests.item 
ORDER BY tests.item,title 

The result ordering is by item code followed by title, but the item codes are listed in the order in which they appear in the ITEMS table where they are the primary key rather than in ascending order as expected. If however I change the ORDER BY clause to sort by item_usage.item rather than tests.item, it works correctly, even though the two values are the same! 

The same thing happens in another unrelated query involving item_usage, and the same workaround cures it. 

The relevant tables are defined like so: 

CREATE TABLE item_usage ( 
username VARCHAR(15) NOT NULL, 
item VARCHAR(15) NOT NULL, 
value SMALLINT DEFAULT 0, 
CONSTRAINT item_usage_pk PRIMARY KEY (username,item), 
CONSTRAINT item_usage_1 FOREIGN KEY (username) 
REFERENCES users(username) 
ON DELETE CASCADE, 
CONSTRAINT item_usage_2 FOREIGN KEY (item) 
REFERENCES items(item) 
ON DELETE CASCADE, 
CONSTRAINT item_usage_3 CHECK (value BETWEEN 0 AND 4) 
); 

CREATE TABLE tests ( 
id INTEGER GENERATED ALWAYS AS IDENTITY, 
item VARCHAR(15) NOT NULL, 
title VARCHAR(255) NOT NULL, 
disp SMALLINT NOT NULL DEFAULT 0, 
starttime TIMESTAMP DEFAULT NULL, 
endtime TIMESTAMP DEFAULT NULL, 
offsetx INTEGER NOT NULL DEFAULT 0, 
offsety INTEGER NOT NULL DEFAULT 0, 
rate INTEGER NOT NULL DEFAULT 0, 
duration INTEGER NOT NULL DEFAULT 0, 
calibrate INTEGER NOT NULL DEFAULT 0, 
deadline TIMESTAMP DEFAULT NULL, 
stepsize INTEGER NOT NULL DEFAULT 0, 
interval INTEGER NOT NULL DEFAULT 0, 
stand CHAR(1) DEFAULT NULL, 
hidden CHAR(1) DEFAULT NULL, 
repeated CHAR(1) DEFAULT NULL, 
private CHAR(1) DEFAULT NULL, 
sequential CHAR(1) DEFAULT NULL, 
final CHAR(1) DEFAULT NULL, 
notes CLOB DEFAULT NULL, 
testxml CLOB NOT NULL, 
author VARCHAR(15) NOT NULL, 
time TIMESTAMP NOT NULL, 
CONSTRAINT tests_pk PRIMARY KEY (id), 
CONSTRAINT tests_1 UNIQUE (item, title), 
CONSTRAINT tests_2 FOREIGN KEY (item) 
REFERENCES items(item) 
ON DELETE CASCADE, 
CONSTRAINT tests_3 CHECK (disp BETWEEN 0 AND 100), 
CONSTRAINT tests_4 CHECK (rate BETWEEN 0 AND 100), 
CONSTRAINT tests_5 CHECK (stepsize BETWEEN 0 AND 100) 
); 

If I run the query manually I get this, as expected: 

ID ITEM TITLE 
37 60001 Test 1 
42 60001 Test 2 
51 60001 Test 3 
17 61303 Test 2a 
16 61303 Test 2b 
7 7205731 Test 2a 
8 7205731 Test 2b 

Now, this is actually part of a web app that should turn this into a list of options in a <select> item using the following code: 

while (query.next()) 

{ println(""<option value='"" + query.getInt(""id"") + ""'>"" + encode(query.getString(""item"") + "": "" + query.getString(""title"")) + ""</option>""); } 
What I actually get is this: 

<option value=""17"">61303: Test 2a</option> 
<option value=""16"">61303: Test 2b</option> 
<option value=""7"">7205731: Test 2a</option> 
<option value=""8"">7205731: Test 2b</option> 
<option value=""37"">60001: Test 1</option> 
<option value=""42"">60001: Test 2</option> 
<option value=""51"">60001: Test 3</option> 

The results are sorted by item then by title, but the item order is the order in which they were originally inserted into the items table (where the item and item description are stored, referenced by item_usage.item) rather than by item code. 

I've tried to reproduce this behaviour in a simple example, but without success. I have logged the query plans for both versions; the log output is as follows, with the INCORRECT query (using ORDER BY tests.item) followed later by the CORRECT query (using ORDER BY item_usage.item): 

(moved queryplans to attachment; see attachment queryplans.txt -dagw)","B: The query returns results ordered by the item codes in the order they appear in the ITEMS table rather than in ascending order.

B: Changing the ORDER BY clause to sort by item_usage.item results in correct ordering, even though the values are the same.

B: The same incorrect ordering occurs in another unrelated query involving item_usage.

B: The web application does not display the results in the expected order when generating options for a <select> item.

B: The expected output of the query is not matching the actual output generated by the web application.

B: The query plans for both versions of the query log the differences between the correct and incorrect ordering behavior.",Major,FALSE,Over-analysis
Remove dependency on cloudflare CDN,https://issues.apache.org/jira/browse/TINKERPOP-2775,"As per ASF privacy policy [1], we should not host JS or CSS files on CDNs such as Cloudflare instead, we should opt to host the Fonts/JS/CSS locally on the Apache server. 

We have multiple instances [2] where we are downloading scripts from cloudflare. This task should remove this dependency and host the files locally. 

[1] https://privacy.apache.org/faq/committers.html 
[2] Instances of Fonts/JS/CSS downloaded from CDN (not exhaustive): https://github.com/apache/tinkerpop/blob/master/docs/site/home/index.html#L26 
https://github.com/apache/tinkerpop/blob/master/docs/site/home/index.html#L951","B: Remove the dependency on Cloudflare for hosting JS files and host them locally on the Apache server.  
B: Remove the dependency on Cloudflare for hosting CSS files and host them locally on the Apache server.  
B: Remove the dependency on Cloudflare for hosting Fonts and host them locally on the Apache server.",Major,FALSE,Lacking key information
Fix website issues,https://issues.apache.org/jira/browse/UNOMI-843,"There are multiple issues on the website: 

Embedded YouTube videos can't work anymore due to server Content-Server-Policy changes, we must replace them with links or use the technique described here : https://privacy.apache.org/faq/committers.html 
Google Analytics tag is old, should be replaced with more recent one. Actually it might be better to replace it with Matomo, see https://privacy.apache.org/faq/committers.html (even better would be Unomi in the future ) 
Some links use HTTP instead of HTTPS 
Fix all the issues reported by Apache's web checker: https://whimsy.apache.org/site/project/unomi 
Add the monthly meeting information to the contribute page with the following information (and ideally a calendar link) : https://lists.apache.org/thread/70oo862br3d4g7j8dvnyy3o4z1p0ozfq","B: Embedded YouTube videos can't work anymore due to server Content-Server-Policy changes; we must replace them with links or use the technique described here: https://privacy.apache.org/faq/committers.html.

B: Google Analytics tag is old and should be replaced with a more recent one; it might be better to replace it with Matomo, see https://privacy.apache.org/faq/committers.html.

B: Some links use HTTP instead of HTTPS.

B: Fix all the issues reported by Apache's web checker: https://whimsy.apache.org/site/project/unomi.

B: Add the monthly meeting information to the contribute page with the following information (and ideally a calendar link): https://lists.apache.org/thread/70oo862br3d4g7j8dvnyy3o4z1p0ozfq.",Major,TRUE,
Broken links on RAT's webpage - mailing-lists / prepare webpage for releasing 0.16.1,https://issues.apache.org/jira/browse/RAT-353,"The current webpage at 
https://creadur.apache.org/rat/ 
links to mail-lists.html, which should be 
https://creadur.apache.org/rat/mailing-lists.html 

Check for other dead links and fix them appropriately. 

Further analysis via 
{{$ linkchecker https://creadur.apache.org/rat/ -F text/UTF-8/rat-linkchecker.txt 
}} showed 108 errors on the current RAT webpage.","B: The current webpage at https://creadur.apache.org/rat/ links to mail-lists.html, which should be https://creadur.apache.org/rat/mailing-lists.html.

B: Check for other dead links on the current RAT webpage.

B: Fix the dead links found on the current RAT webpage.

B: Further analysis via {{$ linkchecker https://creadur.apache.org/rat/ -F text/UTF-8/rat-linkchecker.txt }} showed 108 errors on the current RAT webpage.",Major,FALSE,Over-decomposition
bin/solr script doesn't do ps properly on some systems,https://issues.apache.org/jira/browse/SOLR-17112,"On Google's colab, the following fails: 

!wget https://dlcdn.apache.org/solr/solr/9.4.0/solr-9.4.0.tgz && tar -xf solr-9.4.0.tgz && cd solr-9.4.0 && echo `pwd` 

!apt update && apt install bc -y && cd solr-9.4.0 && bin/solr stop -p 8983; bin/solr -c -force -Denable.packages=true 

!cd solr-9.4.0 && bin/solr package add-repo data-import-handler ""https://raw.githubusercontent.com/searchscale/dataimporthandler/master/repo/"" 
If I add the following before the last line, it works: 

!cat solr-9.4.0/bin/solr|sed -e 's:ps -f -p:ps -fww -p:g' > tmp; cp tmp solr-9.4.0/bin/solr; chmod +x solr-9.4.0/bin/solr 
I think that extra ""ww"" is needed to make sure Solr works fine on all systems. FYI dep4b.","B: On Google's colab, the command `!wget https://dlcdn.apache.org/solr/solr/9.4.0/solr-9.4.0.tgz && tar -xf solr-9.4.0.tgz && cd solr-9.4.0 && echo `pwd`` fails.

B: The command `!apt update && apt install bc -y && cd solr-9.4.0 && bin/solr stop -p 8983; bin/solr -c -force -Denable.packages=true` fails.

B: The command `!cd solr-9.4.0 && bin/solr package add-repo data-import-handler ""https://raw.githubusercontent.com/searchscale/dataimporthandler/master/repo/""` fails without modification.

B: Adding the command `!cat solr-9.4.0/bin/solr|sed -e 's:ps -f -p:ps -fww -p:g' > tmp; cp tmp solr-9.4.0/bin/solr; chmod +x solr-9.4.0/bin/solr` before the last line makes it work.

B: The extra ""ww"" in the `sed` command is needed to ensure Solr works fine on all systems.",Major,FALSE,Over-analysis
[pdmodel.font.PDFont] Error: Could not parse predefined CMAP file for 'PDFXC-Indentity0-0',https://issues.apache.org/jira/browse/PDFBOX-940,"Hi, 

when i am trying to upload a pdf document the following error is thrown in the tomcat.. i am using pdfbox-1.4.0.jar.. 

17:29:33,465 ERROR [pdmodel.font.PDFont] Error: Could not parse predefined CMAP file for 'PDFXC-Indentity0-0' 

please find the solution","B: Error thrown when trying to upload a PDF document in Tomcat.  
B: Issue with parsing predefined CMAP file for 'PDFXC-Indentity0-0' when using pdfbox-1.4.0.jar.",Major,FALSE,Over-decomposition
RPC Sasl QOP is broken,https://issues.apache.org/jira/browse/HADOOP-9816,HADOOP-9421 broke the handling of SASL wrapping for RPC QOP integrity and privacy options.,"B: HADOOP-9421 broke the handling of SASL wrapping for RPC QOP integrity options.  
B: HADOOP-9421 broke the handling of SASL wrapping for RPC QOP privacy options.",Blocker,TRUE,
cordova-plugin-geolocation should reverse permissions request in ios8,https://issues.apache.org/jira/browse/CB-8826,"I would like to suggest a change to how the geolocation plugin requests 
permissions in iOS8. In the event that both iOS8 NSLocation usage 
permissions exist, I suggest that we first request the least permissive one 
(NSLocationWhenInUseUsageDescription). 

This should amount to simply reversing the logic in CDVLocation.m: 

if([[NSBundle mainBundle] 
objectForInfoDictionaryKey:@""NSLocationWhenInUseUsageDescription""]) 
{ 
[self.locationManager requestWhenInUseAuthorization]; 
} else if([[NSBundle mainBundle] objectForInfoDictionaryKey:@ 
""NSLocationAlwaysUsageDescription""]) { 
[self.locationManager requestAlwaysAuthorization]; 
} 
I have a use case where an app launches with both descriptions set, but 
depending on client configuration the ""AlwaysInUse"" permission may not be necessary. As the logic is written now, the plugin will always request that one, which could look a bit extreme to the end user.","B: Suggest a change to how the geolocation plugin requests permissions in iOS8 by reversing the logic to first request the least permissive permission (NSLocationWhenInUseUsageDescription).

B: Modify the logic in CDVLocation.m to first check for NSLocationWhenInUseUsageDescription before checking for NSLocationAlwaysUsageDescription.

B: Create a use case where an app launches with both NSLocation usage descriptions set and demonstrate that the ""AlwaysInUse"" permission may not be necessary depending on client configuration. 

B: Ensure that the updated permission request logic does not always request the more permissive ""AlwaysInUse"" permission to avoid appearing extreme to the end user.",Minor,FALSE,Over-decomposition
TextPosition.getHeight() returns erroneous value for some PDFs,https://issues.apache.org/jira/browse/PDFBOX-1001,"For a PDF that worked fine under 1.2.1 the height value returned is negative and the wrong value (i.e. using Math.abs() won't fix it). Other PDFs work fine. 
PDF Debug shows ""Creator:Crystal Reports"" and ""Producer:PDF-XChange (XCPRO30.DLL v3.30.0064) (Windows 2k)"" 
And when examining the 'Stream' items, the text is not what displays. 

Any suggestions on what to look for so that I can do differential analysis against other PDFs to see what they do/not have in common with this one? 
(It's client data so I can't post the PDF. ) 

It's stopping us from moving off 1.2.1 (and later versions fix another issue we have of seeing question marks instead of the actual characters).","B: The height value returned for a specific PDF is negative and incorrect, and using Math.abs() does not resolve the issue.

B: Other PDFs work fine, indicating that the issue is isolated to this specific PDF.

B: PDF Debug shows the creator as ""Crystal Reports"" and the producer as ""PDF-XChange (XCPRO30.DLL v3.30.0064) (Windows 2k)"".

B: The 'Stream' items in the PDF do not display the text as expected.

B: Suggestions are needed for conducting differential analysis against other PDFs to identify differences or commonalities with the problematic PDF.

B: The issue is preventing the transition from version 1.2.1 to later versions, which resolve a separate issue related to character display.",Major,FALSE,Over-decomposition
Getting image with black background when converting from PDF to Image!!,https://issues.apache.org/jira/browse/PDFBOX-1023,"Everytime I try to conver a PDF file with a graphic on it, to Image (PNG) I get a black background beneath the graphic, where the background is white originally, here's my code: 

PDDocument document = PDDocument.load(new File(""C:\\export_settings 
testReport.pdf"")); 
List<PDPage> pages = document.getDocumentCatalog().getAllPages(); 

for (int i = 0; i < pages.size(); i++) 

{ PDPage singlePage = pages.get(i); BufferedImage buffImage = singlePage.convertToImage(); ImageIO.write(buffImage, ""PNG"", new File(""C:\\export_settings\\page"" + i + "".png"")); } 
The image quality is good, except for this, I tried with two different methos but I got the same result, please help me, thanks!","B: Every time I try to convert a PDF file with a graphic on it to an Image (PNG), I get a black background beneath the graphic, where the background is white originally. 

B: The code I am using is as follows: 
PDDocument document = PDDocument.load(new File(""C:\\export_settings testReport.pdf"")); 
List<PDPage> pages = document.getDocumentCatalog().getAllPages(); 
for (int i = 0; i < pages.size(); i++) { 
    PDPage singlePage = pages.get(i); 
    BufferedImage buffImage = singlePage.convertToImage(); 
    ImageIO.write(buffImage, ""PNG"", new File(""C:\\export_settings\\page"" + i + "".png"")); 
}

B: The image quality is good, except for the black background issue.

B: I tried using two different methods to convert the PDF to PNG but got the same result.",Major,FALSE,Over-decomposition
Copyright Notice of Website outdated,https://issues.apache.org/jira/browse/DIR-240,"Although in 2009, the Copyright notice on our website is still 

 2003-2008, The Apache Software Foundation - Privacy Policy 

For instance here (footer) 
http://directory.apache.org/ 

The templates need an update (unfortunately, I do not know how to accomplish this)","B: Update the copyright notice on the website footer from 2003-2008 to the current year.  
B: Update the templates used on the website to reflect the new copyright notice.  
B: Provide documentation or instructions on how to update the website templates.",Major,FALSE,Over-decomposition
Wire encryption is broken,https://issues.apache.org/jira/browse/HBASE-11149,"Upon some testing with the QOP configuration (hbase.rpc.protection), discovered that RPC doesn't work with ""integrity"" and ""privacy"" values for the configuration key. I was using 0.98.x for testing but I believe the issue is there in trunk as well (haven't checked 0.96 and 0.94).","B: RPC does not function with the ""integrity"" value for the hbase.rpc.protection configuration key.  
B: RPC does not function with the ""privacy"" value for the hbase.rpc.protection configuration key.  
B: The issue with hbase.rpc.protection may be present in the trunk version of the software.  
B: The issue with hbase.rpc.protection has not been verified in versions 0.96 and 0.94.",Major,FALSE,Over-decomposition
Classpath in XML report is wrong,https://issues.apache.org/jira/browse/SUREFIRE-164,"The XML report contains in the property java.class.path Maven's classpath, but not the class path used to execute the tests.","B: The XML report contains the property java.class.path Maven's classpath.  
B: The XML report does not contain the class path used to execute the tests.",Minor,FALSE,Over-decomposition
Added restriction to historic queries on web UI,https://issues.apache.org/jira/browse/HIVE-17701,"The HiveServer2 Web UI (HIVE-12550) shows recently completed queries. 
However, a user can see the queries run by other users as well, and that is a security/privacy concern. 
Only admin users should be allowed to see queries from other users (similar to behavior of display for configs, stack trace etc).","B: The HiveServer2 Web UI shows recently completed queries run by other users, which is a security/privacy concern. 

B: Only admin users should be allowed to see queries from other users in the HiveServer2 Web UI. 

B: The behavior of displaying queries from other users should be similar to the display for configs, stack trace, etc.",Major,FALSE,Over-decomposition
Cannot ship code hints without also shipping file-system paths,https://issues.apache.org/jira/browse/FLEX-23026,"For privacy reasons, developers need a way to ship code hints in .swc files without also shipping file-system paths. A new compiler option should be added, perhaps: -exclude-file-paths. 

Currently, file paths can only be disabled by setting debug=false, but that results in no hints and no paths.","B: A new compiler option should be added to allow developers to ship code hints in .swc files without file-system paths. 

B: The new compiler option should be named -exclude-file-paths.

B: Currently, file paths can only be disabled by setting debug=false, which disables all hints and paths.",Major,FALSE,Over-decomposition
Long rendering time,https://issues.apache.org/jira/browse/PDFBOX-3791,"Attached pdf file takes too long (more then 9 secs) to render in PDFDebugger (this is a simplified version of a real life pdf that I can not publish for privacy reasons, it takes 57 seconds to render and it contains 56 images and some text). 

I have tried with the options provided in https://pdfbox.apache.org/2.0/getting-started.html but performance is the same","B: Attached pdf file takes too long (more than 9 secs) to render in PDFDebugger. 

B: The attached pdf file takes 57 seconds to render and contains 56 images and some text.

B: Performance remains the same despite trying the options provided in https://pdfbox.apache.org/2.0/getting-started.html.",Major,FALSE,Over-decomposition
Security: passwords logging and file permisions,https://issues.apache.org/jira/browse/DRILL-6189,"Prerequisites: 
1. Log level is set to ""all"" in the conf/logback.xml: 

<logger name=""org.apache.drill"" additivity=""false""> 
<level value=""all"" /> 
<appender-ref ref=""FILE"" /> 
</logger> 
2. PLAIN authentication mechanism is configured: 

security.user.auth: { 
enabled: true, 
packages += ""org.apache.drill.exec.rpc.user.security"", 
impl: ""pam"", 
pam_profiles: [ ""sudo"", ""login"" ] 
} 
Steps: 
1. Start the drillbits 
2. Connect by sqlline: 

/opt/mapr/drill/drill-1.13.0/bin/sqlline -u ""jdbc:drill:zk=node1:5181;"" -n user1 -p 1111 
Expected result: Logs shouldn't contain clear-text passwords 

Actual results: During the drillbit startup or establishing connections via the jdbc or odbc, the following lines appear in the drillbit.log: 

properties { 
key: ""password"" 
value: ""1111"" 
} 
Same thing happens with storage configuration data, everything, including passwords is being logged to file. 

Another issue: 

Currently Drill config files has the permissions 0644: 

-rw-r--r--. 1 mapr mapr 1081 Nov 16 14:42 core-site-example.xml 
-rwxr-xr-x. 1 mapr mapr 1807 Dec 19 11:55 distrib-env.sh 
-rw-r--r--. 1 mapr mapr 1424 Nov 16 14:42 distrib-env.sh.prejmx 
-rw-r--r--. 1 mapr mapr 1942 Nov 16 14:42 drill-am-log.xml 
-rw-r--r--. 1 mapr mapr 1279 Dec 19 11:55 drill-distrib.conf 
-rw-r--r--. 1 mapr mapr 117 Nov 16 14:50 drill-distrib-mem-qs.conf 
-rw-r--r--. 1 mapr mapr 6016 Nov 16 14:42 drill-env.sh 
-rw-r--r--. 1 mapr mapr 1855 Nov 16 14:50 drill-on-yarn.conf 
-rw-r--r--. 1 mapr mapr 6913 Nov 16 14:42 drill-on-yarn-example.conf 
-rw-r--r--. 1 mapr mapr 1135 Dec 19 11:55 drill-override.conf 
-rw-r--r--. 1 mapr mapr 7820 Nov 16 14:42 drill-override-example.conf 
-rw-r--r--. 1 mapr mapr 3136 Nov 16 14:42 logback.xml 
-rw-r--r--. 1 mapr mapr 668 Nov 16 14:51 warden.drill-bits.conf 
-rw-r--r--. 1 mapr mapr 1581 Nov 16 14:42 yarn-client-log.xml 
As they may contain some sensitive information, like passwords or secret keys, they cannot be viewable to everyone. So I suggest to reduce the permissions at least to 0640.","B: Logs should not contain clear-text passwords during drillbit startup or when establishing connections via JDBC or ODBC. 

B: Sensitive information, including passwords, is being logged to the drillbit.log file.

B: Drill configuration files have permissions set to 0644 and should be reduced to at least 0640 to prevent unauthorized access to sensitive information.",Major,FALSE,Over-decomposition
Facebook Like iframe too narrow when in topbar,https://issues.apache.org/jira/browse/MSKINS-92,"See Apache Syncope website at http://syncope.apache.org 

On the top right you can see the Facebook Like button rendered by 

<iframe src=""http://www.facebook.com/plugins/like.php?href=http://syncope.apache.org/&send=false&layout=button_count&show-faces=false&action=like&colorscheme=dark"" 
scrolling=""no"" frameborder=""0"" 
style=""border:none; width:80px; height:20px; margin-top: 10px;"" class=""pull-right"" ></iframe> 
when changing style to 

""border:none; width:100px; height:20px; margin-top: 10px;"" 
the right-side box is rendered correctly.","B: The Facebook Like button iframe is not displaying correctly on the Apache Syncope website. 

B: The style of the Facebook Like button iframe should be changed to ""border:none; width:100px; height:20px; margin-top: 10px;"" for proper rendering.",Minor,FALSE,Incorrect Interpretation of Solutions
UUID replacement,https://issues.apache.org/jira/browse/CB-49,"reported at: https://github.com/phonegap/phonegap-iphone/issues/238 
by: https://github.com/sandstrom 

As you might have read iOS 5 will remove the UDID (http://techcrunch.com/2011/08/19/apple-ios-5-phasing-out-udid/). 

This is an excellent alternative and it would be nice if you would implement something along these lines to keep the functionality. The idea of hashing together with the bundle id is great, because it makes it impossible to track across applications, which is what apple wanted to fix (although it can be circumvented that would only anger them, and tracking across apps isn't required for most apps anyway). 

https://github.com/gekitz/UIDevice-with-UniqueIdentifier-for-iOS-5","B: iOS 5 will remove the UDID, which needs to be addressed in the application.  
B: Implement an alternative to UDID that maintains functionality while complying with iOS 5 guidelines.  
B: Explore the idea of using a hash combined with the bundle ID to prevent tracking across applications.  
B: Ensure that the solution does not allow for circumventing Apple's restrictions on tracking.  
B: Assess the necessity of tracking across apps for the application's functionality.",Blocker,FALSE,Over-decomposition
Skip whitespaces when resolving a XRef,https://issues.apache.org/jira/browse/PDFBOX-1737,"Oleg Krechowetzki reported an issue with the non sequential parser via private mail. He provided a working solution and a test pdf which can't be attached due to privacy reasons. 

The following exception occurs when parsing the pdf in question using the non sequential parser: 

Caused by: java.io.IOException: Error: Expected a long type, actual='xref' 
at org.apache.pdfbox.pdfparser.BaseParser.readLong(BaseParser.java:1668) 
at org.apache.pdfbox.pdfparser.BaseParser.readObjectNumber(BaseParser.java:1598) 
at org.apache.pdfbox.pdfparser.NonSequentialPDFParser.parseXrefObjStream(NonSequentialPDFParser.java:458)","B: Oleg Krechowetzki reported an issue with the non sequential parser via private mail. 

B: The provided test pdf cannot be attached due to privacy reasons. 

B: An exception occurs when parsing the pdf in question using the non sequential parser.

B: The exception is a java.io.IOException with the message: ""Error: Expected a long type, actual='xref'"".

B: The exception originates from the method org.apache.pdfbox.pdfparser.BaseParser.readLong(BaseParser.java:1668).

B: The exception is also related to the method org.apache.pdfbox.pdfparser.BaseParser.readObjectNumber(BaseParser.java:1598).

B: The exception is triggered in the method org.apache.pdfbox.pdfparser.NonSequentialPDFParser.parseXrefObjStream(NonSequentialPDFParser.java:458).",Major,FALSE,Over-analysis
iOS 11 Error When Taking Picture Missing NSPhotoLibraryAddUsageDescription,https://issues.apache.org/jira/browse/CB-13332,"I am using this plugin to take a picture, but in iOS11 I receive the following error: 

* 
This app has crashed because it attempted to access privacy-sensitive data without a usage description. The app's Info.plist must contain an NSPhotoLibraryAddUsageDescription key with a string value explaining to the user how the app uses this data.* 

Note: I have descriptions for CAMERA_USAGE_DESCRIPTION and PHOTOLIBRARY_USAGE_DESCRIPTION. I was able to take a picture in iOS 10.","B: This app has crashed because it attempted to access privacy-sensitive data without a usage description. The app's Info.plist must contain an NSPhotoLibraryAddUsageDescription key with a string value explaining to the user how the app uses this data.

B: The app is missing the NSPhotoLibraryAddUsageDescription key in its Info.plist file. 

B: The app successfully took a picture in iOS 10, indicating the issue is specific to iOS 11. 

B: There are existing descriptions for CAMERA_USAGE_DESCRIPTION and PHOTOLIBRARY_USAGE_DESCRIPTION that need to be reviewed for completeness in iOS 11.",Trivial,FALSE,Over-decomposition
Tighten HFileLink api to enable non-snapshot uses,https://issues.apache.org/jira/browse/HBASE-12749,"In HBASE-12332 we'd like to use the FileLink's IO redirecting powers but want to be able to specify arbitrary alternate link paths and not be tied to the SnapshotFileLink file pattern (aka, table=region-hfile). 

To do this we need change the constructors and some internals so that it is more generic. Along the way, we remove the FileStatus constructor arguments in favor of Path's and reduce the number of ways to create HFileLinks, and tighten up the scope privacy of many methods.","B: In HBASE-12332, we need to change the constructors of FileLink to allow specifying arbitrary alternate link paths instead of being tied to the SnapshotFileLink file pattern.

B: We need to modify the internals of FileLink to make it more generic for different link path specifications.

B: We need to remove the FileStatus constructor arguments in FileLink and replace them with Path's.

B: We need to reduce the number of ways to create HFileLinks to simplify the process.

B: We need to tighten up the scope privacy of many methods in FileLink.",Major,FALSE,Over-decomposition
"PDA renderers for tr:page, tr:panelPage and tr:panelPageHeader need to render facets more in line with desktop counterparts",https://issues.apache.org/jira/browse/TRINIDAD-1303,"PDA renderers for tr:page, tr:panelPage and tr:panelPageHeader support different sets of facets and render supported facets at different positions. This makes difficult to develop mobile applications.","B: PDA renderers for tr:page support a different set of facets compared to tr:panelPage and tr:panelPageHeader.  

B: PDA renderers for tr:panelPage support a different set of facets compared to tr:page and tr:panelPageHeader.  

B: PDA renderers for tr:panelPageHeader support a different set of facets compared to tr:page and tr:panelPage.  

B: PDA renderers for tr:page, tr:panelPage, and tr:panelPageHeader render supported facets at different positions.  

B: The inconsistency in facets and their positions in PDA renderers makes it difficult to develop mobile applications.",Minor,FALSE,Over-decomposition
CCITTFaxG31DDecodeInputStream - Extended codes have wrong length,https://issues.apache.org/jira/browse/PDFBOX-1233,"When dealing with large fax images there are Extended Make Up Codes. 
They are added to the tree as ... 

buildUpMakeUp(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT); 
buildUpMakeUp(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT); 

Accept, the length is 0 based not starting at 1792. 

The quick hack is to create a new method so the length of the node is correct 

private static void buildUpMakeUpLong(short[] codes, 
NonLeafLookupTreeNode root) 
{ 
for (int len = 0, c = codes.length; len < c; len++) 

{ LookupTreeNode leaf = new MakeUpTreeNode((len + 28) * 64); addLookupTreeNode(codes[len], root, leaf); } 
} 

as thus ... 

buildUpMakeUpLong(LONG_MAKE_UP, WHITE_LOOKUP_TREE_ROOT); 
buildUpMakeUpLong(LONG_MAKE_UP, BLACK_LOOKUP_TREE_ROOT);","B: When dealing with large fax images, Extended Make Up Codes need to be added to the tree using the method buildUpMakeUp. 

B: The length for the node in the method buildUpMakeUp should be adjusted to start at 0 instead of 1792.

B: A new method called buildUpMakeUpLong should be created to correctly handle the length of the node.

B: The loop inside the buildUpMakeUpLong method correctly initializes LookupTreeNode instances with the formula (len + 28) * 64.

B: The method buildUpMakeUpLong should be called with LONG_MAKE_UP and WHITE_LOOKUP_TREE_ROOT as arguments.

B: The method buildUpMakeUpLong should also be called with LONG_MAKE_UP and BLACK_LOOKUP_TREE_ROOT as arguments.",Major,FALSE,Over-decomposition
Make i18n/LocalizedDisplay.sql and i18n/LocalizedConnectionAttribute.sql behave equally on different platforms,https://issues.apache.org/jira/browse/DERBY-1726,"Myrna van Lunteren commented on DERBY-244: 

The one remark I have is that I still cannot get the LocalizedDisplay.sql and LocalizedConnectionAttribute.sql test from the i18n directory to behave the same under windows and Linux (with sun jdk 1.4.2.). 
For windows, I had to update the masters for these tests, but running them on Linux still failed for me. 
With jdk131, ibm131 and ibm142 the LocalizedDisplay.sql test hung, and LocalizedConnectionAttribute exits with a MalformedInputException. 
It would be nice if we could figure out a way to add these tests to the suites... 

¡ª stack of LocalizedConnectionAttribute on Linux ¡ª 
Exception in thread ""main"" sun.io.MalformedInputException 
at sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java(Compiled Code)) 
at sun.nio.cs.StreamDecoder$ConverterSD.convertInto(StreamDecoder.java:287) 
at sun.nio.cs.StreamDecoder$ConverterSD.implRead(StreamDecoder.java:337) 
at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:223) 
at java.io.InputStreamReader.read(InputStreamReader.java:208) 
at java.io.BufferedReader.fill(BufferedReader.java:153) 
at java.io.BufferedReader.readLine(BufferedReader.java:316) 
at java.io.BufferedReader.readLine(BufferedReader.java:379) 
at org.apache.derbyTesting.functionTests.harness.RunTest.setDirectories(RunTest.java:729) 
at org.apache.derbyTesting.functionTests.harness.RunTest.main(RunTest.java:262) 
----------------------------------------------------------------------------","B: The LocalizedDisplay.sql test behaves differently under Windows and Linux with Sun JDK 1.4.2. 

B: The masters for the LocalizedDisplay.sql and LocalizedConnectionAttribute.sql tests need to be updated for Windows.

B: The LocalizedDisplay.sql test hangs when run on Linux with JDK 1.3.1, IBM JDK 1.3.1, and IBM JDK 1.4.2.

B: The LocalizedConnectionAttribute.sql test exits with a MalformedInputException when run on Linux with JDK 1.4.2.

B: A solution is needed to add the LocalizedDisplay.sql and LocalizedConnectionAttribute.sql tests to the test suites.",Minor,FALSE,Over-decomposition
java.awt.geom.IllegalPathStateException: missing initial moveto in path definition,https://issues.apache.org/jira/browse/PDFBOX-2728,"I get this exception :

java.awt.geom.IllegalPathStateException: missing initial moveto in path definition
at java.awt.geom.Path2D$Float.needRoom(Path2D.java:280)
at java.awt.geom.Path2D.closePath(Path2D.java:1769)
at org.apache.pdfbox.rendering.PageDrawer.closePath(PageDrawer.java:693)
at org.apache.pdfbox.contentstream.operator.graphics.ClosePath.process(ClosePath.java:35)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processOperator(PDFStreamEngine.java:788)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStreamOperators(PDFStreamEngine.java:454)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:425)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:398)
at org.apache.pdfbox.contentstream.PDFStreamEngine.processPage(PDFStreamEngine.java:164)
at org.apache.pdfbox.rendering.PageDrawer.drawPage(PageDrawer.java:164)
at org.apache.pdfbox.rendering.PDFRenderer.renderPage(PDFRenderer.java:213)
similar to PDFBOX-2189.

I can't include the PDF file for privacy reason but I think a similar solution applied for the other bug could fix this problem too","B: I get the exception: java.awt.geom.IllegalPathStateException: missing initial moveto in path definition.

B: The exception occurs at java.awt.geom.Path2D$Float.needRoom(Path2D.java:280).

B: The exception occurs at java.awt.geom.Path2D.closePath(Path2D.java:1769).

B: The exception occurs at org.apache.pdfbox.rendering.PageDrawer.closePath(PageDrawer.java:693).

B: The exception occurs at org.apache.pdfbox.contentstream.operator.graphics.ClosePath.process(ClosePath.java:35).

B: The exception occurs at org.apache.pdfbox.contentstream.PDFStreamEngine.processOperator(PDFStreamEngine.java:788).

B: The exception occurs at org.apache.pdfbox.contentstream.PDFStreamEngine.processStreamOperators(PDFStreamEngine.java:454).

B: The exception occurs at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:425).

B: The exception occurs at org.apache.pdfbox.contentstream.PDFStreamEngine.processPage(PDFStreamEngine.java:164).

B: The exception occurs at org.apache.pdfbox.rendering.PageDrawer.drawPage(PageDrawer.java:164).

B: The exception occurs at org.apache.pdfbox.rendering.PDFRenderer.renderPage(PDFRenderer.java:213).

B: A similar solution applied for other bugs could fix this problem too.",Major,FALSE,Over-analysis
Regression in 2.0.19,https://issues.apache.org/jira/browse/PDFBOX-4805,"Joel Hirsh reported a regression with PDFTextStripper which was introduced with 2.0.19, see his post on users@ for details. 

He can't share the pdf in questions due to privacy but did some debugging and found out that PDFBOX-4760 is the case for that regression. I accidentally committed some unrelated code which leads to bad text extraction results. As the code targets some corner cases it didn't came up as an issue when running our pre release tests. The issue is limited to the 2.0 trunk.","B: Regression with PDFTextStripper introduced with version 2.0.19.  
B: Bad text extraction results due to unrelated code committed accidentally.  
B: Issue is limited to the 2.0 trunk.  
B: PDFBOX-4760 is related to the regression found during debugging.  
B: Pre-release tests did not catch the issue as it targets corner cases.",Major,FALSE,Over-decomposition
Invalidated signature signing pdf twice,https://issues.apache.org/jira/browse/PDFBOX-4261,"A customer sent us a pdf that has this problem: when it is signed twice by pdfbox 1.8.x the second signature invalidates the first one. 

If we apply the same procedure using pdfbox 2.0.x the problem doesn't occur, but the customer required java 1.5 so we can't switch to the new version in this case. 

For privacy purposes we had anonymized the original PDF file by editing 3 stream inside the pdf, without altering the original structure. So the file ""92752146_noSign_anonymous.pdf"" you can find in attachement has not the original text/image streams, but reproduces the problem as the original one. 

Thank you in advance","B: A customer reported that when a PDF is signed twice using pdfbox 1.8.x, the second signature invalidates the first one.

B: The customer requires Java 1.5 and cannot switch to pdfbox 2.0.x, which does not exhibit the same problem.

B: The original PDF file was anonymized by editing 3 streams without altering the original structure, resulting in a file named ""92752146_noSign_anonymous.pdf"" that reproduces the problem.",Major,FALSE,Over-decomposition
PHP feature is not activated after Oracle JS Parser Implementation is installed,https://issues.apache.org/jira/browse/NETBEANS-2847,"While re-opening a project that had been imported from previous version and was marked ""Broken"". It re-opened and was no longer marked ""broken"", but then this exception occurred.

","B: While re-opening a project that had been imported from a previous version, it was marked ""Broken"".  

B: After re-opening the project, it was no longer marked ""Broken"".  

B: An exception occurred after the project was re-opened.",Major,FALSE,Over-decomposition
Subversion demands unnecessary access to parent directories of operations,https://issues.apache.org/jira/browse/SVN-3242,"We have updated to the latest 1.5 version of svn. Now we have a problem with
some operations. E.g.
C:\work>svn -m version cp https://someserver.com/svn/ProjectName/2008/trunk
-r56318 https://someserver.com/svn/ProjectName/2008/tags/8.11.07
svn: Server sent unexpected return value (403 Forbidden) in response to PROPFIND
request for '/svn'

C:\work>svn --version
svn, version 1.5.0 (r31699)
compiled Jun 23 2008, 12:59:48

but the same command line works fine with svn 1.4.

Our svn-server is configured to give no access to root folder
(https://someserver.com/svn/) but gives rw access to project folders
(https://someserver.com/svn/ProjectName/), and seems svn1.5 want to do
something with root even it I work only with project folders.
Moreover this command is not only one which gives that problem, in some
circumstances (not sure which, but I guess if new files were added) it
gives the same error even for ""svn up"" command.
Original issue reported by kan","B: We have updated to the latest 1.5 version of svn and are experiencing a 403 Forbidden error with the command `svn -m version cp https://someserver.com/svn/ProjectName/2008/trunk -r56318 https://someserver.com/svn/ProjectName/2008/tags/8.11.07`.

B: The command `svn --version` reports the version as svn, version 1.5.0 (r31699) compiled Jun 23 2008, 12:59:48.

B: The same command line works fine with svn version 1.4, indicating a regression in the 1.5 version functionality.

B: Our svn-server is configured to give no access to the root folder (https://someserver.com/svn/) but provides read-write access to project folders (https://someserver.com/svn/ProjectName/).

B: It seems that svn 1.5 attempts to access the root folder even when working only with project folders.

B: In some circumstances, possibly related to the addition of new files, the `svn up` command also returns a 403 Forbidden error.",Critical,FALSE,Over-decomposition
(ios) Present notification view controller by inappbrowser view controller,https://issues.apache.org/jira/browse/CB-13555,"When inappbrowser window is shown, if main uiwebview or wkwebview calls cordova Dialog plugin method to show the dialog view, the dialog should show to user on top of the inappbrowser view controller. 

However, currently the dialog view is shown behind the inappbrowser view, so user cannot see it or click button on the dialog 

An similar issue was reported for barcode scanner plugin at 
https://github.com/phonegap/phonegap-plugin-barcodescanner/issues/570 

The issue can be repeated with the below method 
function confirm(){ 
var win = window.open( ""https://www.google.com"", ""_blank"" ); 
win.addEventListener( ""loadstop"", function() { 
setTimeout(function() { 
function onConfirm(buttonIndex) 

{ console.log('You selected button ' + buttonIndex); } 
navigator.notification.confirm( 
'You are the winner!', // message 
onConfirm, // callback to invoke with index of button pressed 
'Game Over', // title 
['Restart','Exit'] // buttonLabels 
); 
}, 1000 ); 
}); 
}","B: When the inappbrowser window is shown, the dialog view from the Cordova Dialog plugin should appear on top of the inappbrowser view controller.

B: Currently, the dialog view from the Cordova Dialog plugin is shown behind the inappbrowser view, preventing the user from seeing or interacting with it.

B: The bug can be reproduced using the provided `confirm` function which opens a new window and attempts to display a dialog after a delay.",Major,FALSE,Over-analysis
Possible wrong calculation of header length,https://issues.apache.org/jira/browse/MIME4J-265,"I've implemented a sort of mail server and I have many threads listening for incoming emails.
I'm using mime4j to parse javamail Message.

I had only one case of:
Caused by: org.apache.james.mime4j.io.MaxHeaderLengthLimitException: Maximum header length limit exceeded
at org.apache.james.mime4j.stream.DefaultFieldBuilder.append(DefaultFieldBuilder.java:63)
at org.apache.james.mime4j.stream.MimeEntity.readRawField(MimeEntity.java:212)
at org.apache.james.mime4j.stream.MimeEntity.nextField(MimeEntity.java:258)

Looking at the code of DefaultFieldBuilder, it seems that the check over line length is not done on the single line but on the overall header, I'm refering to this line:

if (this.maxlen > 0 && this.buf.length() + len >= this.maxlen) {
Why should you add ""this.buf.length"" ?
I know that there is no limit on header length, but only in its lines.

I can't attach my eml for privacy reasons but I can confirm that I have no too much long line

Thanks","B: The mail server implementation has many threads listening for incoming emails. 

B: There is an exception occurring: org.apache.james.mime4j.io.MaxHeaderLengthLimitException: Maximum header length limit exceeded.

B: The exception is triggered in the DefaultFieldBuilder class, specifically in the append method.

B: The header length check in DefaultFieldBuilder is being done on the overall header length instead of individual line length.

B: The line of code causing concern is: if (this.maxlen > 0 && this.buf.length() + len >= this.maxlen).

B: There is a misunderstanding regarding the header length limits; the limit should apply to individual lines, not the overall header.

B: The user cannot provide the eml file for privacy reasons but confirms that there are no excessively long lines in the headers.",Major,FALSE,Over-analysis
Issue with SQL Server Database with JUDDI 3.3.6 : The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000),https://issues.apache.org/jira/browse/JUDDI-999,"We were using SQLServer with JUDDI 3.0.4. It is working fine so far.

Now, we are trying to move to JUDDI version 3.3.6. We are encountering following issue on start-up.

Caused by: <openjpa-2.3.0-r422266:1540826 nonfatal general error> org.apache.openjpa.persistence.PersistenceException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002]Caused by: <openjpa-2.3.0-r422266:1540826 nonfatal general error> org.apache.openjpa.persistence.PersistenceException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002] at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:559) at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:455) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.synchronizeMappings(JDBCBrokerFactory.java:160) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.synchronizeMappings(JDBCBrokerFactory.java:164) at org.apache.openjpa.jdbc.kernel.JDBCBrokerFactory.newBrokerImpl(JDBCBrokerFactory.java:122) at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:209) at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:155) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:226) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:153) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:59) at org.apache.juddi.config.PersistenceManager.getEntityManager(PersistenceManager.java:48) at org.apache.juddi.config.AppConfig.getPersistentConfiguration(AppConfig.java:174) at org.apache.juddi.config.AppConfig.loadConfiguration(AppConfig.java:160) at org.apache.juddi.config.AppConfig.<init>(AppConfig.java:82) at org.apache.juddi.config.AppConfig.getInstance(AppConfig.java:272) at org.apache.juddi.config.AppConfig.getConfiguration(AppConfig.java:298) at org.apache.juddi.api.impl.AuthenticatedService.<init>(AuthenticatedService.java:75) at org.apache.juddi.api.impl.UDDIInquiryImpl.<init>(UDDIInquiryImpl.java:88) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Unknown Source) at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:142) ... 36 moreCaused by: org.apache.openjpa.lib.jdbc.ReportingSQLException: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000). {stmnt 1090863652 CREATE TABLE j3_tmodel_instance_info (id BIGINT NOT NULL, instance_parms VARCHAR(8192), tmodel_key VARCHAR(255) NOT NULL, entity_key VARCHAR(255) NOT NULL, PRIMARY KEY (id))} [code=131, state=S0002] at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:219) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:203) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.access$700(LoggingConnectionDecorator.java:59) at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingStatement.executeUpdate(LoggingConnectionDecorator.java:914) at org.apache.openjpa.lib.jdbc.DelegatingStatement.executeUpdate(DelegatingStatement.java:118) at org.apache.openjpa.jdbc.schema.SchemaTool.executeSQL(SchemaTool.java:1231) at org.apache.openjpa.jdbc.schema.SchemaTool.createTable(SchemaTool.java:976) at org.apache.openjpa.jdbc.schema.SchemaTool.add(SchemaTool.java:552) at org.apache.openjpa.jdbc.schema.SchemaTool.add(SchemaTool.java:364) at org.apache.openjpa.jdbc.schema.SchemaTool.run(SchemaTool.java:341) at org.apache.openjpa.jdbc.meta.MappingTool.record(MappingTool.java:505) ... 58 more


The error is ""The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000)""

Eventually the table ""j3_tmodel_instance_info"" failed to create. We are using SQLServer version 12.0.5207.0. It poses a limit on varchar fields to 8000.

We have tried modifying the column length in class ""TmodelInstanceInfo"" and redeploying the app, however then it starts giving other issue.

The type ""class org.apache.juddi.model.TmodelInstanceInfo"" has not been enhanced.


Could anyone please help us. We are in RED flag and our application cease to work after update to JUDDI 3.3.6

Any help be greatly appreciated. Kindly let me know if I need to provide more information to assist investigation.

Thanks a lot","B: The size (8192) given to the column 'instance_parms' exceeds the maximum allowed for any data type (8000).

B: The table ""j3_tmodel_instance_info"" failed to create.

B: We are using SQLServer version 12.0.5207.0, which has a limit on varchar fields to 8000.

B: We have tried modifying the column length in class ""TmodelInstanceInfo"" and redeploying the app, but it starts giving another issue.

B: The type ""class org.apache.juddi.model.TmodelInstanceInfo"" has not been enhanced.",Major,FALSE,Over-analysis
NullPointerException in CmapSubtable.getCharCode,https://issues.apache.org/jira/browse/PDFBOX-5465,"Hi, 

I got a NPE in the getCharCode method of CmapSubtable : 

java.lang.NullPointerException: null 
at org.apache.fontbox.ttf.CmapSubtable.getCharCode(CmapSubtable.java:669) ~[fontbox-2.0.25.jar!/:2.0.25] 
at org.apache.fontbox.ttf.CmapSubtable.getCharCodes(CmapSubtable.java:686) ~[fontbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.pdmodel.font.PDType0Font.toUnicode(PDType0Font.java:528) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.showGlyph(PDFStreamEngine.java:811) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.showText(PDFStreamEngine.java:749) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.showTextString(PDFStreamEngine.java:608) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.operator.text.ShowText.process(ShowText.java:56) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processOperator(PDFStreamEngine.java:939) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStreamOperators(PDFStreamEngine.java:514) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processStream(PDFStreamEngine.java:492) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.contentstream.PDFStreamEngine.processPage(PDFStreamEngine.java:155) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.LegacyPDFStreamEngine.processPage(LegacyPDFStreamEngine.java:155) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.processPage(PDFTextStripper.java:363) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.processPages(PDFTextStripper.java:291) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.writeText(PDFTextStripper.java:238) ~[pdfbox-2.0.25.jar!/:2.0.25] 
at org.apache.pdfbox.text.PDFTextStripper.getText(PDFTextStripper.java:202) ~[pdfbox-2.0.25.jar!/:2.0.25] 


-> It seems, in some cases the glyphIdToCharacterCode array is not instantiated. 

Sorry, but for privacy reason I can't share the PDF which cause this issue.","B: A NullPointerException occurs in the getCharCode method of CmapSubtable when the glyphIdToCharacterCode array is not instantiated. 

B: Investigate the initialization of the glyphIdToCharacterCode array in CmapSubtable to ensure it is properly instantiated before being accessed in the getCharCode method.

B: Review the flow of data leading to the getCharCode method to identify cases where the glyphIdToCharacterCode array might remain uninitialized.

B: Create error handling in the getCharCode method of CmapSubtable to manage cases where the glyphIdToCharacterCode array is null, preventing a NullPointerException.",Major,FALSE,Over-analysis
TestReloadableDefinitionsFactory fails when the project is in a path with spaces in its name,https://issues.apache.org/jira/browse/TILES-33,"TestReloadableDefinitionsFactory fails if it is run through Maven under Windows 2000 when the project is in a path with spaces in its name 
The same test run from Eclipse 3.2 does not show the problem. 

Here is the report from Maven (asterisks are there to protect privacy) 
<snip> 
------------------------------------------------------- 
T E S T S 
------------------------------------------------------- 
[surefire] Running org.apache.tiles.TestReloadableDefinitionsFactory 
[surefire] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0,031 sec 
[surefire] 
[surefire] testReloadableDefinitionsFactory(org.apache.tiles.TestReloadableDefinitionsFactory) Time elapsed: 0,016 sec <<< FAILURE! 
junit.framework.AssertionFailedError: Error running test: java.net.URISyntaxException: Illegal character in path at index 18: file:/C:/Documents and Settings/***/tiles/tiles-core/target/test-classes/org/apache/tiles/config/temp-defs.xml 
at junit.framework.Assert.fail(Assert.java:47) 
at org.apache.tiles.TestReloadableDefinitionsFactory.testReloadableDefinitionsFactory(TestReloadableDefinitionsFactory.java:148) 
... 
</snip> 

The reason seem to be that sun.misc.Launcher$AppClassLoader.getResource (used in Eclipse) replaces spaces with '%20', while URLClassLoader.getResource (used by Maven) does not replace them.","B: TestReloadableDefinitionsFactory fails if it is run through Maven under Windows 2000 when the project is in a path with spaces in its name.  
B: The same test run from Eclipse 3.2 does not show the problem.  
B: The error reported during the Maven test run is a junit.framework.AssertionFailedError due to java.net.URISyntaxException caused by an illegal character in the path.  
B: The issue arises because sun.misc.Launcher$AppClassLoader.getResource (used in Eclipse) replaces spaces with '%20', while URLClassLoader.getResource (used by Maven) does not replace them.",Minor,FALSE,Over-analysis
LDAP injection vulnerability in LDAPAuthenticationSchemeImpl,https://issues.apache.org/jira/browse/DERBY-7147,"An LDAP injection vulnerability has been identified in LDAPAuthenticationSchemeImpl.getDNFromUID(). An exploit has not been provided, but there is a possibility that an intruder could bypass authentication checks in Derby-powered applications which rely on external LDAP servers. 

For more information on LDAP injection, see https://www.synopsys.com/glossary/what-is-ldap-injection.html","B: An LDAP injection vulnerability has been identified in LDAPAuthenticationSchemeImpl.getDNFromUID().  
B: An exploit has not been provided for the LDAP injection vulnerability.  
B: There is a possibility that an intruder could bypass authentication checks in Derby-powered applications which rely on external LDAP servers.",Major,FALSE,Over-decomposition
Unable to decrypt PDF with String and Stream filter to identity,https://issues.apache.org/jira/browse/PDFBOX-4517,"I receive a PDF that contains the following Encryption Dictionnary: 

32 0 obj 
<</O (___\f__I_&_¨¦ ^¡ã_>5N,\\q¨¨_#O¡ë2__\b_5__j;_P)/EFF/StdCF/P -1852/R 5/OE (Q0_z_¨¨^_____¨¦_nP}___]_.y_¨²_c¨²_^)/U (_T7_Hib__\t|____U¡é__Nb¨ª¨¤>_@_ ___¡êX_¡®¨²-Uz_L<0_)/EncryptMetadata false/V 5/Length 256/CF<</StdCF<</AuthEvent/EFOpen/Length 32/CFM/AESV3>>>>/StmF/Identity/Filter/Standard/StrF/Identity/Perms (_;_¡ª__]m____)/UE (_____k$__f¡®_0£¤_e""__]_9_N_¡®1__)>> 
endobj 
and I was unable to open it with PDF Box. 

Unfortunately, I can't share this PDF with you due to customer privacy and I was unable to find a tool that allow to create such a PDF. 

This kind of encryption is useless I think, but it's probably intersting to support it anyway. Browsers and Adobe Reader have no problem to open it.","B: I receive a PDF that contains an encryption dictionary that cannot be opened with PDF Box. 

B: The encryption method used in the PDF is unsupported by PDF Box, but it is compatible with browsers and Adobe Reader. 

B: I am unable to share the PDF due to customer privacy concerns. 

B: I was unable to find a tool that allows creating a PDF with the specified encryption dictionary. 

B: The encryption used in the PDF is perceived as useless, but it may be worth supporting in PDF Box.",Major,FALSE,Over-decomposition
Error when starting Apache Unomi when offline,https://issues.apache.org/jira/browse/UNOMI-75,"Happened when I started unomi while being offline (wifi stopped) 

2017-01-27 15:16:50,399 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/parameterized-types to bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT 
2017-01-27 15:16:50,400 | ERROR | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Unable to start blueprint container for bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT 
org.xml.sax.SAXParseException: src-resolve: Cannot resolve the name 'ptp:ParameterizedInt' to a 'simpleType definition' component. 
at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)[:] 
at org.apache.xerces.util.ErrorHandlerWrapper.error(Unknown Source)[:] 
at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.reportSchemaError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAbstractTraverser.reportSchemaError(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAttributeTraverser.traverseNamedAttr(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAttributeTraverser.traverseLocal(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDAbstractTraverser.traverseAttrsAndAttrGrps(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.processComplexContent(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.traverseComplexTypeDecl(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDComplexTypeTraverser.traverseLocal(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseNamedElement(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDElementTraverser.traverseGlobal(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.traverseSchemas(Unknown Source)[:] 
at org.apache.xerces.impl.xs.traversers.XSDHandler.parseSchema(Unknown Source)[:] 
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadSchema(Unknown Source)[:] 
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:] 
at org.apache.xerces.impl.xs.XMLSchemaLoader.loadGrammar(Unknown Source)[:] 
at org.apache.xerces.jaxp.validation.XMLSchemaFactory.newSchema(Unknown Source)[:] 
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.createSchema(NamespaceHandlerRegistryImpl.java:637)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.doGetSchema(NamespaceHandlerRegistryImpl.java:458)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.namespace.NamespaceHandlerRegistryImpl$NamespaceHandlerSetImpl.getSchema(NamespaceHandlerRegistryImpl.java:443)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:343)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1] 
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1] 
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:] 
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:] 
at java.lang.Thread.run(Thread.java:745)[:1.8.0_111] 
2017-01-27 15:16:50,439 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/beans to bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT 
2017-01-27 15:16:50,440 | WARN | FelixStartLevel | NamespaceHandlerRegistryImpl | 15 - org.apache.aries.blueprint.core - 1.6.1 | Dynamically adding namespace handler http://cxf.apache.org/configuration/parameterized-types to bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT 
After some more research this is really a problem with Aries Blueprint and Apache CXF, here are the relevant tickets : 

https://issues.apache.org/jira/browse/ARIES-1540 

and 

https://issues.apache.org/jira/browse/CXF-7183 

We could use the first fix for the current release but we will need to somehow upgrade CXF again when we switch to Karaf 4.","B: Happened when I started unomi while being offline (wifi stopped).

B: Unable to start blueprint container for bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT due to a SAXParseException.

B: The SAXParseException indicates that the name 'ptp:ParameterizedInt' cannot be resolved to a 'simpleType definition' component.

B: A warning is generated when dynamically adding the namespace handler for http://cxf.apache.org/configuration/parameterized-types to the bundle org.apache.unomi.cxs-geonames-rest/1.2.0.incubating-SNAPSHOT.

B: A warning is generated when dynamically adding the namespace handler for http://cxf.apache.org/configuration/beans to the bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT.

B: A warning is generated when dynamically adding the namespace handler for http://cxf.apache.org/configuration/parameterized-types to the bundle org.apache.unomi.cxs-privacy-extension-rest/1.2.0.incubating-SNAPSHOT.

B: There are relevant tickets for the problem with Aries Blueprint and Apache CXF, which may require fixes or upgrades.",Major,FALSE,Over-analysis
base class warning in SAXException.hpp copy constructor,https://issues.apache.org/jira/browse/XERCESC-1162,"I have been getting multiple copies of this warning: 

include/xercesc/sax/SAXException.hpp: In copy constructor 
`xercesc_2_5::SAXException::SAXException(const xercesc_2_5::SAXException&)': 
include/xercesc/sax/SAXException.hpp:195: warning: base class `class 
xercesc_2_5::XMemory' should be explicitly initialized in the copy constructor 

I have to fix the code in SAXException.hpp by changing this: 

SAXException(const SAXException& toCopy) : 

fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager)) 
, fMemoryManager(toCopy.fMemoryManager) 
{ 
} 

into this: 

SAXException(const SAXException& toCopy) : XMemory(), 

fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager)) 
, fMemoryManager(toCopy.fMemoryManager) 
{ 
} 

once XMemory() is declared as a base class, all warnings are gone. I've seen 
this in 2.5.0 as well. Below are the compiler flags that I have set which 
should help you recreate this bug: 

-g3 -I. -I./include -isystem ./libs/crystalize/Linux/include -I. - 
I./include -isystem ./libs/crystalize/Linux/include -D_linux_ -D_x86_ - 
DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT -I./libs/Database - 
I./libs/Database/libs/xerces/Linux/include isystem ./libs/sybase/sybase 
12.5.1/Linux/include I./libs/xerces/Linux/include -Wall -W -pedantic -Wno 
long-long Wpointer-arith -Wcast-qual -Wwrite-strings -Wconversion -Wstrict 
prototypes -Wmissing-prototypes -Woverloaded-virtual -Wchar-subscripts - 
Wparentheses Wpointer-arith -Winline -Wnon-virtual-dtor -Wreorder -fno-gnu 
keywords -Wctor-dtor-privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar - 
Wno-float-equal Wdisabled-optimization -Wswitch -Wpointer-arith -pipe -Wno 
cast-qual Wno-unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage 
length=0 -DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64 - 
D_linux_ -D_x86_ -DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT - 
I./libs/utilities/include -I./libs/AST_Common -I./libs/AST_Common/AST/enums - 
I./libs/Database -I./libs/Database/libs/boost/Linux - 
I./libs/Database/libs/omni/Linux/include I./libs/Database/libs/sybase/sybase 
12.5.1/Linux/include -I./libs/Database/libs/xerces/Linux/include - 
isystem ./libs/sybase/sybase-12.5.1/Linux/include - 
I./libs/xerces/Linux/include -Wall -W -pedantic -Wno-long-long -Wpointer-arith - 
Wcast-qual Wwrite-strings -Wconversion -Wstrict-prototypes -Wmissing 
prototypes Woverloaded-virtual -Wchar-subscripts -Wparentheses -Wpointer 
arith Winline -Wnon-virtual-dtor -Wreorder -fno-gnu-keywords -Wctor-dtor 
privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar -Wno-float-equal - 
Wdisabled-optimization Wswitch -Wpointer-arith -pipe -Wno-cast-qual -Wno 
unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage-length=0 - 
DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64 

Let me know if you need more information. Thank you. 

-Vrajesh","B: In the copy constructor `xercesc_2_5::SAXException::SAXException(const xercesc_2_5::SAXException&)`, the base class `class xercesc_2_5::XMemory` should be explicitly initialized.

B: Change the copy constructor implementation of `SAXException` from `SAXException(const SAXException& toCopy) : fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager), fMemoryManager(toCopy.fMemoryManager) { }` to `SAXException(const SAXException& toCopy) : XMemory(), fMsg(XMLString::replicate(toCopy.fMsg, toCopy.fMemoryManager), fMemoryManager(toCopy.fMemoryManager) { }`.

B: Once `XMemory()` is declared as a base class in the copy constructor, all warnings should be resolved.

B: The compiler flags set are `-g3 -I. -I./include -isystem ./libs/crystalize/Linux/include -I. -I./include -isystem ./libs/crystalize/Linux/include -D_linux_ -D_x86_ -DOMNI -DOMNI_VERSION=""4"" -I./idl -D_REENTRANT -I./libs/Database -I./libs/Database/libs/xerces/Linux/include -isystem ./libs/sybase/sybase 12.5.1/Linux/include -I./libs/xerces/Linux/include -Wall -W -pedantic -Wno-long-long -Wpointer-arith -Wcast-qual -Wwrite-strings -Wconversion -Wstrict-prototypes -Wmissing-prototypes -Woverloaded-virtual -Wchar-subscripts -Wparentheses -Wpointer-arith -Winline -Wnon-virtual-dtor -Wreorder -fno-gnu-keywords -Wctor-dtor-privacy -Wno-format-y2k -Wdeprecated -Wformat -Wmultichar -Wno-float-equal -Wdisabled-optimization -Wswitch -Wpointer-arith -pipe -Wno-cast-qual -Wno-unused-parameter -Wno-overloaded-virtual -fsigned-char -fmessage-length=0 -DSIZE_MAX=4294967295U -DCORBA_ENUMS -D_FILE_OFFSET_BITS=64`.",Major,FALSE,Over-analysis
svnmerge.py migration tool(s) do not guarantee proper svn:mergeinfo range ordering,https://issues.apache.org/jira/browse/SVN-3302,"Per http://svn.haxx.se/dev/archive-2008-10/0280.shtml, 
svnmerge-migrate-history.py (and probably svnmerge-migrate-history-remotely.py, 
too) do not ensure that the svn:mergeinfo properties they create have their 
rangelists sorted properly, which can result in the versioning of bogus property 
value that Subversion can't use.","B: svnmerge-migrate-history.py does not ensure that the svn:mergeinfo properties created have their rangelists sorted properly.

B: svnmerge-migrate-history-remotely.py does not ensure that the svn:mergeinfo properties created have their rangelists sorted properly.

B: The improper sorting of rangelists can result in the versioning of bogus property values that Subversion can't use.",Critical,FALSE,Over-decomposition
byte[].encodeBase64() incorrectly introduces line breaks,https://issues.apache.org/jira/browse/GROOVY-2878,"Groovy's encodeBase64 inserts 0x0A chars (LF) into long strings to break lines. This is contrary to my reading of RFC4648: 

{codec} 
3.1. Line Feeds in Encoded Data 

MIME [4] is often used as a reference for base 64 encoding. However, 
MIME does not define ""base 64"" per se, but rather a ""base 64 Content- 
Transfer-Encoding"" for use within MIME. As such, MIME enforces a 
limit on line length of base 64-encoded data to 76 characters. MIME 
inherits the encoding from Privacy Enhanced Mail (PEM) [3], stating 
that it is ""virtually identical""; however, PEM uses a line length of 
64 characters. The MIME and PEM limits are both due to limits within 
SMTP. 

Implementations MUST NOT add line feeds to base-encoded data unless 
the specification referring to this document explicitly directs base 
encoders to add line feeds after a specific number of characters.{codec} 
This has resulted in incorrect behaviour in Grails also. However the author notes that some groovy applications may rely on this functionality currently, so this could be a breaking change for some. 

It would be better to be correct IMO. There is no clue in groovy docs that this introduces line breaks in this MIME/SMTP specific way.","B: Groovy's encodeBase64 inserts 0x0A chars (LF) into long strings to break lines contrary to RFC4648.  

B: The behavior of Groovy's encodeBase64 results in incorrect behavior in Grails.  

B: Some Groovy applications may rely on the current behavior of encodeBase64, which could cause a breaking change if modified.  

B: The Groovy documentation does not indicate that line breaks are introduced in a MIME/SMTP specific way.",Major,FALSE,Over-decomposition
Cleanup suspect coding practices in the org.apache.derby.impl.tools.ij package.,https://issues.apache.org/jira/browse/DERBY-6195,Similar to DERBY-6177.,B: Similar to DERBY-6177.,Minor,TRUE,
ArrayIndexOutOfBoundsException: 9 parsing RTF,https://issues.apache.org/jira/browse/TIKA-1192,"When trying to parse an RTF file I'm getting the following exception. I am not able to attach the file for privacy reasons: 

java.lang.ArrayIndexOutOfBoundsException: 9 
TextExtractor.java:872 org.apache.tika.parser.rtf.TextExtractor.processControlWord 
TextExtractor.java:566 org.apache.tika.parser.rtf.TextExtractor.parseControlWord 
TextExtractor.java:492 org.apache.tika.parser.rtf.TextExtractor.parseControlToken 
TextExtractor.java:459 org.apache.tika.parser.rtf.TextExtractor.extract 
TextExtractor.java:448 org.apache.tika.parser.rtf.TextExtractor.extract 
RTFParser.java:56 org.apache.tika.parser.rtf.RTFParser.parse 
(Unknown Source) sun.reflect.NativeMethodAccessorImpl.invoke0 
NativeMethodAccessorImpl.java:57 sun.reflect.NativeMethodAccessorImpl.invoke 
DelegatingMethodAccessorImpl.java:43 sun.reflect.DelegatingMethodAccessorImpl.invoke 
Method.java:606 java.lang.reflect.Method.invoke 
Reflector.java:93 clojure.lang.Reflector.invokeMatchingMethod 
Reflector.java:28 clojure.lang.Reflector.invokeInstanceMethod 
tika_parser.clj:20 rtf-parser.tika-parser/parse 
form-init2921349737948661927.clj:1 rtf-parser.tika-parser/eval4200 
Compiler.java:6619 clojure.lang.Compiler.eval 
Compiler.java:6582 clojure.lang.Compiler.eval 
core.clj:2852 clojure.core/eval 
main.clj:259 clojure.main/repl[fn] 
main.clj:259 clojure.main/repl[fn] 
main.clj:277 clojure.main/repl[fn] 
main.clj:277 clojure.main/repl 
RestFn.java:1096 clojure.lang.RestFn.invoke 
interruptible_eval.clj:56 clojure.tools.nrepl.middleware.interruptible-eval/evaluate[fn] 
AFn.java:159 clojure.lang.AFn.applyToHelper 
AFn.java:151 clojure.lang.AFn.applyTo 
core.clj:617 clojure.core/apply 
core.clj:1788 clojure.core/with-bindings* 
RestFn.java:425 clojure.lang.RestFn.invoke 
interruptible_eval.clj:41 clojure.tools.nrepl.middleware.interruptible-eval/evaluate 
interruptible_eval.clj:171 clojure.tools.nrepl.middleware.interruptible-eval/interruptible-eval[fn] 
core.clj:2330 clojure.core/comp[fn] 
interruptible_eval.clj:138 clojure.tools.nrepl.middleware.interruptible-eval/run-next[fn] 
AFn.java:24 clojure.lang.AFn.run 
ThreadPoolExecutor.java:1145 java.util.concurrent.ThreadPoolExecutor.runWorker 
ThreadPoolExecutor.java:615 java.util.concurrent.ThreadPoolExecutor$Worker.run 
Thread.java:724 java.lang.Thread.run","B: When trying to parse an RTF file, an ArrayIndexOutOfBoundsException is thrown at line 872 in TextExtractor.java.

B: The method processControlWord in TextExtractor.java fails to handle a specific control word when parsing RTF files.

B: The method parseControlWord in TextExtractor.java does not correctly process certain control words leading to an exception.

B: The method parseControlToken in TextExtractor.java encounters unexpected input that causes an ArrayIndexOutOfBoundsException.

B: The extract method in TextExtractor.java needs to handle edge cases during extraction from RTF files to prevent exceptions.

B: The RTFParser class does not properly manage exceptions thrown during the parsing process, which results in an uncaught ArrayIndexOutOfBoundsException.",Major,FALSE,Over-analysis
strange Digester parsing error,https://issues.apache.org/jira/browse/DIGESTER-27,"While testing our application we ran into a strange Digester parse issue. 
It looks like the Digester sometimes forgets to parse a value in the xml. Here 
the situation: 

1. If we fire testscript A that doesnot comply to the schema we set on the 
Digester then we get a parsing error as expected. The error is that field Z in 
the xml was not valid. 
2. We fire testscript B which should return an answer. The first time we fire 
it the Digester doesnot map field Z (which now has a valid value) to the java 
class as defined in the rule file. 
3. We fire testscript B again unchanged and now field Z is mapped by the 
Digester to the correct attribute in the corresponding java class. 

If at point 1 we dont fire testscript A (with the invalid value for attribute Z) 
but say C or any other this doesnot occur and we get the reply we expect...... 

It seems like that after a call which results in a SAXException due to an 
invalid value in the XML according to the attached schema the next call fails 
to parse the xml correctly to the java object defined in the rule file. The 
third call however (which is exactly the same as the second) succeeds. 

Any idea's? 

Regards, 
Lars Vonk","B: The Digester sometimes fails to parse a valid value for field Z in the XML when testscript B is executed after a previous testscript A that resulted in a parsing error.

B: Executing testscript A with an invalid value for field Z causes the Digester to throw a SAXException.

B: After executing testscript A, executing testscript B the first time results in field Z not being mapped to the corresponding Java class.

B: Executing testscript B a second time, unchanged, results in field Z being correctly mapped to the corresponding Java class.

B: If testscript A is not executed before testscript B, the Digester parses the XML correctly and provides the expected output.",Major,TRUE,
Wrong XAException return code when broker timeout is hit,https://issues.apache.org/jira/browse/ARTEMIS-591,"By creating testcases for checking behavior of transaction timeout I've hit an issue of wrong error code being returned when broker transaction timeout is hit before TM transaction timeout expires. 
It uses XAER_PROTO instead of RBTIMEOUT. 

This issue does not cause data inconsistency. 

Scenario: 

ejb sends a message to a queue 
processing inside of the ejb takes long time 
TM transaction timeout is set big enough to not hit the timeout 
jms broker internal transaction timeout is smaller than time needed for processing ejb method 
jms broker txn timeout occurs - broker local txn is rolled back 
txn is removed from list of broker's local in-process transactions 
TM calls XAResource.end 
the call returns XAException.XAER_PROTO 
That's current implementation returns XAER_PROTO in this scenario but RBTIMEOUT would be more appropriate. 

From discussion with Narayana developers, RM should return the most specific error return code as possible. In this scenario it's RBTIMEOUT. 

Other notes from TM dev point of view: 

""[XA_RBTIMEOUT] 
The work represented by this transaction branch took too long."" 
per XA spec page 39. 

The more complex question is, at what point can the resource manager forget about that branch (and therefore return NOTA to subsequent calls)? 

The XA spec says ""After the transaction manager calls xa_end(), it should no longer consider the calling thread associated with that resource manager (although it must consider the resource manager part of the transaction branch when it prepares the branch.)"" 
which implies the branch is still considered live at that point, a view corroborated by: 

""[XA_RB_] 
The resource manager has dissociated the transaction branch from the thread of control and has marked rollback-only the work performed on behalf of _xid."" 
Exception being thrown 

WARN [com.arjuna.ats.jta] (Thread-0 
(ActiveMQ-client-global-threads-1468293951)) ARJUNA016056: 
TransactionImple.delistResource - caught exception during delist : 
XAException.XAER_PROTO: javax.transaction.xa.XAException 
at 
org.apache.activemq.artemis.core.protocol.core.impl.ActiveMQSessionContext.xaEnd(ActiveMQSessionContext.java:346) 
at 
org.apache.activemq.artemis.core.client.impl.ClientSessionImpl.end(ClientSessionImpl.java:1115) 
at 
org.apache.activemq.artemis.ra.ActiveMQRAXAResource.end(ActiveMQRAXAResource.java:112) 
at 
org.apache.activemq.artemis.service.extensions.xa.ActiveMQXAResourceWrapperImpl.end(ActiveMQXAResourceWrapperImpl.java:81) 
at 
com.arjuna.ats.internal.jta.transaction.arjunacore.TransactionImple.delistResource(TransactionImple.java:897) 
at 
org.jboss.jca.core.connectionmanager.listener.TxConnectionListener$TransactionSynchronization.beforeCompletion(TxConnectionListener.java:1063) 
at 
org.jboss.jca.core.connectionmanager.transaction.TransactionSynchronizer.invokeBefore(TransactionSynchronizer.java:438) 
at 
org.jboss.jca.core.connectionmanager.transaction.TransactionSynchronizer.beforeCompletion(TransactionSynchronizer.java:376) 
at 
org.jboss.as.txn.service.internal.tsr.JCAOrderedLastSynchronizationList.beforeCompletion(JCAOrderedLastSynchronizationList.java:130) 
at 
com.arjuna.ats.internal.jta.resources.arjunacore.SynchronizationImple.beforeCompletion(SynchronizationImple.java:76) 
at 
com.arjuna.ats.arjuna.coordinator.TwoPhaseCoordinator.beforeCompletion(TwoPhaseCoordinator.java:371) 
at 
com.arjuna.ats.arjuna.coordinator.TwoPhaseCoordinator.end(TwoPhaseCoordinator.java:91) 
at com.arjuna.ats.arjuna.AtomicAction.commit(AtomicAction.java:162) 
at 
com.arjuna.ats.internal.jta.transaction.arjunacore.TransactionImple.commitAndDisassociate(TransactionImple.java:1200) 
at 
com.arjuna.ats.internal.jta.transaction.arjunacore.BaseTransaction.commit(BaseTransaction.java:126) 
at 
com.arjuna.ats.jbossatx.BaseTransactionManagerDelegate.commit(BaseTransactionManagerDelegate.java:89) 
at 
org.jboss.as.ejb3.inflow.MessageEndpointInvocationHandler.afterDelivery(MessageEndpointInvocationHandler.java:71) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:498) 
at 
org.jboss.as.ejb3.inflow.AbstractInvocationHandler.handle(AbstractInvocationHandler.java:60) 
at 
org.jboss.as.ejb3.inflow.MessageEndpointInvocationHandler.doInvoke(MessageEndpointInvocationHandler.java:135) 
at 
org.jboss.as.ejb3.inflow.AbstractInvocationHandler.invoke(AbstractInvocationHandler.java:73) 
at 
org.jboss.as.test.jbossts.crashrec.jms.mdb.JMSCrashMessageDrivenBean$$$endpoint1.afterDelivery(Unknown 
Source) 
at 
org.apache.activemq.artemis.ra.inflow.ActiveMQMessageHandler.onMessage(ActiveMQMessageHandler.java:321) 
at 
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl.callOnMessage(ClientConsumerImpl.java:932) 
at 
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl.access$400(ClientConsumerImpl.java:47) 
at 
org.apache.activemq.artemis.core.client.impl.ClientConsumerImpl$Runner.run(ClientConsumerImpl.java:1045) 
at 
org.apache.activemq.artemis.utils.OrderedExecutorFactory$OrderedExecutor$ExecutorTask.run(OrderedExecutorFactory.java:100) 
at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
at java.lang.Thread.run(Thread.java:745)","B: The broker returns the wrong error code XAER_PROTO instead of RBTIMEOUT when a broker transaction timeout occurs before the TM transaction timeout expires.

B: The jms broker internal transaction timeout is smaller than the time needed for processing the ejb method.

B: The TM transaction timeout is set to a value that is larger than the time taken for the processing of the ejb method.

B: The current implementation incorrectly returns XAER_PROTO instead of the more appropriate error code RBTIMEOUT in the scenario described.

B: Investigate the implications of the XA spec regarding when the resource manager can forget about a transaction branch after the TM calls xa_end.

B: The exception WARN [com.arjuna.ats.jta] indicates that an XAException.XAER_PROTO is caught during the delistResource operation. 

B: Review the stack trace provided to identify the source of the exception during the transaction management process.",Major,FALSE,Over-decomposition
CouchDB fails to bind to IPv6 address on Windows,https://issues.apache.org/jira/browse/COUCHDB-1032,"I'm trying to bind to IPv6 address :: but couchDB didn't start. 

I've tried with 
bind_address = :: 
and with the specific IPv6 address. 

LOG level setted do debug ########################### 
Erlang R14A (erts-5.8) [source] [smp:2:2] [rq:2] [async-threads:0] 

Eshell V5.8 (abort with ^G) 
1> Apache CouchDB 1.0.1 (LogLevel=debug) is starting. 
Configuration Settings [""../etc/couchdb/default.ini"", 
""../etc/couchdb/local.ini""]: 
[admins] **LINE REMOVED** 
[admins] **LINE REMOVED** 
[attachments] compressible_types=""text/*, application/javascript, application/json, application/xml"" 
[attachments] compression_level=""8"" 
[couch_httpd_auth] auth_cache_size=""50"" 
[couch_httpd_auth] authentication_db=""_users"" 
[couch_httpd_auth] authentication_redirect=""/_utils/session.html"" 
[couch_httpd_auth] require_valid_user=""false"" 
[couch_httpd_auth] **LINE REMOVED** 
[couch_httpd_auth] timeout=""600"" 
[couchdb] database_dir=""../var/lib/couchdb"" 
[couchdb] delayed_commits=""true"" 
[couchdb] max_attachment_chunk_size=""4294967296"" 
[couchdb] max_dbs_open=""100"" 
[couchdb] max_document_size=""4294967296"" 
[couchdb] os_process_timeout=""5000"" 
[couchdb] uri_file=""../var/lib/couchdb/couch.uri"" 
[couchdb] util_driver_dir=""../lib/couch-1.0.1/priv/lib"" 
[couchdb] view_index_dir=""../var/lib/couchdb"" 
[daemons] auth_cache="" 

{couch_auth_cache, start_link, []} 
"" 
[daemons] db_update_notifier="" 

{couch_db_update_notifier_sup, start_link, []} 
"" 
[daemons] external_manager="" 

{couch_external_manager, start_link, []} 
"" 
[daemons] httpd="" 

{couch_httpd, start_link, []} 
"" 
[daemons] query_servers="" 

{couch_query_servers, start_link, []} 
"" 
[daemons] stats_aggregator="" 

{couch_stats_aggregator, start, []} 
"" 
[daemons] stats_collector="" 

{couch_stats_collector, start, []} 
"" 
[daemons] uuids="" 

{couch_uuids, start, []} 
"" 
[daemons] view_manager="" 

{couch_view, start_link, []} 
"" 
[httpd] allow_jsonp=""false"" 
[httpd] authentication_handlers="" 

{couch_httpd_oauth, oauth_authentication_handler} 
, 

{couch_httpd_auth, cookie_authentication_handler} 
, 

{couch_httpd_auth, default_authentication_handler} 
"" 
[httpd] bind_address=""::"" 
[httpd] default_handler="" 

{couch_httpd_db, handle_request} 
"" 
[httpd] max_connections=""2048"" 
[httpd] port=""5984"" 
[httpd] secure_rewrites=""true"" 
[httpd] vhost_global_handlers=""_utils, _uuids, _session, _oauth, _users"" 
[httpd_db_handlers] _changes="" 

{couch_httpd_db, handle_changes_req} 
"" 
[httpd_db_handlers] _compact="" 

{couch_httpd_db, handle_compact_req} 
"" 
[httpd_db_handlers] _design="" 

{couch_httpd_db, handle_design_req} 
"" 
[httpd_db_handlers] _temp_view="" 

{couch_httpd_view, handle_temp_view_req} 
"" 
[httpd_db_handlers] _view_cleanup="" 

{couch_httpd_db, handle_view_cleanup_req} 
"" 
[httpd_design_handlers] _info="" 

{couch_httpd_db, handle_design_info_req} 
"" 
[httpd_design_handlers] _list="" 

{couch_httpd_show, handle_view_list_req} 
"" 
[httpd_design_handlers] _rewrite="" 

{couch_httpd_rewrite, handle_rewrite_req} 
"" 
[httpd_design_handlers] _show="" 

{couch_httpd_show, handle_doc_show_req} 
"" 
[httpd_design_handlers] _update="" 

{couch_httpd_show, handle_doc_update_req} 
"" 
[httpd_design_handlers] _view="" 

{couch_httpd_view, handle_view_req} 
"" 
[httpd_global_handlers] /="" 

{couch_httpd_misc_handlers, handle_welcome_req, <<\""Welcome\"">>} 
"" 
[httpd_global_handlers] _active_tasks="" 

{couch_httpd_misc_handlers, handle_task_status_req} 
"" 
[httpd_global_handlers] _all_dbs="" 

{couch_httpd_misc_handlers, handle_all_dbs_req} 
"" 
[httpd_global_handlers] _config="" 

{couch_httpd_misc_handlers, handle_config_req} 
"" 
[httpd_global_handlers] _log="" 

{couch_httpd_misc_handlers, handle_log_req} 
"" 
[httpd_global_handlers] _oauth="" 

{couch_httpd_oauth, handle_oauth_req} 
"" 
[httpd_global_handlers] _replicate="" 

{couch_httpd_misc_handlers, handle_replicate_req} 
"" 
[httpd_global_handlers] _restart="" 

{couch_httpd_misc_handlers, handle_restart_req} 
"" 
[httpd_global_handlers] _session="" 

{couch_httpd_auth, handle_session_req} 
"" 
[httpd_global_handlers] _stats="" 

{couch_httpd_stats_handlers, handle_stats_req} 
"" 
[httpd_global_handlers] _utils="" 

{couch_httpd_misc_handlers, handle_utils_dir_req, \""../share/couchdb/www\""} 
"" 
[httpd_global_handlers] _uuids="" 

{couch_httpd_misc_handlers, handle_uuids_req} 
"" 
[httpd_global_handlers] favicon.ico="" 

{couch_httpd_misc_handlers, handle_favicon_req, \""../share/couchdb/www\""} 
"" 
[log] file=""../var/log/couchdb/couch.log"" 
[log] include_sasl=""true"" 
[log] level=""debug"" 
[query_server_config] reduce_limit=""true"" 
[query_servers] javascript=""./couchjs.exe ../share/couchdb/server/main.js"" 
[replicator] max_http_pipeline_size=""10"" 
[replicator] max_http_sessions=""10"" 
[stats] rate=""1000"" 
[stats] samples=""[0, 60, 300, 900]"" 
[uuids] algorithm=""sequential"" 
Failure to start Mochiweb: eafnosupport 
[error] [<0.106.0>] {error_report,<0.34.0>, 
{<0.106.0>,crash_report, 
[[{initial_call,{mochiweb_socket_server,init,['Argument__1']}}, 
{pid,<0.106.0>} 
, 
{registered_name,[]} 
, 
{error_info,{exit,eafnosupport, 
[ 

{gen_server,init_it,6} 
, 
{proc_lib,init_p_do_apply,3} 
]}}, 
{ancestors,[couch_secondary_services,couch_server_sup, <0.35.0>]} 
, 
{messages,[]} 
, 
{links,[<0.89.0>]} 
, 
{dictionary,[]} 
, 
{trap_exit,true} 
, 
{status,running} 
, 
{heap_size,1597} 
, 
{stack_size,24} 
, 
{reductions,352} 
], 
[]]}} 

=CRASH REPORT==== 20-Jan-2011::09:39:55 === 
crasher: 
initial call: mochiweb_socket_server:init/1 
pid: <0.106.0> 
registered_name: [] 
exception exit: eafnosupport 
in function gen_server:init_it/6 
ancestors: [couch_secondary_services,couch_server_sup,<0.35.0>] 
messages: [] 
links: [<0.89.0>] 
dictionary: [] 
trap_exit: true 
status: running 
heap_size: 1597 
stack_size: 24 
reductions: 352 
neighbours: 
[error] [<0.89.0>] {error_report,<0.34.0>, 
{<0.89.0>,supervisor_report, 
[{supervisor,{local,couch_secondary_services}}, 
{errorContext,start_error}, 
{reason,eafnosupport}, 
{offender,[{pid,undefined}, 
{name,httpd}, 
{mfargs,{couch_httpd,start_link,[]}}, 
{restart_type,permanent}, 
{shutdown,1000}, 
{child_type,worker}]}]}} 

=SUPERVISOR REPORT==== 20-Jan-2011::09:39:55 === 
Supervisor: {local,couch_secondary_services} 
Context: start_error 
Reason: eafnosupport 
Offender: [{pid,undefined}, 
{name,httpd}, 
{mfargs,{couch_httpd,start_link,[]}}, 
{restart_type,permanent}, 
{shutdown,1000}, 
{child_type,worker}] 

[error] [<0.81.0>] {error_report,<0.34.0>, 
{<0.81.0>,supervisor_report, 
[{supervisor,{local,couch_server_sup}}, 
{errorContext,start_error} 
, 
{reason,shutdown} 
, 
{offender, 
[ 

{pid,undefined}, 
{name,couch_secondary_services}, 
{mfargs,{couch_server_sup,start_secondary_services,[]}}, 
{restart_type,permanent}, 
{shutdown,infinity}, 
{child_type,supervisor}]}]}} 

=SUPERVISOR REPORT==== 20-Jan-2011::09:39:55 === 
Supervisor: {local,couch_server_sup} 
Context: start_error 
Reason: shutdown 
Offender: [{pid,undefined} 
, 
{name,couch_secondary_services} 
, 
{mfargs,{couch_server_sup,start_secondary_services,[]}}, 
{restart_type,permanent} 
, 
{shutdown,infinity} 
, 
{child_type,supervisor} 
] 

=CRASH REPORT==== 20-Jan-2011::09:39:55 === 
crasher: 
initial call: application_master:init/4 
pid: <0.34.0> 
registered_name: [] 
exception exit: {bad_return, 
{{couch_app,start, 
[normal, 
[""../etc/couchdb/default.ini"", 
""../etc/couchdb/local.ini""]]}, 
{'EXIT', 
badmatch,{error,shutdown, 
[ 

{couch_server_sup,start_server,1}, 
{application_master,start_it_old,4}]}}}} 
in function application_master:init/4 
ancestors: [<0.33.0>] 
messages: [{'EXIT',<0.35.0>,normal}] 
links: [<0.33.0>,<0.6.0>] 
dictionary: [] 
trap_exit: true 
status: running 
heap_size: 610 
stack_size: 24 
reductions: 422 
neighbours: 

=INFO REPORT==== 20-Jan-2011::09:39:55 === 
application: couch 
exited: {bad_return,{{couch_app,start, 
[normal, 
[""../etc/couchdb/default.ini"", 
""../etc/couchdb/local.ini""]]}, 
{'EXIT',badmatch,{error,shutdown, 
[{couch_server_sup,start_server,1} 
, 
{application_master,start_it_old,4} 
]}}}} 
type: temporary 
1>","B: I'm trying to bind to IPv6 address :: but CouchDB didn't start.

B: I've tried with bind_address = :: and with the specific IPv6 address.

B: Failure to start Mochiweb: eafnosupport.

B: The error report indicates a crash due to the initial call: mochiweb_socket_server:init/1 with the exception exit: eafnosupport.

B: The supervisor report shows a start_error due to reason: eafnosupport for the httpd service.

B: The application crash report indicates a bad_return during the startup of CouchDB related to the couch_server_sup.",Critical,FALSE,Over-analysis
Access Denied Exceptions fill up the logs with tracebacks that give no additional information,https://issues.apache.org/jira/browse/SLING-1727,"All errors in AbstractSlingPostOperation are logged with full tracebacks, however AccessDeniedExceptions all happen on the Save operation and so the traceback just fills the log up without providing any extra information 

The traceback should happen at debug level, with a one line message at info level. 

currently the traceback is 

03.09.2010 11:02:27.298 ERROR [0:0:0:0:0:0:0:1%0 [1283508147295] POST /test/authztest/node1283508146/childnode.html HTTP/1.1] org.apache.sling.servlets.post.impl.operations.ModifyOperation Exception during response processing. javax.jcr.AccessDeniedException: /test/authztest/node1283508146/childnode/user2-1283508146: not allowed to add or modify item 
at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:411) 
at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1097) 
at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920) 
at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109) 
at $Proxy11.save(Unknown Source) 
at org.apache.sling.servlets.post.AbstractSlingPostOperation.run(AbstractSlingPostOperation.java:125) 
at org.apache.sling.servlets.post.impl.SlingPostServlet.doPost(SlingPostServlet.java:242) 
at org.apache.sling.api.servlets.SlingAllMethodsServlet.mayService(SlingAllMethodsServlet.java:148) 
at org.apache.sling.api.servlets.SlingSafeMethodsServlet.service(SlingSafeMethodsServlet.java:344) 
at org.apache.sling.api.servlets.SlingSafeMethodsServlet.service(SlingSafeMethodsServlet.java:375) 
at org.apache.sling.engine.impl.request.RequestData.service(RequestData.java:523) 
at org.apache.sling.engine.impl.SlingMainServlet.processRequest(SlingMainServlet.java:427) 
at org.apache.sling.engine.impl.filter.RequestSlingFilterChain.render(RequestSlingFilterChain.java:48) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:64) 
at org.apache.sling.engine.impl.debug.RequestProgressTrackerLogFilter.doFilter(RequestProgressTrackerLogFilter.java:59) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.batch.RequestEventsFilter.doFilter(RequestEventsFilter.java:96) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.files.pool.ContentPoolFilter.doFilter(ContentPoolFilter.java:78) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.privacy.RestPrivacyFilter.doFilter(RestPrivacyFilter.java:81) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.persistence.TransactionManagerFilter.doFilter(TransactionManagerFilter.java:95) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.sakaiproject.nakamura.cluster.ClusterTrackingFilter.doFilter(ClusterTrackingFilter.java:87) 
at org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60) 
at org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:313) 
at org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:207) 
at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502) 
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389) 
at org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:64) 
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181) 
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765) 
at org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:111) 
at org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:64) 
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) 
at org.mortbay.jetty.Server.handle(Server.java:324) 
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535) 
at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:880) 
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747) 
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218) 
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) 
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409) 
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)","B: All errors in AbstractSlingPostOperation should be logged with full tracebacks at debug level instead of error level. 

B: AccessDeniedExceptions occurring during the Save operation should log a one line message at info level instead of full tracebacks.

B: ModifyOperation should be adjusted to ensure that AccessDeniedExceptions do not fill the log with unnecessary traceback information.",Major,FALSE,Over-decomposition
Unable to create profile with consent info,https://issues.apache.org/jira/browse/UNOMI-238,"Hi, 
I'm not able to create a profile including its consents properties. 
If I try to add the ""consents"" property to the profile, Unomi returns a 500 status code. Looking at the docs, I've seen that the only way to add consents is by posting an event on a given session. 

I currently have millions of profiles to be imported and adding their consent during profile creation would be awesome.","B: I'm not able to create a profile including its consents properties.  

B: If I try to add the ""consents"" property to the profile, Unomi returns a 500 status code.  

B: The only way to add consents is by posting an event on a given session.  

B: I currently have millions of profiles to be imported.  

B: Adding consent during profile creation would be awesome.",Critical,FALSE,Over-decomposition
Build failure on FreeBSD with CMake,https://issues.apache.org/jira/browse/XERCESC-2109,"{{{ 
19:20:26 cd /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build/src && /usr/bin/CC -DHAVE_CONFIG_H=1 -DXERCES_BUILDING_LIBRARY=1 -D_FILE_OFFSET_BITS=64 -D_THREAD_SAFE=1 -Dxerces_c_EXPORTS -I/usr/local/include -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src -I/opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-build/src -isystem /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/stage/include -Wall -Wcast-align -Wcast-qual -Wctor-dtor-privacy -Wextra -Wformat=2 -Wimplicit-atomic-properties -Wmissing-declarations -Wno-long-long -Woverlength-strings -Woverloaded-virtual -Wredundant-decls -Wreorder -Wswitch-default -Wunused-variable -Wwrite-strings -Wno-variadic-macros -fstrict-aliasing -msse2 -O3 -DNDEBUG -fPIC -pthread -std=gnu++14 -o CMakeFiles/xerces-c.dir/xercesc/util/Base64.cpp.o -c /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp 
19:20:26 /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp:149:14: error: use of undeclared identifier 'XERCES_SIZE_MAX' 
19:20:26 else if (XERCES_SIZE_MAX - inputLength < 2) 

{ 19:20:26 ^ 19:20:26 1 error generated. }} 
}","B: The identifier 'XERCES_SIZE_MAX' is undeclared in the file /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp at line 149.

B: The expression 'XERCES_SIZE_MAX - inputLength < 2' in the file /opt/hudson/workspace/OME-FILES-CPP-DEV-merge-superbuild/BUILD_TYPE/Release/node/brill/build/xerces-source/src/xercesc/util/Base64.cpp causes a compiler error.",Major,FALSE,Over-analysis
Directory Studio doesn't use the SASL confidentiality layer after negotiating its use,https://issues.apache.org/jira/browse/DIRSTUDIO-1220,"There is an issue connecting to an OpenLDAP server configured with olcSaslSecProps: noplain,noanonymous,minssf=1 

i.e. The server requires some form of transport encryption. Having a different issue with StartTLS (DIRSTUDIO-1219), I tried relying on the SASL confidentiality layer that SASL's GSSAPI mechanism can provide, to meet the requirement for encryption. I have chosen ""No encryption"" i.e. no SSL or StartTLS, in the Network Parameters, and then GSSAPI authentication method and Quality of Protection: Authentication with integrity and privacy protection in the SASL settings. 

When connecting to the server, what I can see happening when looking at the network traffic with Wireshark is: 

Client obtains a Kerberos service ticket for the LDAP server and passes it in the bind request for SASL GSSAPI authentication 
Server replies with a bind response, continuing SASL GSSAPI authentication, result code 14 (SASL bind in progress), with a 4 byte message wrapped using GSS_Wrap. The 4 bytes are 0x06 0x01 0x00 0x00 - referring to RFC4752, the first byte indicates the server supports ""Integrity protection"" and/or ""Confidentiality protection"" but not ""No security layer"", as expected. 
Client replies with a bind request, continuing SASL GSSAPI authentication, with a 4 byte message wrapped using GSS_Wrap. The 4 bytes are 0x04 0x01 0x00 0x00 - again referring to RFC4752, the first byte indicates the client has selected ""Confidentiality protection"". 
Server replies with a bind response with result code 0 (success). 
Client sends a search request with base DN: """", scope: base, filter: (objectClass=), for attributes: subschemaSubentry, **with no confidentiality protection*. This is the point where the client violates the protocol described in RFC4752 - after negotiating confidentiality protection, the client needs to actually use it! 
Server interprets the lack of confidentiality protection as an error and immediately drops the connection (this makes sense from the server's POV as it could indicate an attempted man-in-the-middle attack) 
Client immediately re-connects to the server, *doesn't bother to bind at all* and then issues more search requests on the base object, cn=Subschema, etc. 
An error message appears in Directory Studio ""Error while opening connection 
- Missing schema location in RootDSE, using default schema"" - this is presumably because the connection isn't bound, and the server limits what it will disclose to un-bound clients. 

Directory Studio can't browse the directory at all because it's not properly bound. 
As you can see, there's possibly two issues here - definitely an issue with the SASL GSSAPI mechanism, and possibly also an issue with the reconnect logic.","B: There is an issue with the SASL GSSAPI mechanism when connecting to an OpenLDAP server, where the client fails to use confidentiality protection after it has been negotiated.

B: The server drops the connection when the client sends a search request without confidentiality protection after negotiating it during the SASL GSSAPI authentication.

B: The client does not bind after reconnecting to the server, which results in further search requests being issued without a valid bind.

B: An error message appears in Directory Studio stating ""Error while opening connection - Missing schema location in RootDSE, using default schema"" due to the client not being properly bound to the server.

B: Directory Studio is unable to browse the directory because the connection is not properly bound.",Major,FALSE,Over-decomposition
/ImageMask true does not work. Patch included.,https://issues.apache.org/jira/browse/PDFBOX-1445,"I have the following pdf... 

10 0 obj 
<< 
/Type /Page 
/MediaBox [ 0 0 612.0 792.0 ] 
/Parent 3 0 R 
/Resources << /XObject << /Obj4 4 0 R /Obj5 5 0 R /Obj6 6 0 R /Obj7 7 0 R >> /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ] >> 
/Contents [ 8 0 R 9 0 R ] 
>> 
endobj 

Which then draws 4 images. The first one is a ""base"" and then rest are image masks 

9 0 obj 
<< /Filter /FlateDecode /Length 121 >> 
stream 
q 
612.00 0 0 792.00 0.00 0.00 cm 
/Obj4 Do 
Q 
q 
0.129 g 
524.16 0 0 556.80 48.00 127.68 cm 
/Obj5 Do 
Q 
q 
0.302 g 
220.80 0 0 398.40 48.00 286.08 cm 
/Obj6 Do 
Q 
q 
0.204 g 
524.16 0 0 469.44 48.00 185.28 cm 
/Obj7 Do 
Q 
endstream 
endobj 

4 0 obj 
<< /Type /XObject /Subtype /Image /Width 1275 /Height 1650 /BitsPerComponent 8 
/ColorSpace /DeviceGray /Filter [ /FlateDecode /DCTDecode ] /Length 50485 >> 
stream 
endstream 
endobj 

5 0 obj 
<< /Type /XObject /Subtype /Image /Width 2184 /Height 2320 /BitsPerComponent 1 
/ImageMask true /Filter /CCITTFaxDecode /DecodeParms << /K -1 /Columns 2184 >> 
/Length 15580 >> 
stream 

etc ... 

The current code simply treats the imagemask as an image. Since this is just a 1 bit image it has no Alpha channel it overwrites the existing image and we simply get the last image drawn. 

In 

org.apache.pdfbox.util.operator.pagedrawer.Invoke.java 

method 

public void process(PDFOperator operator, List<COSBase> arguments) throws IOException 

after 

if (awtImage == null) 

{ LOG.warn(""getRGBImage returned NULL""); return;//TODO PKOCH } 
If you add the following code it fixes the problem. I can not provide the sample doc due to privacy reasons. 

/** 

Spec 8.9.6.2 
If ImageMask is true then the image is one bit. Black means draw the current colour and white means use the colour on the current image (ie Mask). 
Convert the map to an image with an Alpha channel so we can lay it on top 
*/ 
if(image.getImageMask()) 
{ 
Color currentColour = drawer.getGraphicsState().getStrokingColor().getJavaColor(); 
final int onColour = 0xff000000 | currentColour.getRGB(); 
BufferedImage bia = new BufferedImage(awtImage.getWidth(),awtImage.getHeight(),BufferedImage.TYPE_INT_ARGB); 
for(int y=0;y<awtImage.getHeight();y++) 
Unknown macro: { for(int x=0;x<awtImage.getWidth();x++) { bia.setRGB(x, y, (awtImage.getRGB(x, y) & 0x00ffffff) == 0xffffff ? 0x00ffffff : onColour); } } 
awtImage = bia; 
}","B: The current code treats the imagemask as an image, which leads to incorrect rendering since it overwrites the existing image and only the last image drawn is visible.

B: Modify the code in the method `process(PDFOperator operator, List<COSBase> arguments)` of the class `org.apache.pdfbox.util.operator.pagedrawer.Invoke` to properly handle imagemasks.

B: Add a check for `image.getImageMask()` to determine if the image is a one-bit image and needs special handling.

B: Implement the logic to create a new `BufferedImage` with an Alpha channel based on the current color state when the image mask is true.

B: In the loop that processes each pixel of the image, set the pixel color based on the current color for black and maintain the existing color for white.

B: Ensure that the new `BufferedImage` replaces the original `awtImage` after processing for imagemasks.",Major,FALSE,Over-analysis
build-couchdb build fails in SpiderMonkey on Debian Lenny 32-bit,https://issues.apache.org/jira/browse/COUCHDB-938,"I've followed the steps on couch.io for building without dependency hell on Debian Lenny 32-bit. However, this is a linux-vserver which runs on a 64-bit Ubuntu 10.04 host system. Maybe the configure script is making the wrong assumptions about the word size? 

(07:29:17) frank [mika]:~/git/build-couchdb# file /bin/bash 
/bin/bash: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.8, stripped 

git clone git://github.com/couchone/build-couchdb 
cd build-couchdb 
git submodule init 
git submodule update 
rake 

... 

--------- ## 
Platform. ## 
--------- ## 
hostname = frank 
uname -m = x86_64 
uname -r = 2.6.32-25-vserver 
uname -s = Linux 
uname -v = #44~ppa2-Ubuntu SMP Wed Oct 27 21:12:19 UTC 2010 

... 

c++ -o jsapi.o -c -I./dist/system_wrappers_js -include /root/git/build-couchdb/dependencies/js_src/config/gcc_hidden.h -DOSTYPE=\""Linux2.6\"" -DOSARCH=Linux -DEXPORT_JS_API -I/root/git/build-couchdb/dependencies/js_src -I. -I./dist/include -I./dist/include/nsprpub -I/root/git/build-couchdb/dependencies/js_src -fPIC -fno-rtti -fno-exceptions -Wall -Wpointer-arith -Woverloaded-virtual -Wsynth -Wno-ctor-dtor-privacy -Wno-non-virtual-dtor -Wcast-align -Wno-invalid-offsetof -Wno-variadic-macros -Wno-long-long -pedantic -fno-strict-aliasing -pthread -pipe -DNDEBUG -DTRIMMED -O3 -fstrict-aliasing -DMOZILLA_CLIENT -include ./js-confdefs.h -Wp,-MD,.deps/jsapi.pp /root/git/build-couchdb/dependencies/js_src/jsapi.cpp 
In file included from /root/git/build-couchdb/dependencies/js_src/nanojit/nanojit.h:277, 
from /root/git/build-couchdb/dependencies/js_src/jsbuiltins.h:45, 
from /root/git/build-couchdb/dependencies/js_src/jsapi.cpp:59: 
/root/git/build-couchdb/dependencies/js_src/nanojit/Containers.h:164: error: integer constant is too large for 'long' type 
make[1]: *** [jsapi.o] Error 1 
make[1]: Leaving directory `/tmp/tracemonkey_build20101107-10375-oeuo6o' 
make: *** [default] Error 2 
git checkout HEAD configure.in 
git clean -df . 
Removing configure 
rake aborted! 
Command failed with status (2): [make ...] 

(See full trace by running task with --trace)","B: The configure script may be making incorrect assumptions about the word size when building on a linux-vserver that runs on a 64-bit Ubuntu 10.04 host system while targeting a Debian Lenny 32-bit environment.

B: An error occurs during the compilation of `jsapi.o` related to an integer constant being too large for the 'long' type in the file `/root/git/build-couchdb/dependencies/js_src/nanojit/Containers.h`.

B: The build process fails with the message ""make: *** [default] Error 2,"" indicating that the make command encountered an error that prevented it from completing successfully.

B: The command ""rake"" fails with an aborted status due to the previous compilation errors, specifically stating ""Command failed with status (2): [make ...]"".",Major,FALSE,Over-analysis
ValidField component creates javascript referencing handle_invalid_field() method that no longer exists,https://issues.apache.org/jira/browse/TAPESTRY-649,"Here is the html output from the <body>. I have a @FormBorder component 
which includes a form that encloses the actual form inputs. The handle_invalid_field() function does not get created. 

<script type=""text/javascript"" 
src=""/app?digest=b4909c59529064c46eb8843b65911500&path=%2Forg% 
2Fapache%2Ftapestry%2Fform%2FForm.js&service=asset""></script> 
<script type=""text/javascript""><!-- 

function validate_name(event) 
{ 
var field = document.Form.name; 

if (field.value.length == 0) 

{ handle_invalid_field(event, field, ""You must enter a value for Name.""); return; } 
} 

// --></script> 

<div class=""page""> 

<div class=""maindiv""> 

<div class=""topnav""> 
<div class=""globalnav""><span class=""links""><a href=""#"">Contact Us</a| <a href=""#"">Sitemap</a| <a href=""#"">Search</a| <a href=""#"">Join Us</a></span></div> 
<div class=""logo""><a href=""#""><img src=""images/logo_ulifeline.gif"" width=""219"" height=""52"" alt="""" border=""0"" /></a></div> 
<div class=""mitmed""><img src=""images/logo_mitmed.gif"" width=""139"" height=""21"" alt="""" /></div> 
<img src=""images/topwave.png"" width=""798"" height=""60"" alt="""" class=""bottomwave"" /> 
</div> 

<div class=""c-content""> 

<div class=""leftcol""> 
<p class=""navbutton""><a href=""#"" class=""button"">Log Out</a></p> 
<!-- <div class=""stylebuttonout""><div class=""stylebuttonin""><a href=""#"">Log Out</a></div></div--> 
<div class=""leftnav""> 
<table class=""navigation"" cellpadding=""0"" cellspacing=""0""> 
<tr><td><a href=""/app?component=%24AdminBorder.%24DirectLink&page=admin%2FAddSpecialty&service=direct"">Ulifeline Administrator Home</a></td></tr> 
<tr><td><a href=""/app?component=%24AdminBorder.%24DirectLink_0&page=admin%2FAddSpecialty&service=direct"" id=""nav_specialties"">Specialties</a></td></tr> 

</table> 
</div> 
</div> 

<div class=""contentarea""> 

<h1 id=""pagetitle"">Add Specialty</h1> 

<form method=""post"" action=""/app"" name=""Form"" id=""Form""> 
<div><input type=""hidden"" name=""formids"" value=""Hidden,Hidden_0,name,Checkbox,Submit""/> 
<input type=""hidden"" name=""component"" value=""formBorder.$Form""/> 
<input type=""hidden"" name=""page"" value=""admin/AddSpecialty""/> 
<input type=""hidden"" name=""service"" value=""direct""/> 
<input type=""hidden"" name=""submitmode"" value=""""/> 
<input type=""hidden"" name=""Hidden"" value=""0""/> 
<input type=""hidden"" name=""Hidden_0"" value=""0""/> 
</div> 
<table class=""form""> 

<tr> 
<th><label for=""name"">Name</label></th> 
<td><input type=""text"" name=""name"" id=""name""/></td> 
</tr> 
<tr> 
<th>Active</th> 
<td><input type=""checkbox"" name=""Checkbox"" id=""Checkbox""/></td> 
</tr> 

</table> 

<div class=""formsubmit""> 
<input type=""submit"" name=""Submit"" value=""Add Specialty"" id=""Submit"" class=""button""/> 
</div> 
</form> 

</div> 

</div> 

<hr /> 

<div class=""footer""> 
<span class=""left""> 
<span class=""right""> 
_Lifeline<br /> 
<a href=""#"">Terms of Use</a| <a href=""#"">Privacy Statement</a> 
</span> 
University Specific Contact Info goes here. Lorem ipsum dolor sit amet:<br /> 
Phone: 555-555-1234 Online: <a href=""#"">www.somwebaddress.edu</a> 
</span> 
</div> 

</div> 

</div> 

<script language=""JavaScript"" type=""text/javascript""><!-- 
Tapestry.register_form('Form'); 

Tapestry.onsubmit('Form', validate_name); 

Tapestry.set_focus('name'); 

// --></script></body>","B: The `handle_invalid_field()` function does not get created.

B: The `validate_name(event)` function does not validate the name input correctly when it is empty.

B: The form should prevent submission if the name input is empty.

B: The JavaScript function `Tapestry.onsubmit('Form', validate_name);` should be modified to handle form validation properly.

B: The HTML form action should be reviewed to ensure it points to the correct endpoint.

B: The `src` attribute in the script tag for Form.js may need to be checked for correctness. 

B: The form input for ""name"" should have an appropriate validation message displayed when it is empty.",Major,FALSE,Over-analysis
"Canonical-ize hostnames for Hive metastore, and HS2 servers.",https://issues.apache.org/jira/browse/HIVE-17218,"Currently, the HiveMetastoreClient and HiveConnection do not canonical-ize the hostnames of the metastore/HS2 servers. In deployments where there are multiple such servers behind a VIP, this causes a number of inconveniences: 

The client-side configuration (e.g. hive.metastore.uris in hive-site.xml) needs to specify the VIP's hostname, and cannot use a simplified CNAME, in the thrift URL. If the hive.metastore.kerberos.principal is specified using _HOST, one sees GSS failures as follows: 
hive --hiveconf hive.metastore.kerberos.principal=hive/_HOST@GRID.MYTH.NET --hiveconf hive.metastore.uris=""thrift://simplified-hcat-cname.grid.myth.net:56789"" 
... 
Exception in thread ""main"" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient 
at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:542) 
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) 
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) 
... 
This is because _HOST is filled in with the CNAME, and not the canonicalized name. 

Oozie workflows that use HCat <credential> have to always use the VIP hostname, and can't use _HOST-based service principals, if the CNAME differs from the VIP name. 
If the client-code simply canonical-ized the hostnames, it would enable the use of both simplified CNAMEs, and _HOST in service principals.","B: The HiveMetastoreClient does not canonicalize the hostname of the metastore server, which causes issues when multiple servers are behind a VIP.

B: The HiveConnection does not canonicalize the hostname of the HS2 server, leading to complications in deployments with multiple servers behind a VIP.

B: The client-side configuration requires the VIP's hostname in hive.metastore.uris and does not allow the use of a simplified CNAME, causing configuration difficulties.

B: Specifying hive.metastore.kerberos.principal using _HOST results in GSS failures due to _HOST being filled with the CNAME instead of the canonicalized name.

B: Oozie workflows that use HCat <credential> cannot utilize _HOST-based service principals when the CNAME differs from the VIP name, limiting flexibility in service principal usage.

B: The lack of hostname canonicalization prevents the use of simplified CNAMEs in conjunction with _HOST in service principals.",Major,FALSE,Over-decomposition
Unable to start karaf instance on fresh installation - elasticsearch error : java.lang.IllegalStateException: Received message from unsupported version: [5.2.2] minimal compatible version is: [5.6.0],https://issues.apache.org/jira/browse/UNOMI-196,"Services do not go live on a fresh installation. 

 
1. running ""bin/karaf"" results in karaf loading and then ""Initializing Unomi..."" which never finished.

2. Looking into log:display - it says: ""Unable to initialize bean elasticSearchPersistenceServiceImpl""

2018-08-23 12:55:58,615 | INFO | FelixStartLevel | RegionsPersistenceImpl | 49 - org.apache.karaf.region.persist - 3.0.8 | Loading region digraph persistence
2018-08-23 12:55:58,899 | INFO | FelixStartLevel | SecurityUtils | 30 - org.apache.sshd.core - 0.14.0 | BouncyCastle not registered, using the default JCE provider
2018-08-23 12:55:59,110 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Starting JMX OSGi agent
2018-08-23 12:55:59,157 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering MBean with ObjectName [osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9] for service with service.id [15]
2018-08-23 12:55:59,194 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.ServiceStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=serviceState,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,196 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.wiring.BundleWiringStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=wiringState,version=1.1,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,196 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.BundleStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=bundleState,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.PackageStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=packageState,version=1.5,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.service.cm.ConfigurationAdminMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,197 | INFO | FelixStartLevel | core | 60 - org.apache.aries.jmx.core - 1.1.6 | Registering org.osgi.jmx.framework.FrameworkMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@5e481248 with name osgi.core:type=framework,version=1.7,framework=org.apache.felix.framework,uuid=987fafb7-71a9-42eb-afcb-8ad75973c0c9
2018-08-23 12:55:59,371 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | EventAdmin support enabled, servlet events will be postet to topics.
2018-08-23 12:55:59,386 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | LogService support enabled, log events will be created.
2018-08-23 12:55:59,406 | INFO | FelixStartLevel | Activator | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | Pax Web started
2018-08-23 12:55:59,839 | INFO | pool-6-thread-1 | Server | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | jetty-8.1.19.v20160209
2018-08-23 12:55:59,935 | INFO | pool-6-thread-1 | AbstractConnector | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Started SelectChannelConnector@0.0.0.0:8181
2018-08-23 12:55:59,936 | INFO | pool-6-thread-1 | JettyServerImpl | 79 - org.ops4j.pax.web.pax-web-jetty - 3.2.9 | Pax Web available at [0.0.0.0]:[8181]
2018-08-23 12:55:59,964 | INFO | pool-6-thread-1 | JettyServerImpl | 79 - org.ops4j.pax.web.pax-web-jetty - 3.2.9 | Pax Web available at [0.0.0.0]:[9443]
2018-08-23 12:55:59,997 | INFO | pool-6-thread-1 | SslContextFactory | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Enabled Protocols [SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2] of [SSLv2Hello, SSLv3, TLSv1, TLSv1.1, TLSv1.2]
2018-08-23 12:55:59,999 | INFO | pool-6-thread-1 | AbstractConnector | 70 - org.eclipse.jetty.aggregate.jetty-all-server - 8.1.19.v20160209 | Started SslSelectChannelConnector@0.0.0.0:9443
2018-08-23 12:56:00,077 | INFO | FelixStartLevel | ContextLoaderListener | 106 - org.springframework.osgi.extender - 1.2.1 | Starting [org.springframework.osgi.extender] bundle v.[1.2.1]
2018-08-23 12:56:00,188 | INFO | FelixStartLevel | ExtenderConfiguration | 106 - org.springframework.osgi.extender - 1.2.1 | No custom extender configuration detected; using defaults...
2018-08-23 12:56:00,199 | INFO | FelixStartLevel | TimerTaskExecutor | 101 - org.apache.servicemix.bundles.spring-context - 3.2.17.RELEASE_1 | Initializing Timer
2018-08-23 12:56:00,833 | INFO | pache.cxf.osgi]) | HttpServiceFactoryImpl | 78 - org.ops4j.pax.web.pax-web-runtime - 3.2.9 | Binding bundle: [org.apache.cxf.cxf-rt-transports-http [133]] to http service
2018-08-23 12:56:01,359 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer)]
2018-08-23 12:56:01,418 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer)]
2018-08-23 12:56:01,436 | INFO | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.event.EventProducer), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:01,764 | WARN | FelixStartLevel | OSGiScriptEngineManager | 184 - com.hazelcast - 3.4.2 | Found ScriptEngineFactory candidate for com.sun.script.javascript.RhinoScriptEngineFactory, but cannot load class! -> java.lang.ClassNotFoundException: com.sun.script.javascript.RhinoScriptEngineFactory not found by com.hazelcast [184]
2018-08-23 12:56:01,827 | INFO | FelixStartLevel | XmlConfigLocator | 184 - com.hazelcast - 3.4.2 | Loading configuration /home/unomi/unomi-1.2.0-incubating/etc/hazelcast.xml from System property 'hazelcast.config'
2018-08-23 12:56:01,827 | INFO | FelixStartLevel | XmlConfigLocator | 184 - com.hazelcast - 3.4.2 | Using configuration file at /home/unomi/unomi-1.2.0-incubating/etc/hazelcast.xml
2018-08-23 12:56:02,118 | INFO | FelixStartLevel | DefaultAddressPicker | 184 - com.hazelcast - 3.4.2 | [LOCAL] [cellar] [3.4.2] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2018-08-23 12:56:02,128 | INFO | FelixStartLevel | DefaultAddressPicker | 184 - com.hazelcast - 3.4.2 | [LOCAL] [cellar] [3.4.2] Picked Address[127.0.0.1]:5701, using socket ServerSocket[addr=/0.0.0.0,localport=5701], bind any local is true
2018-08-23 12:56:02,313 | INFO | FelixStartLevel | OperationService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Backpressure is disabled
2018-08-23 12:56:02,316 | INFO | FelixStartLevel | BasicOperationScheduler | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Starting with 2 generic operation threads and 2 partition operation threads.
2018-08-23 12:56:02,369 | INFO | FelixStartLevel | system | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Hazelcast 3.4.2 (20150326 - f6349a4) starting at Address[127.0.0.1]:5701
2018-08-23 12:56:02,370 | INFO | FelixStartLevel | system | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Copyright (C) 2008-2014 Hazelcast.com
2018-08-23 12:56:02,371 | INFO | FelixStartLevel | Node | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Creating TcpIpJoiner
2018-08-23 12:56:02,372 | INFO | FelixStartLevel | LifecycleService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5701 is STARTING
2018-08-23 12:56:02,496 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Connecting to /127.0.0.1:5703, timeout: 0, bind-any: true
2018-08-23 12:56:02,497 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Could not connect to: /127.0.0.1:5703. Reason: SocketException[Connection refused to address /127.0.0.1:5703]
2018-08-23 12:56:02,497 | INFO | .cached.thread-2 | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5703 is added to the blacklist.
2018-08-23 12:56:02,498 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Connecting to /127.0.0.1:5702, timeout: 0, bind-any: true
2018-08-23 12:56:02,498 | INFO | .cached.thread-2 | SocketConnector | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Could not connect to: /127.0.0.1:5702. Reason: SocketException[Connection refused to address /127.0.0.1:5702]
2018-08-23 12:56:02,499 | INFO | .cached.thread-2 | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5702 is added to the blacklist.
2018-08-23 12:56:03,499 | INFO | FelixStartLevel | TcpIpJoiner | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2]

Members [1]

{ Member [127.0.0.1]:5701 this }
2018-08-23 12:56:03,520 | INFO | FelixStartLevel | LifecycleService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Address[127.0.0.1]:5701 is STARTED
2018-08-23 12:56:03,582 | INFO | FelixStartLevel | InternalPartitionService | 184 - com.hazelcast - 3.4.2 | [127.0.0.1]:5701 [cellar] [3.4.2] Initializing cluster partition table first arrangement...
2018-08-23 12:56:03,681 | INFO | rint Extender: 2 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,682 | INFO | rint Extender: 3 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,682 | INFO | rint Extender: 1 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.GroupManager), (objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 3 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.config/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 1 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.bundle/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,742 | INFO | rint Extender: 2 | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Bundle org.apache.karaf.cellar.features/3.0.3 is waiting for dependencies [(objectClass=org.apache.karaf.cellar.core.ClusterManager)]
2018-08-23 12:56:03,927 | INFO | FelixStartLevel | BundleWatcher | 193 - org.apache.unomi.lifecycle-watcher - 1.2.0.incubating | Bundle watcher initialized.
2018-08-23 12:56:04,119 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/default.json, loading...
2018-08-23 12:56:04,119 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/campaign.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/campaignevent.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/event.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/goal.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/personaSession.json, loading...
2018-08-23 12:56:04,120 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/profile.json, loading...
2018-08-23 12:56:04,121 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/propertyType.json, loading...
2018-08-23 12:56:04,121 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/rule.json, loading...
2018-08-23 12:56:04,123 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/scoring.json, loading...
2018-08-23 12:56:04,125 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/segment.json, loading...
2018-08-23 12:56:04,129 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Found mapping at bundle://205.0:0/META-INF/cxs/mappings/session.json, loading...
2018-08-23 12:56:04,156 | INFO | FelixStartLevel | sticSearchPersistenceServiceImpl | 205 - org.apache.unomi.persistence-elasticsearch-core - 1.2.0.incubating | Connecting to ElasticSearch persistence backend using cluster name contextElasticSearch and index name context...
2018-08-23 12:56:06,682 | ERROR | FelixStartLevel | ServiceRecipe | 15 - org.apache.aries.blueprint.core - 1.6.2 | Error retrieving service from ServiceRecipe[name='elasticSearchPersistenceService']
org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:738)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate2(BeanRecipe.java:848)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:811)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.RefRecipe.internalCreate(RefRecipe.java:62)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:106)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:285)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalGetService(ServiceRecipe.java:252)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalCreate(ServiceRecipe.java:149)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:186)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:724)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:411)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1]
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:]
at java.lang.Thread.run(Thread.java:748)[:1.8.0_181]
Caused by: NoNodeAvailableException[None of the configured nodes are available: [{#transport#-1}

{o4uAdKg_QpCsuv2dzXKFyw}{localhost}{127.0.0.1:9300}]]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
[TRIMMED 50K CHAR LIMIT]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498)[:1.8.0_181]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:299)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:980)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:736)[15:org.apache.aries.blueprint.core:1.6.2]
... 35 more
2018-08-23 12:56:06,711 | WARN | FelixStartLevel | BeanRecipe | 15 - org.apache.aries.blueprint.core - 1.6.2 | Object to be destroyed is not an instance of UnwrapperedBeanHolder, type: null
2018-08-23 12:56:06,715 | ERROR | FelixStartLevel | BlueprintContainerImpl | 15 - org.apache.aries.blueprint.core - 1.6.2 | Unable to start blueprint container for bundle org.apache.unomi.persistence-elasticsearch-core/1.2.0.incubating
org.osgi.service.blueprint.container.ComponentDefinitionException: org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:310)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalGetService(ServiceRecipe.java:252)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.internalCreate(ServiceRecipe.java:149)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:186)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:724)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:411)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:276)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:300)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:269)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.createContainer(BlueprintExtender.java:265)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BlueprintExtender.modifiedBundle(BlueprintExtender.java:255)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:500)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.customizerModified(BundleHookBundleTracker.java:433)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$AbstractTracked.track(BundleHookBundleTracker.java:725)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$Tracked.bundleChanged(BundleHookBundleTracker.java:463)[9:org.apache.aries.util:1.1.1]
at org.apache.aries.util.tracker.hook.BundleHookBundleTracker$BundleEventHook.event(BundleHookBundleTracker.java:422)[9:org.apache.aries.util:1.1.1]
at org.apache.felix.framework.util.SecureAction.invokeBundleEventHook(SecureAction.java:1103)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.createWhitelistFromHooks(EventDispatcher.java:695)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:483)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4403)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.startBundle(Felix.java:2092)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)[org.apache.felix.framework-4.2.1.jar:]
at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)[org.apache.felix.framework-4.2.1.jar:]
at java.lang.Thread.run(Thread.java:748)[:1.8.0_181]
Caused by: org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize bean elasticSearchPersistenceServiceImpl
at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:738)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate2(BeanRecipe.java:848)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:811)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe$1.call(AbstractRecipe.java:79)[15:org.apache.aries.blueprint.core:1.6.2]
at java.util.concurrent.FutureTask.run(FutureTask.java:266)[:1.8.0_181]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:88)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.RefRecipe.internalCreate(RefRecipe.java:62)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:106)[15:org.apache.aries.blueprint.core:1.6.2]
at org.apache.aries.blueprint.container.ServiceRecipe.createService(ServiceRecipe.java:285)[15:org.apache.aries.blueprint.core:1.6.2]
... 27 more
Caused by: NoNodeAvailableException[None of the configured nodes are available: [{#transport#-1}

{o4uAdKg_QpCsuv2dzXKFyw}
{localhost}

{127.0.0.1:9300}
]]
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:344)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:242)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:356)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)[205:org.apache.unomi.persistence-elasticsearch-core:1.2.0.incubating]
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$1.execute(ElasticSearchPersistenceServiceImpl.java:290)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl$InClassLoaderExecute.executeInClassLoader(ElasticSearchPersistenceServiceImpl.java:1747)
at org.apache.unomi.persistence.elasticsearch.ElasticSearchPersistenceServiceImpl.start(ElasticSearchPersistenceServiceImpl.java:367)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodA","B: Services do not go live on a fresh installation.

B: Running ""bin/karaf"" results in karaf loading and then ""Initializing Unomi..."" which never finishes.

B: Log message indicates: ""Unable to initialize bean elasticSearchPersistenceServiceImpl"".

B: The system is unable to connect to the ElasticSearch persistence backend using the specified cluster name and index name.

B: The error message states that no nodes are available for the configured ElasticSearch instance.

B: Several bundles are waiting for dependencies related to GroupManager and ClusterManager.

B: A ClassNotFoundException is encountered for the class com.sun.script.javascript.RhinoScriptEngineFactory.

B: Multiple connection attempts to localhost at various ports (5702, 5703) are refused, adding them to a blacklist.",Major,FALSE,Over-analysis
Need a 32×32 icon for Java options,https://issues.apache.org/jira/browse/NETBEANS-284,"The ContainerRegistration for Java options, defined in options.java/src/org/netbeans/modules/options/java/resources/package-info.java, is currently using a 20¡Á20 iconBase, whereas other options icons are 32¡Á32. This makes the icon look smaller than the others in NetBeans > Preferences. See the attached screenshot.","B: The ContainerRegistration for Java options is using a 20×20 iconBase.  
B: Other options icons in NetBeans > Preferences are using a 32×32 iconBase.  
B: The icon size difference causes the Java options icon to appear smaller than others in NetBeans > Preferences.  
B: A screenshot is attached to illustrate the size discrepancy.",Minor,FALSE,Over-decomposition
Hadoop configurations on the classpath seep into the S3 file system configs,https://issues.apache.org/jira/browse/FLINK-10383,"The S3 connectors are based on a self-contained shaded Hadoop. By design, they should only use config value from the Flink configuration. 

However, because Hadoop loads implicitly configs from the classpath, existing ""core-site.xml"" files can interfere with the configuration in ways intransparent for the user. We should ensure such configs are not loaded.","B: The S3 connectors should only use configuration values from the Flink configuration.  

B: Existing ""core-site.xml"" files should not interfere with the configuration of the S3 connectors.  

B: Ensure that Hadoop does not load configurations implicitly from the classpath.",Major,FALSE,Incorrect Interpretation of Solutions
Bandar Togel Terbaik,https://issues.apache.org/jira/browse/AAR-9319,"Bandar Togel Hadiah 4D 10 Juta Terbaik & Terpercaya | Daftar di BO Togel Terpercaya bet 100 perak 2021 
[Bandar Togel Hadiah 4D 10 Juta Terpercaya|#] terhadap Zaman sekarang ini kami telah perlu pandai pandai di dalam melacak penghasilan sampingan, apabila selama ini kamu sekedar mengandalkan penghasilan primer saja gara-gara telah berlimpah peluang yang terbuka sementara ini dan keliru satunya ialah bersama bermain di Bandar Togel Hadiah 4D 10 Juta Terbesar Dan Terpercaya Kepada kamu sekalian. Layaknya yang kamu ketahui sendiri bahwa permainan tebak angka ini udah berlimpah pengaruhi nasib seseorang jadi jutawan sekedar bersama kapital yang kecil saja, Pasaran Togel Singapore jadi tidak benar satu pasaran yang paling tak terhitung dimainkan oleh bettor sementara ini dan cocok bagi kamu yang dambakan mulai terjun di dalam global pertogelan. 

Pastinya kamu seluruh telah tau apa tersebut Togel Online sebab memang permainan BO Togel Hadiah 4d 10 juta satu ini telah terlampau populer berasal dari pernah sampai sekarang ini, nah disini kita inginkan merekomendasikan sebagian Bandar Singapore Pools yang tak terhitung dimainkan oleh para Togellers yang tersedia di Internet. Kudu diketahui bahwa sebelum kita merekomendasikan seluruh Web site Bandar Togel Hadiah 4d 10 Juta terpercaya di bawah ini udah kita coba mainkan terlebih dahulu sebelum memberitahukannya kepada kamu seluruh, bagi kamu yang tengah melacak Web Togel Terpercaya maka sanggup mempertimbangkan untuk bermain di tidak benar satu web site ini. 

Daftar Bandar Togel Online Resmi Terpercaya 2021 
Laman ini menghadirkan Daftar Bandar Togel Hadiah 4D 10 Juta Terpercaya tahun ini yang tersedia di kawasan Asia, khususnya wilayah Indonesia. Kumpulan Bandar Togel Hadiah 4D 10 Juta Terpercaya ini, akan dibahas satu persatu secara singkat dan memahami, supaya tiap tiap betor yang menonton akan jelas segera julukan web togel terpercaya yang sanggup digabung untuk daftar akun togel formal dan dijadikan partner bertaruh toto togel online. Yang tentu dan bukan barangkali mengecewakan kamu terhadap waktu ini adalah, seluruh Bandar yang kita rekomendasikan disini merupakan Bandar Togel Terpercaya hadiah terbesar. Pasti saja tersebut akan memicu untuk terhadap waktu bergabung, sebab bukan seluruh Bandar sekarang ini udah jadi Bandar Togel Hadiah 4D 10 Juta layaknya yang kita referensikan disini. 

Sehabis mempunyai account member bandar Permainan Toto Hadiah 4D 10 Juta Terpercaya Indonesia, lakukanlah pengisian game credit user id. Mampu bersama dengan transfer lewat atm, m banking, e-banking bank lokal terbesar layaknya Bca, Bni, Bri, Mandiri, CIMB Niaga ataupun deposit togel via pulsa Telkomsel. Dapat juga mengikuti kecanggihan teknologi era now bersama kenakan pelaksanaan dompet online, e-wallet layaknya Gopay, Ovo, DANA sertar Linkaja!. Agen togel deposit ovo selalu siap online 24jam melayani segala keperluan kesibukan permainan judi togel depo termurah. Pokoknya ga bakal rugi deh main di bandar togel deposit 10 ribu karna layanannya kondusif, cepat dan nyaman. 

Bandar Togel Hadiah 4D 10 Juta Terbesar Dan Terpercaya 
Pada Zaman sekarang ini kita sudah harus pintar _ pintar dalam mencari penghasilan sampingan, apabila selama ini anda hanya mengandalkan penghasilan utama saja karena sudah banyak peluang yang terbuka saat ini dan salah satunya ialah dengan bermain di [BO TOGEL HADIAH 4D 10 JUTA TERBESAR DAN TERPERCAYA|#] kepada anda sekalian. Seperti yang anda ketahui sendiri bahwa permainan tebak angka ini sudah banyak merubah nasib seseorang menjadi jutawan hanya dengan modal yang kecil saja, Pasaran Togel Singapore menjadi salah satu pasaran yang paling banyak dimainkan oleh bettor saat ini dan cocok bagi anda yang ingin mulai terjun dalam dunia pertogelan. 

Pastinya anda semua sudah tau apa itu Togel Online karena memang permainan BO Togel Hadiah 4D 10 juta satu ini sudah sangat populer dari dulu hingga sekarang ini, nah disini kami ingin merekomendasikan beberapa Bandar Singapore Pools yang banyak dimainkan oleh para Togellers yang ada di Internet. Perlu diketahui bahwa sebelum kami merekomendasikan semua Situs Bandar Togel Hadiah 4D 10 Juta terpercaya di bawah ini sudah kami coba mainkan terlebih dahulu sebelum memberitahukannya kepada anda semua, bagi anda yang sedang mencari Website Togel Terpercaya maka bisa mempertimbangkan untuk bermain di salah satu situs ini. 

Cara Daftar Akun di Situs BO Togel Terpercaya di Indonesia 24jam 
Untuk bermain permainan togel di Togel Situs Bandar Togel Terpercaya. Tentunya kalian harus memiliki akun untuk melakukan berbagai jenis transaksi dalam melakukan betting di situs togel resmi toto. Karena kalau kita tidak punya akun, Tentunya kita tidak dapat melakukan betting, deposit, maupun withdraw. Disini kami sediakan langkah-langkah khusus untuk para bettor yang akan bermain di Togel 5 Bandar Togel Terpercaya sebagia berikut : 

Pastikan anda berumur 18 Tahun lebih 
Kunjungi situs 5 Bandar Togel Terpercaya 
Klik tombol Pendaftaran / Daftar 
Isi Biodata Diri dengan baik dan benar 
Isi Username akun 
isi password sesuai selera anda 
Masukkan email yang aktif 24jam 
Masukkan no handphone yang dapat dihubungi 
Pilih Jenis pembayaran (Bank Lokal maupun E-wallet) 
Isi atas nama rekening tsb 
Masukan nomor rekening anda 
Selesai. 


Sebelum melakukan pendaftaran. Pastikan jaringan internet yang kamu gunakan cepat dan tentunya stabil. Karena untuk bermain togel online. Kamu harus mempunyai koneksi yang bagus untuk bermain maupun daftar akun togel di [Daftar Bandar Togel Terpercaya|#]. Maka dari itulah kami sarankan agar menggunkan koneksi Wi-Fi ketika bermain togel online di Situs Bandar Togel Hadiah 4d 10 Juta Terbesar. 

4 Negara Pasaran Bandar Togel Resmi Dan Terbaik Di Dunia 
Di dalam perjudian togel, tentunya tidak akan menarik apabila tidak memasang taruhan pada pasaran togel yang resmi dan terbaik di dunia. Sebab bila kita memasang pada pasaran togel sembarangan atau tidak legal di dunia. Pastinya kemenangan yang akan diraih akan sangat sulit atau bisa dikatakan itu adalah pasaran togel settingan. Sehingga apapun angka yang akan kita pasangkan maka kekalahan yang akan didapatkan. 

Lalu seperti apakah negara-negara yang menjadi pasaran togel resmi dan terbaik di dunia menurut organisasi resmi seperti WLA (World Lottery Association) dan APLA (Asia Pacific on the World Lottery Association). Tentunya seluruh bettor togel sekarang ini ingin mengetahui atau penasaran dengan nama-nama pasaran togel terbaik tersebut bukan?. 

Mari langsung saja admin dari blog seputaran permainan togel online ini akan menyebutkan 4 saja negara yang menjadi pasaran togel resmi dan terbaik di dunia menurut APLA dan WLA sebagai berikut : 

1. Pasaran Toto Macau 
Toto macau adalah pasaran togel terbaik dan resmi di dunia yang sekarang ini menjadi pasaran nomor 1 di indonesia. Dimana pada pasaran togel toto macau ini. Memiliki 3 mode jenis betting terbaik yang memudahkan seluruh bettor untuk melakukan taruhan. Bisa memilih mode dengan hadiah full terbesar ataupun mode diskon yang memiliki hadiah standart namun sewaktu melakukan taruhan modal yang dikeluarkan tidak terlalu banyak karena memiliki diskon atau potongan. 

Namun toto macau sendiri dipilih bukan karena hal sepele itu saja. Ada berbagai kelebihan dan keuntungan lainnya yang menjadikan pasaran tersebut nomor 1 di indonesia sekarang ini. Meskipun pasaran ini baru saja hadir sejak tahun 2015 silam dan paling muda dibandingkan dengan pasaran-pasaran togel lainnya seperti Hongkong (HK) , Sydney (SDY) ataupun Singapore (SGP). Tetapi pasaran ini memiliki hal yang tidak dimiliki oleh pasaran-pasaran togel tertua ataupun lawas itu. 

Biasanya pasaran togel umum atau pasaran togel dunia lainnya hanya memiliki jadwal perputaran angka result 1 kali dalam setiap harinya. Namun berbeda halnya dengan pasaran togel macau ini. Ia memiliki 4 kali perputaran angka yang live yang bisa dimainkan oleh seluruh bettor setiap harinya. Dengan hadiah yang besar dan memiliki hasil result terbanyak, tentunya hal tersebut menjadikan toto macau adalah pasaran nomor 1 di dunia sekarang ini karena tidak ada pasaran yang mengeluarkan angka sebanyak itu dalam 1 harinya. 

Selain itu ia memiliki situs resmi pengeluaran angka yang berbasis live streaming atau siaran langsung setiap jadwal atau jam tutup pasaran. Sehingga seluruh bettor di tanah air kita dapat menyaksikan secara langsung perputaran angka tersebut Oleh karena pasaran ini memiliki lisensi resmi dari pihak WLA dan APLA tentunya setiap pengeluaran angka itu di jamin aman 100% dan tidak ada kecurangan apapun. 

Buat seluruh bettor di luar sana yang masih kurang mengetahui jadwal perputaran angka atau jam result dari pasaran toto macau di Bandar Togel Hadiah 4D 10 Juta Terpercaya ini anda semua dapat melihatnya di bawah ini : 

Jam Tutup Pasaran 
12:59 WIB 
15:59 WIB 
18:59 WIB 
21:59 WIB 
Jam Tutup Pasaran 
13:15 WIB 
16:15 WIB 
19:15 WIB 
22:15 WIB 


Setiap perputaran atau bola gelinding yang akan di tampilkan pada permainan toto macau, Hanya akan mencari 4 bola terakhir yang masih berada pada meja. Sehingga kita dapat menyimpulkan dengan mudah bahwasannya angka apa saja yang akan menjadi result setiap perputaran. Dan untuk live streaming atau situs resmi toto macau dapat di lihat langsung pada website ataupun situs Bandar Togel Hadiah 4D 10 Juta Terpercaya. Pastikan anda melakukan taruhan pada pasaran terbaik ini dan menjadi salah seorang bettor ternama di indonesia tahun 2021. 

Setelah anda sudah yakin pada situs pilihan kalian, anda dapat langsung membuat akun togel online dengan melakukan pendaftaran pada Bandar Togel Hadiah 4D 10 Juta Terpercaya secara gratis. Cara daftar akun togel resmi online sangat mudah dilakukan oleh siapa saja. Yang terpenting anda bersedia memberikan beberapa data pribadi anda dan kemanan privacy nya terjamin 100%. Karna data-data tersebut sangat berguna bagi kelangsungan aktivitas judi togel uang asli di bo toto macau bet 100 diskon besar. Perangkat elektronik seperti computer atau smartphone dan koneksi internet yang stabil sangatlah dibutuhkan. Sebab itu semua merupakan alat 
Utama bila ingin bertaruh lotto secara daring. Setelah menyelesaikan proses registrasi, anda memiliki akses masuk ke semua pasaran togel online terlengkap di asia dan dunia. Ayo segera bikin akun toto dengan Bandar Togel Hadiah 4D 10 Juta Terpercaya sekarang juga! 

2. Pasaran Togel Hongkong 
Hongkong pools adalah nama pasaran togel terbaik ke-2 yang ditampilkan pada situs blog seputar permainan togel online ini. Dimana hongkong atau yang biasa dikenal dengan nama lain yaitu HK merupakan salah satu pasaran togel terlama dan terbaik di indonesia. Hampir seluruh bettor di indonesia mengenal dengan nama pasaran togel tersebut. 

Hongkong pools dulunya terkenal melalui via bandar darat togel di indonesia. Tidak seperti pasaran toto macau yang terkenal karena zaman sudah sangat modern seperti sekarang ini. Dulunya pasaran hongkong hanya dikenal oleh bettor-bettor lama maupun yang sudah tua saja. Karena dulunya pasaran togel yang aktif di indonesia hanya beberapa saja. Di antara salah satunya adalah pasaran togel hongkong sendiri. 

Pasaran yang sudah lama hadir dan memiliki lisensi atau sertifikat resmi dari pihak APLA dan WLA di dunia adalah Hongkong pools. Pasaran ini sangat diminati oleh seluruh bettor togel di indonesia karena memiliki jadwal result yang sangat cocok untuk dimainkan. Dimana jam result yang dimiliki oleh hongkong pools adalah 23.00 WIB yang merupakan adalah waktu senggang atau waktu santai untuk para masyarakat pekerja di indonesia. Sehingga dapat dimainkan tanpa harus terburu-buru. 

Tetapi jika anda semua malas atau enggan mencari satu-persatu website yang berada di internet sekarang ini. Karena ribet ataupun memakan waktu yang cukup lama. Tentunya pada situs blog togel online ini anda semua bisa memilih dari beberapa situs yang kami sajikan tersebut sebagai partner resmi anda semua dalam melakukan taruhan. Dikarenakan setiap website yang telah direferensikan pada situs ini dijamin aman dan terpercaya 100%. 

Untuk hadiah togel pada pasaran hongkong ini dulunya memiliki hadiah togel yang standar ataupun tergolong kecil. Sebab para bettor hanya dapat melakukan taruhan togel pada bandar darat di indonesia. Berbeda dengan pasaran toto macau yang memiliki hadiah togel terbesar dan spektakuler. Tentunya seiring berjalannya waktu pasaran togel pun berkembang mengikuti zaman yang modern. Dan sekarang kita bisa melakukan taruhan pada pasaran togel tersebut dengan 3 mode betting yang memiliki hadiah togel setimpal dengan pasaran-pasaran lainnya. 

3. Pasaran Togel Sydney 
Sydney pools atau biasa yang dikenal dengan nama lain oleh para bettor yaitu SDY merupakan salah satu pasaran togel resmi dan terbaik yang memiliki sertifikat dan reputasi terbaik di indonesia. Tidak seperti pasaran togel lainnya atau pasaran abal-abal di luar sana yang sewaktu-waktu bisa mengubah hasil result pada situs pengeluaran angka. Tentunya pada pasaran sydney di jamin 100% aman dan tidak ada kecurangan karena memiliki akses resmi dari pihak WLA dan APLA. 

Namun siapa sangka bahwasannya pasaran togel sydney ini menjadi salah satu pasaran togel terbaik di indonesia yang memiliki peminat judi terbanyak di indonesia. Pasaran yang berasal dari negara australia yang bertempatkan pada kota Sydney ini. memiliki jadwal result siang hari yaitu pukul 13.50 WIB dari situs resmi yaitu Bandar Togel Hadiah 4D 10 Juta Terpercaya. 

Pasaran togel sydney sama dengan pasaran hongkong ataupun singapore yang merupakan pasaran ter-lawas atau terlama di indonesia. Sehingga hampir seluruh bettor mengenal dengan pasaran yang satu ini dan di jamin 100% aman tidak adanya kecurangan apapun saat memutarkan angka. Dan pastinya pasaran yang satu ini tersedia pada seluruh website ataupun Bandar Togel Hadiah 4D 10 Juta Terpercaya Di indonesia. 

4. Pasaran Togel Singapore 
Pasaran togel terbaik ke-4 adalah singapore pools atau SGP yang dikenal oleh seluruh penjudi di tanah air indonesia. Pastinya seluruh masyarakat indonesia mengenal baik dengan pasaran yang satu ini. Baik kalangan muda maupun lanjut usia mengetahui betul bagaimana dari pasaran ini. Karena pasaran ini resmi dari negara SINGAPORE untuk perputaran lottery atau angka keluarannya. 

Selain ada akses resmi dari pihak APLA dan WLA tentunya ada berbagai organisasi resmi yang wajib dimiliki oleh tiap-tiap pasaran togel resmi dan terbaik di dunia. Sebab setiap perputaran togel yang di lakukan tersebut wajib memiliki pengamatan dari organisasi tersebut agar tidak terjadi kecurangan ataupun hal-hal yang tidak diinginkan. Dan dengan adanya organisasi tersebut hadiah togel online pun bisa menjadi sangat besar karena sponsor yang diberikan oleh pihak organisasi tersebut. 

Namun pasaran ini sempat menjadi kontroversi karena pada tahun 2020 silam pasaran ini libur cukup panjang dari bulan 4 2020 hingga akhir tahun 2021. Hal tersebut menjadikan pasaran singapore menjadi redup ataupun ditakuti oleh sebagian bettor togel di indonesia. Sebab semasa libur panjang dari negara tersebut banyak sekali munculnya website-website palsu yang mengaku-ngaku sebagai situs resmi Singapore. 

Tetapi sekarang ini Singapore atau SGP pools sudah kembali bersinar dan memberikan banyak sekali keuntungan untuk seluruh peminat judi togel di indonesia. Untuk hasil result dari pasaran ini hanya pada website Bandar Togel Hadiah 4D 10 Juta Terpercaya dan memiliki jadwal tayang yaitu 17.50 WIB untuk hari senin , rabu , kamis , sabtu dan minggu, sedangkan hari selasa dan jumat seperti biasa pasaran ini akan tutup alias libur. 

Related Keywords 

bandar togel terpercaya dan terlengkap 
bandar togel terpercaya 2020 
bandar togel online terpercaya dan berbayar 
bandar togel resmi indonesia 
bandar togel hadiah terbesar 
bandar togel hadiah prize 12345 
bandar togel resmi terbaru deposit 10 ribu 
bandar togel terbaik dan terbesar 2021 
bandar togel hadiah 4d 10 juta 
daftar bandar togel terbesar dan terpercaya 
5 bandar togel terpercaya 2021 
daftar bandar togel terbesar resmi deposit 10 ribu 
daftar bandar togel tertua 
semua bandar togel terbaik dan terpercaya no 1 
bandar lotre togel deposit via dana 
bandar togel terpercaya via gopay 
bandar togel terbesar 2021 
bandar togel 4D hadiah terbesar 
bandar togel terbaru dan terpercaya 2020 
bandar togel terbesar di dunia 
bandar togel resmi terpercaya 2021 
bandar togel terjitu di indonesia 
bandar togel terpercaya hadiah terbesar 
bandar togel terpercaya dan paling jitu 2021 
bandar togel terbesar di indonesia 
bandar togel terbaik di indonesia 
bandar togel terjitu dan terpercaya online 
Bandar Togel terpercaya indonesia via dana 
Bandar Togel terbesar via gopay 
Bandar Togel online 24 jam terpercaya 
Bandar Togel terpercaya 2020 
data togel terlengkap 2021 
Bandar Togel terbaik 
Bandar Togel termudah via ovo 
Bandar Togel terbaru 
Bandar Togel terbaru dan termurah via linkaja 
Bandar Togel terbesar dan terpercaya 
Bandar Togel terpercaya di indonesiabandar togel terpercaya dan terlengkap 
bandar togel terpercaya 2020 
bandar togel online terpercaya dan berbayar 
bandar togel formal indonesia 
bandar togel hadiah terbesar 
bandar togel hadiah prize 12345 
bandar togel formal terbaru deposit 10 ribu 
bandar togel paling baik dan terbesar 2021 
bandar togel hadiah 4d 10 juta 
daftar bandar togel terbesar dan terpercaya 
5 bandar togel terpercaya 2021 
daftar bandar togel terbesar formal deposit 10 ribu 
daftar bandar togel tertua 
seluruh bandar togel paling baik dan terpercaya no 1 
bandar lotre togel deposit via dana 
bandar togel terpercaya via gopay 
bandar togel terbesar 2021 
bandar togel 4d hadiah terbesar 
bandar togel terbaru dan terpercaya 2020 
bandar togel terbesar di global 
bandar togel formal terpercaya 2021 
bandar togel terjitu di indonesia 
bandar togel terpercaya hadiah terbesar 
bandar togel terpercaya dan paling jitu 2021 
bandar togel terbesar di indonesia 
bandar togel paling baik di indonesia 
bandar togel terjitu dan terpercaya online 
Bandar Togel terpercaya indonesia via dana 
Bandar Togel terbesar via gopay 
Bandar Togel online 24 jam terpercaya 
Bandar Togel terpercaya 2020 
knowledge togel terlengkap 2021 
Bandar Togel paling baik 
Bandar Togel termudah via ovo 
Bandar Togel terbaru 
Bandar Togel terbaru dan termurah via linkaja 
Bandar Togel terbesar dan terpercaya 
Bandar Togel terpercaya di indonesia 
Bandar Togel terpercaya hari ini","B: Penjelasan tentang pentingnya mencari penghasilan sampingan melalui permainan Bandar Togel Hadiah 4D 10 Juta.  
B: Rekomendasi beberapa Bandar Singapore Pools yang populer di kalangan Togellers.  
B: Daftar langkah-langkah untuk mendaftar akun di Situs BO Togel Terpercaya.  
B: Penjelasan pentingnya memiliki koneksi internet yang cepat dan stabil saat bermain togel online.  
B: Informasi tentang 4 negara pasaran Bandar Togel resmi dan terbaik di dunia.  
B: Deskripsi mengenai Pasaran Toto Macau sebagai pasaran togel terbaik dan resmi.  
B: Deskripsi mengenai Pasaran Togel Hongkong dan keunggulannya.  
B: Deskripsi mengenai Pasaran Togel Sydney dan reputasinya.  
B: Deskripsi mengenai Pasaran Togel Singapore dan kontroversi yang dihadapi.",-,FALSE,Over-decomposition
ERROR: Unable to generate spec: read file info,https://issues.apache.org/jira/browse/COUCHDB-2741,"Trying to build 51b98a4, I'm getting this failure during the make install target: 

/usr/bin/make DESTDIR=/home/micah/debian/couchdb/couchdb-2.0/debian/couchdb install 
make[2]: Entering directory '/home/micah/debian/couchdb/couchdb-2.0' 
==> b64url (compile) 
==> cassim (compile) 
==> lager (compile) 
==> couch_log (compile) 
==> config (compile) 
==> chttpd (compile) 
==> couch (compile) 
==> couch_epi (compile) 
==> couch_index (compile) 
==> couch_mrview (compile) 
==> couch_replicator (compile) 
==> couch_plugins (compile) 
==> couch_event (compile) 
==> couch_stats (compile) 
==> ddoc_cache (compile) 
==> ets_lru (compile) 
==> meck (compile) 
==> fabric (compile) 
==> bear (compile) 
==> folsom (compile) 
==> global_changes (compile) 
==> goldrush (compile) 
==> ibrowse (compile) 
==> ioq (compile) 
==> jiffy (compile) 
==> khash (compile) 
==> mango (compile) 
==> mem3 (compile) 
==> mochiweb (compile) 
==> oauth (compile) 
==> rexi (compile) 
==> snappy (compile) 
==> setup (compile) 
==> rel (compile) 
==> couchdb-2.0 (compile) 
Installing CouchDB into /home/micah/debian/couchdb/couchdb-2.0/debian/couchdb//usr/lib/couchdb... 
==> rel (generate) 
ERROR: Unable to generate spec: read file info /usr/lib/erlang/man/man8/cups-deviced.8.gz failed 
ERROR: Unexpected error: rebar_abort 
ERROR: generate failed while processing /home/micah/debian/couchdb/couchdb-2.0/rel: rebar_abort 
Makefile:84: recipe for target 'install' failed 
this is my install.mk: 

# Licensed under the Apache License, Version 2.0 (the ""License""); you may not 
# use this file except in compliance with the License. You may obtain a copy of 
# the License at 
# 
# http://www.apache.org/licenses/LICENSE-2.0 
# 
# Unless required by applicable law or agreed to in writing, software 
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the 
# License for the specific language governing permissions and limitations under 
# the License. 
# 
# The contents of this file are auto-generated by configure 
# 
package_author_name = The Apache Software Foundation 
install_dir = /usr/lib/couchdb 

bin_dir = /usr/bin 
libexec_dir = /lib/x86_64-linux-gnu/couchdb 
doc_dir = /usr/share/doc/apache-couchdb/couchdb 
sysconf_dir = /etc/couchdb 
data_dir = /usr/share/couchdb 

database_dir = /var/lib 
view_index_dir = /var/lib 
log_file = /var/log/couch.log 

html_dir = /usr/share/doc/apache-couchdb/html 
pdf_dir = /usr/share/doc/apache-couchdb/pdf 
man_dir = /usr/share/man 
info_dir = /share/info 

with_fauxton = 1 
with_docs = 1 

user = micah","B: ERROR: Unable to generate spec: read file info /usr/lib/erlang/man/man8/cups-deviced.8.gz failed  
B: ERROR: Unexpected error: rebar_abort  
B: ERROR: generate failed while processing /home/micah/debian/couchdb/couchdb-2.0/rel: rebar_abort  
B: Makefile:84: recipe for target 'install' failed  
B: The install.mk file is auto-generated by configure and may contain paths that need verification.",Major,FALSE,Over-analysis
LICENSE and NOTICE files are not correct,https://issues.apache.org/jira/browse/FLINK-10987,Flink's LICENSE and NOTICE files are not correct wrt http://www.apache.org/dev/licensing-howto.html. We need to update them before we can release 1.7.0.,"B: Flink's LICENSE file is not correct according to http://www.apache.org/dev/licensing-howto.html.  
B: Flink's NOTICE file is not correct according to http://www.apache.org/dev/licensing-howto.html.  
B: We need to update the LICENSE file before we can release 1.7.0.  
B: We need to update the NOTICE file before we can release 1.7.0.",Blocker,FALSE,Incorrect Interpretation of Solutions